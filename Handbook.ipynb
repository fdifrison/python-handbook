{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Python Manual</h1>\n",
    "\n",
    "by Ing. Giovanni Frison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " last update: 2023-01-16\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "print(f' last update: {date.today()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [PEP8 naming convention](#PEP8-naming-convention)\n",
    "2. [Everything is an Object](#Everything-is-an-Object)\n",
    "3. [Modules](#Modules)\n",
    "    1. [Some clarifications](#Some-clarifications)\n",
    "    2. [Reload a module](#Reload-a-module)\n",
    "4. [Packages](#Packages)\n",
    "    1. [Package structure](#Package-structure)\n",
    "    2. [Package Namespace PEP 420](#Package-Namespace-PEP-420)\n",
    "4. [Variables and Memory](#Variables-and-Memory)\n",
    "    1. [id function](#id-function)\n",
    "    2. [Reference Counting](#Reference-Counting)\n",
    "    3. [Shared Reference](#Shared-Reference)\n",
    "    4. [Garbage Collection](#Garbage-Collection)\n",
    "    5. [Object Mutability](#Object-Mutability)\n",
    "    6. [Variable Equality](#Variable-Equality)\n",
    "5. [Built-in methods](#Built-in-methods)\n",
    "   1. [isinstance(object, class)](#isinstance(object,-class))\n",
    "   2. [issubclass(object, class)](#issubclass(object,-class))  \n",
    "16. [Numeric Types](#Numeric-Types)\n",
    "    1.  [Integers](#Integers)\n",
    "        1.  [Operations](#Operations)\n",
    "        2.  [Base](#Base)\n",
    "    2.  [Rational Numbers](#Rational-Numbers)\n",
    "    3.  [Floats (Real Numbers)](#floats-real-numbers)\n",
    "        1.  [equality](#equality)\n",
    "    4.  [Booleans PEP 285](#Booleans-PEP-285)\n",
    "        1.  [Booleans operators](#Booleans-operators)\n",
    "        2.  [Short-Circuiting](#Short-Circuiting)\n",
    "6. [Iterable and Iterators](#Iterable-and-Iterators)\n",
    "    1. [Consume iterators manually](#Consume-iterators-manually)\n",
    "    2. [Lazy Iterables](#Lazy-Iterables)\n",
    "    3. [iter() method](#iter()-method)\n",
    "        1. [iter() with callables](#iter()-with-callables)\n",
    "    4. [Delegating Iterators](#Delegating-Iterators)\n",
    "    5. [Reversed Iteration](#Reversed-Iteration)\n",
    "    6. [Caveats: using iterators as function arguments](#Caveats:-using-iterators-as-function-arguments)\n",
    "7. [Generators](#Generators)\n",
    "   1. [Iterables from generators](#Iterables-from-generators)\n",
    "   2. [Generator expressions](#Generator-expressions)\n",
    "   3. [Yield from](#Yield-from)\n",
    "8. [Sequence Type](#Sequence-Type)\n",
    "   1. [Mutating sequence](#Mutating-sequence)\n",
    "      1. [in-place concatenation and repetition](#in-place-concatenation-and-repetition)\n",
    "      2. [Mutation by assignment](#Mutation-by-assignment)\n",
    "      3. [Never return a mutated object](#Never-return-a-mutated-object)\n",
    "   2. [Copying Sequences](#Copying-Sequences)\n",
    "      1. [Shallow copies](#Shallow-copies)\n",
    "      2. [Deep Copies](#Deep-Copies)\n",
    "   3. [Slicing](#Slicing)\n",
    "   4. [Custom Sequences](#Custom-Sequences)\n",
    "      1. [Mutation in custom sequences](#Mutation-in-custom-sequences)\n",
    "   5. [Sorting sequences](#sorting-sequences)\n",
    "   6. [Zero-based Index](#Zero-based-Index)\n",
    "   7. [Application - Polygon](#Application---Polygon)\n",
    "   8. [Lists vs Tuples](#Lists-vs-Tuples)\n",
    "       1. [Copying](#Copying)\n",
    "       2. [Storing Efficiency](#Storing-Efficiency)\n",
    "9.  [Iteration Tools - The itertools module](#Iteration-Tools---The-itertools-module)\n",
    "    1. [Aggregators](#Aggregators)\n",
    "    2. [iSlicing](#iSlicing)\n",
    "    3. [Selecting and Filtering](#Selecting-and-Filtering)\n",
    "    4. [Infinite iterators](#Infinite-iterators)\n",
    "    5. [Chaining and Teeing](#Chaining-and-Teeing)\n",
    "    6. [Mapping and Accumulation](#Mapping-and-Accumulation)\n",
    "    7. [Zipping](#Zipping)\n",
    "    8. [Grouping](#Grouping)\n",
    "        1. [Caveat: lazy iterators in I/O operation](#Caveat:-lazy-iterators-in-I/O-operation)\n",
    "    10. [Combinatorics](#Combinatorics)\n",
    "        1. [Cartesian Product](#Cartesian-Product)\n",
    "        2. [Combinations](#Combinations)\n",
    "        3. [Permutations](#Permutations)\n",
    "10. [Context Managers PEP 343](#Context-Managers-PEP-343)\n",
    "    1.  [Try..Except..Finally](#Try..Except..Finally)\n",
    "    2.  [The context management protocol](#The-context-management-protocol)\n",
    "    3.  [The 'with' Scope](#The-'with'-Scope)\n",
    "    4.  [The \\_\\_enter\\_\\_ mehtod](#The-\\_\\_enter\\_\\_-mehtod)\n",
    "    5.  [The \\_\\_exit\\_\\_ mehtod](#The-\\_\\_exit\\_\\_-mehtod)\n",
    "    6.  [Context Manager Class](#Context-Manager-Class)\n",
    "    7.  [Caveat with Lazy Iterator](#Caveat-with-Lazy-Iterator)\n",
    "    8.  [Additional use of Context Manager](#Additional-use-of-Context-Manager)\n",
    "        1.  [Redirect Standard Output](#Redirect-Standard-Output)\n",
    "        2.  [Timing a Function](#Timing-a-Function)\n",
    "    9.  [Context Manager Decorator](#Context#Manager#Decorator)\n",
    "    10. [Nested Context Manager: ExitStack](#Nested-Context-Manager:-ExitStack)\n",
    "11. [Strings](#Strings)\n",
    "    1. [Common methods](#Common-methods) \n",
    "12. [Lists](#Lists)\n",
    "    1. [List comprehension](#List-comprehension)\n",
    "13. [Tuples](#Tuples)\n",
    "   9. [Named Tuples](#Named-Tuples)\n",
    "      1. [Introspection](#Introspection)\n",
    "      2. [Modify and Extending](#Modify-and-Extending)\n",
    "      3. [Docstring](#Docstring)\n",
    "      4. [Defaults values](#Defaults-values)\n",
    "14. [Associative arrays](#Associative-arrays)\n",
    "15. [Dictionaries](#Dictionaries)\n",
    "    1.  [Creating dictionaries](#Creating-dictionaries)\n",
    "    2.  [Common operations with dictionaries](#Common-operations-with-dictionaries)\n",
    "    3.  [Dictionary view](#Dictionary-view)\n",
    "    4.  [Custom Class and Hashing](#Custom-Class-and-Hashing)\n",
    "    5.  [Deafultdict](#Defaultdict)\n",
    "    6.  [Counters](#Counters)\n",
    "    7.  [ChainMap](#ChainMap)\n",
    "    8.  [UserDict](#UserDict)\n",
    "    9.  [MappingProxy](#MappingProxy)\n",
    "16. [Sets](#Sets)\n",
    "    1.  [Basic Set Theory](#Basic-Set-Theory)\n",
    "    2.  [Python implementation of sets](#Python-implementation-of-sets)\n",
    "    3.  [Sets creation](#Sets-creation)\n",
    "    4.  [Common operation in sets](#Common-operation-in-sets)\n",
    "    5.  [Frozen Sets](#Frozen-Sets)\n",
    "17. [Serialization and Deserialization](#Serialization-and-Deserialization)\n",
    "    1. [Pickle](#Pickle)\n",
    "    2. [JSON](#JSON)\n",
    "    3. [JSONEncoder Class](#JSONEncoder-Class)\n",
    "    4. [Custom Encoding](#Custom-Encoding)\n",
    "    5. [JSONDncoder Class](#JSONDecoder-Class)\n",
    "    6. [JSON Schema](#JSON-Schema)\n",
    "    7. [Marshmallow](#Marshmallow)\n",
    "    8. [PyYaml](#PyYaml)\n",
    "    9. [Serpy](#Serpy)\n",
    "18. [Unpacking iterables](#Unpacking-iterables)\n",
    "   10. [Unpacking with *](#Unpacking-with-*)\n",
    "   11. [Nested unpacking](#Nested-unpacking)\n",
    "19. [Loops](#Loops)\n",
    "   12. [While loop](#While-loop)\n",
    "   13. [Try statement](#Try-statement)\n",
    "20. [Functions](#Functions)\n",
    "    1.  [Docstrings and annotations - PEP 257](#Docstrings-and-annotations---PEP-257)\n",
    "    2.  [lambda expression](#lambda-expression)\n",
    "    3.  [Function Introspection](#Function-Introspection)\n",
    "    4.  [\\*args and **kwargs](#\\*args-and-**kwargs)\n",
    "    5.  [Parameters default](#Parameters-default)\n",
    "    6.  [Map, Filter and Zip functions](#Map-Filter-and-Zip-functions)\n",
    "    7.  [Reducing Functions](#Reducing-Functions)\n",
    "    8.  [Partial functions](#Partial-functions)\n",
    "    9.  [The operator module](#The-operator-module)\n",
    "21. [Classes](#Classes)\n",
    "    1. [Attributes of classes](#Attributes-of-classes)\n",
    "    2. [Functions as instance attribute](#Functions-as-instance-attribute)\n",
    "    3. [Class initialization](#Class-initialization)\n",
    "    4. [Class Properties](#Class-Properties)\n",
    "        1. [@property class](#@property-class)\n",
    "    5. [Property Decorator](#Property-Decorator)\n",
    "        1. [Read-Only and Computed Properties](#Read-Only-and-Computed-Properties)\n",
    "    6. [Class and Static Methods](#Class-and-Static-Methods)\n",
    "    7. [Class body scope](#Class-body-scope)\n",
    "    8. [Polymorphism](#Polymorphism)\n",
    "    9. [Special Methods](#Special-Methods)\n",
    "        1. [\\_\\_str\\_\\_ method](#\\_\\_str\\_\\_-method)\n",
    "        2. [\\_\\_repr\\_\\_ method](#\\_\\_str\\_\\_-method)\n",
    "        3. [Arithmetic operators](#Arithmetic-operators)\n",
    "        4. [Truth value](#Truth-value)\n",
    "        5. [Callables](#Callables)\n",
    "    10. [Single Inheritance](#Single-Inheritance)\n",
    "        1. [instance vs type](#instance vs type)\n",
    "        2. [The *object* class](#The *object* class)\n",
    "        3. [Overriding](#Overriding)\n",
    "        4. [Extending](#Extending)\n",
    "        5. [Delegating to parent class](#Delegating to parent class)\n",
    "        6. [Method Binding](#Method Binding)\n",
    "        7. [Slots](#Slots)\n",
    "22. [Descriptors](#Descriptors)\n",
    "    1. [non-data descriptors](#non-data descriptors)\n",
    "        1. [getter and setter](#getter and setter)\n",
    "        2. [Storing instance properties](#Storing instance properties)\n",
    "    2. [Strong and Week references](#Strong and Week references)\n",
    "    3. [The set_name method](#The set_name method)\n",
    "    4. [Properties are decriptors!](#Properties are decriptors!)\n",
    "    5. [Functions implements the descriptor protocol!](#Functions implements the descriptor protocol!)\n",
    "22. [Scopes and Namespaces](#Scopes-and-Namespaces)\n",
    "    1.  [Masking](#Masking)\n",
    "    2.  [Nonlocal scope](#Nonlocal-scope)\n",
    "23. [Closure](#Closure)\n",
    "    1.  [Shared extend scope](#Shared-extend-scope)\n",
    "    2.  [Nested Closure](#Nested-Closure)\n",
    "    3.  [Application](#Application)\n",
    "24. [Decorators](#Decorators)\n",
    "    1.  [Multiple decorators](#Multiple-decorators)\n",
    "    2.  [Memoization](#Memoization)\n",
    "    3.  [Parametrized decorators](#Parametrized-decorators)\n",
    "    4.  [Decorator class](#Decorator-class)\n",
    "    5.  [Monkey Patching and Decorating classes](#Monkey-Patching-and-Decorating-classes)\n",
    "    6.  [Single Dispatch Generic Functions](#Single-Dispatch-Generic-Functions)\n",
    "        1.  [Application - Htmlizer](#Application---Htmlizer)\n",
    "25. [Python optimizations](#Python-optimizations)\n",
    "    1.  [Interning](#Interning)\n",
    "    2.  [Peephole](#Peephole)\n",
    "26. [Common Modules](#Common-Modules)\n",
    "    1. [string](#string)\n",
    "    2. [functools](#functools)\n",
    "    3. [itertools](#itertools)\n",
    "    4. [collections](#collections)\n",
    "    5. [random](#random)\n",
    "    6. [timeit](#timeit)\n",
    "    7. [argparser](#argparser)\n",
    "27. [Tips and tricks](#Tips-and-tricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEP8 naming convention\n",
    "---\n",
    "\n",
    "- `packages`: short lowercase and without underscore es. `utilities`\n",
    "- `modules`: short lowercase and with underscore es. `db_utils`\n",
    "- `classes`: first letter of each word are uppercase, no spaces and no underscore es. `MyClass`\n",
    "- `functions` lowercase and with underscore es. `open_account`\n",
    "- `variables` lowercase and with underscore es. `account_id`\n",
    "- `constants` all uppercase with underscore es `MIN_VAL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is an Object\n",
    "---\n",
    "In python everything is an object. Functions for example, inherit from the built-in function class; the same happen for classes which inherit from class function. This implies that every objects has a memory address (yes, even function and classes). In the same way every object che by assigned to a variable, passed as argument to a function or returned by a function. We can look at the object type of any variable with the `type` built-in function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules\n",
    "---\n",
    "\n",
    "First lest define what is the `Namespace`: essentially it is a dictionary that contains all the reference in memory that are currently loaded into the python interpreter. It can be access with two different keywords, `globals()` to access the global namespace, and `locals()` to access the local namespace of, let's say, a function.\n",
    "\n",
    "Like everything in python, also modules are object of the type module. When a module is imported, it get cached into memory (not in the namespace), and its memory reference is reported in the global namespace. \n",
    "Due to this, if we were to import the same module from two different scripts, the memory address would be the same across the scripts (we can think about it as a singleton object). We can look at the OS cache using the `sys.modules` which is a dictionary aswell.\n",
    "\n",
    "The importing is done at runtime (i.e. while the python interpreter is already running) and not at compilation time like in C for example. The way python retrieve its modules is quite complex but to help understand it we can make use of the `sys` module which has some useful functionalities. For example we can look at where the current python execution ans the relative C binaries are with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/usr', '/usr')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.prefix, sys.exec_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see we are using the active virtual environment where both the installation and the C binaries are locate. As a matter of fact, modifying the `prefix` is the way python use to activate/deactivate a virtual environment.\n",
    "\n",
    "But where python looks for import modules? There is a list of directory where python is looking, and these can be inspect with `sys.path`. If a module import fails, we can check if it is actually stored in one of the directory listed in path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fdifrison/Desktop/GitHub/PyFry-v1/Python',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/fdifrison/.local/lib/python3.10/site-packages',\n",
       " '/usr/local/lib/python3.10/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/home/fdifrison/.local/lib/python3.10/site-packages/IPython/extensions',\n",
       " '/home/fdifrison/.ipython']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations that python does during the import of a modules are:\n",
    "* checking if already exist in cache -> `sys.modules`\n",
    "* if not, create a new modules type object -> `types.ModuleType`\n",
    "* load the source code from file\n",
    "* add the entry to sys.modules\n",
    "* compile adn execute the source code -> N.B. the code in the imported module is executed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the importing of a module seems to be quite straightforward, what is more complicated is how python actually find the module we want to load. We can simply saying that there are 3 constructure at play:\n",
    "* finders\n",
    "* loaders\n",
    "* finder + loaders == importer\n",
    "\n",
    "First the `finders` are question wheter or not they know anything about the module we are trying to import; the list of the available finders can be found like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_frozen_importlib.BuiltinImporter,\n",
       " _frozen_importlib.FrozenImporter,\n",
       " _frozen_importlib_external.PathFinder,\n",
       " <six._SixMetaPathImporter at 0x7fa059cb57b0>,\n",
       " <pkg_resources.extern.VendorImporter at 0x7fa058a28580>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.meta_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one of the importer know the module it will built a `Modulespec` and tell the `loader` to load it. (es. the module math, since is a builti-in module, is found by the `BuiltinImporter` finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleSpec(name='math', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.__spec__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find if a module is in the python path and look at its specs at the same time with the built-in module `importlib` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleSpec(name='math', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.util.find_spec('math')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if `module_name` exist in `sys.path`, then its spec will be returned, if not we can solve the issue appending to the path list the directory where the module is found:\n",
    "\n",
    "```py\n",
    "import sys\n",
    "sys.path.append('module_path')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this has to become systematic, for example in a project where many paths have to added, the best way to procede is to compile a `.pth` file. For more information look at [https://docs.python.org/3/library/site.html](#https://docs.python.org/3/library/site.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some clarifications\n",
    "We have seen that when a module is imported for the first time, if founded in `sys.path`, its address is added to `sys.module`. Instead, what goes inside the namespace `globals()` depends on how we import the module itself, wheter with an alias or not. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# in this way the module math is loaded into sys.module and the name `math` is added to the namespace globals()\n",
    "# both the `math` names point to the same address\n",
    "\n",
    "import math as math_alias\n",
    "\n",
    "# in this way the module math is loaded into sys.module but name `math` is not added to the namespace globals()\n",
    "# instead we found the name `math_alias` which point to the same address associated to `math` in sys.module\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# in this way the module math is loaded into sys.module but in the namespace globals() we found only `sqrt`\n",
    "# that points to the function `math.sqrt`\n",
    "\n",
    "from math import *\n",
    "# in this way the module math is loaded into sys.module and the name of every function inside the math module is added to the namespace globals()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. if a name is already present in the namespace and we import something that has the same name, it gets overwritten. This is why it is not recommended to use the `import *` unless we are fully aware of every name we are importing and that there is no conflict between different module's names.\n",
    "\n",
    "N.B.B some things that using an explicit import of a function in a module like `from math import sqrt` is more lightweight; this is essentially not true because the entire math module is loaded in the `sys.module`, the only thing that change is that only the mane `sqrt` is added to the namespace. There is a very small advantage in calling the function because `sqrt(2)` has one less dictionary look-up that `math.sqrt(2)`, but since a dict-lookup is super fast the difference in efficiency is very very small. Therefore, in this case, do things for READABILITY and not EFFICIENCY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload a module\n",
    "If, for some reason, we want to reload a module, it is not sufficient to repeat the `import` statement, because the module name is already in the `sys.module`, and when the loader finds it it will skips its reloading. Neither using `del module_name` is enough because we are only deleting the name reference from the namespace. What we need to do is to delete the memory reference in the `sys.module` dictionary: `del sys.module['module_name']` . Now if we re-import the module, we are creating a new object, with a new `id`.\n",
    "\n",
    "As an alternative, if we want to reload the module without destroy and recreate the module object, we can use the `importlib.reload(module_name)` function, which will reload the module keeping the same memory id both for the `sys.module` and the namespace. However, care must be taken when reloading modules, for example, if we are loading only a specific function of a module to the namespace, let's say `from math import sqrt` we can directly reload the module since we don't have the name `math` in the namespace, but we'll have to refer to the sys.module reference -> `imporlib.reload(sys.module[math])`. However, even if `math` has been now reloaded, the same is not happened for the `sqrt` function. To do taht we would have needed to do something like `sqrt = sys.module[math].sqrt`. So be aware! reloading is not a safe process, isn't something we want to do in a production environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From more references on modules and module import look at :Ã¹\n",
    "* `PEP 302`\n",
    "* [https://docs.python.org/3/tutorial/modules.html](#https://docs.python.org/3/tutorial/modules.html)\n",
    "* [https://docs.python.org/3/reference/import.html](#https://docs.python.org/3/reference/import.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages are a collection of modules and possibly sub-packages that usually have some kind of specialized scope. The substantial difference that can tell us if a module is a package is the presence of a non-empty `__path__` attribute.\n",
    "\n",
    "\n",
    "The 99% of the packages are file-based, therefore structured into directories in the file system. In particular we have that the `directory name` became the `package-name` and the directory needs to contain the code somewhere (since a package is also a module but not vice-versa). In fact, the code goes inside the `__init__.py` file inside the directory, and the pair `directory + __ini__.py` compose a `module`. Substantially, when python finds inside a folder a `__init__.py` file it knows that it is looking at a `package` and not a standard directory. If not, python create an `implicit namespace package` that allow us to navigate trough folders inside the package itself.\n",
    "\n",
    "Now lets look at a typical app folder structure containing modules, package and sub-packages:\n",
    "\n",
    "```\n",
    "app/\n",
    "\n",
    "    module_1.py\n",
    "\n",
    "    pack_1/\n",
    "        `__init__.py`\n",
    "        module_1_a.py\n",
    "        \n",
    "        pack_1_1/\n",
    "            `__init__.py`\n",
    "            module_1_1_a.py   \n",
    "```\n",
    "\n",
    "Imagine we are executing our python interpreter inside the `app` folder. We can now simply say:\n",
    "* `import module_1` : import whats inside module_1.py. If we try to look at `module_1.__path__` we will find that it is empty `''` since module_1 is a module and not a package;\n",
    "* `import pack_1` : import the package `pack1` and loads what inside the `__init__.py`. Now the `pack1.__path__` is not empty since pack1 is a package. Also, python has added `pack1` has been added to both the `sys.modules` and `globals()` (namespace)\n",
    "* `import pack_1_1`: result in an ERROR since the python finder can't navigate up to this package; instead we need to:\n",
    "  * `import pack_1.pack_1_1` to be able to access the nested package. N.B. now `pack_1.pack_1_1` is stored inside `sys.modules` but not in `globals()` where only the root package is stored (`pack_1`); this because we are anyway always gonna refer it as `pack_1.pack_1_1` in our code.\n",
    "  * `from pack_1 import pack_1_1`: in this way we are storing `pack_1_1` in `globals()` but it is only a placeholder since it is actually pointing to the same objects as `pack_1.pack_1_1` (the `id(pack_1_1 == id(sys.modules['pack_1.pack_1_1']`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package structure\n",
    "\n",
    "When creating a package we need to keep in mind two quite opposite point of view: the `developer` of the package, which needs a good breakdown of the package's functionality to have a better modularity, better debugging, reading etc.. and the `user` which instead only needs to access the functionality coded into the package itself. Therefore, care is needed when structuring the package, and a good use of the `__init__.py` files has a key role. \n",
    "\n",
    "Looking at the previous dummy example, if the user need to access something inside module_1_1_a.py, the import would be something like `import pack_1.pack_1_1.module_1_1_a` i.e. something very tedious and inefficient. If instead the `__init__.py` inside `pack_1` contains directly something like `from pack_1_1 import *`, the user would be able to access everything just doing `import pack_1` because when the package is imported also the `__init__.py` is executed and added to the namespace. \n",
    "\n",
    "However, more then often, using a `*` import is something that we want to avoid, since inside a package there will be many functions/classes that are needed only by the developer and not the user (it might be even 'dangerous' if the user is able to access them). To avoid this there are essentially two methods:\n",
    "\n",
    "* name the 'private' (developer side) functions with an underscore in front (es. `def _func_only_for_dev()`) since the `*` import will avoid those\n",
    "* specify inside each module the list -> `__all__` = ['names_i_want_to_export_from_a_module'], i.e. the list of all the objects we want to be imported with `*` import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Namespace PEP 420\n",
    "Packages Namespace are essentially packages without a `__init__.py` file. They have the advantage that can be spread wherever in the file system (even in a zip file) but the import of sub-modules/packages can't be flatten (we don't have a `__init__.py` to leverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables and Memory\n",
    "---\n",
    "When a variable is created, what python is doing under the hood is to link the variable name to the memory slot (slots) which contains the element assign to the variable. Therefore the name is nothing more than a reference to the memory slot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `id` function\n",
    "`id` is the function that returns the memory address of a variable in base-10 ( can be converted with `hex` to see the hexadecimal representation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Counting\n",
    "Reference counting is a process carried out by the python memory manager internally. Each time we create a new variable, we are creating a reference to a memory slot. If we create a new variable that is equal to an existing one, we are adding a reference to the same memory slot (which now has a reference count equal to two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.refcount returns: 3\n",
      "ctypes returns: 2\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "import sys\n",
    "\n",
    "my_var_1 = list() # my_var is pointing to the memory slot id(my_var)\n",
    "\n",
    "other_var = my_var_1  # other var is pointing to the same memory slot of my_var\n",
    "# at this point, the ref count of id(my_var) is equal to 2\n",
    "\n",
    "print('sys.refcount returns:' , sys.getrefcount(my_var_1))\n",
    "# return the ref count of the variable + 1 far the call of sys itself\n",
    "\n",
    "print('ctypes returns:' , ctypes.c_long.from_address(id(my_var_1)).value)\n",
    "# is a lower level way to find the ref count of a memory slot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Reference\n",
    "When python variables share memory references?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, True, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 10\n",
    "b = a\n",
    "# b is not copying the content of a, it is pointing to the same memory address\n",
    "\n",
    "a, b, a == b, a is b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, True, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = 10\n",
    "b = 10\n",
    "# since the number 10 is immutable, both a and b are pointing to the same memory address\n",
    "\n",
    "a, b, a == b, a is b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4], [1, 2, 3, 4], True, True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = [1,2,3]\n",
    "b = a\n",
    "b.append(4)\n",
    "# now both b and a are equal to [1,2,3,4] since a list is mutable object and appending an element modify only its internal state with the same memory address\n",
    "\n",
    "a, b, a == b, a is b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [1, 2, 3], True, False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = [1,2,3]\n",
    "b = [1,2,3]\n",
    "# in this case python doesn't create shared references, so a and b are pointing to different objects, this to prevent that modifying b affects also a\n",
    "\n",
    "a, b, a == b, a is b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.b. There will be always a shared reference to `None` object, created automatically by python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage Collection\n",
    "It is the way python use to avoid memory leaks such that generated by circular references (objects pointing one to the other). Garbage collection can be controlled using the `gc` method. It can be turned off (only if we are super-sure that there are not circular reference in the code, in order to improve performance). The gc runs periodically on its own but can also be called manually to program a specific cleanup of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Mutability\n",
    "Changing the data inside an object is called `modifying the internal state` since the memory address is not changed but only its content (es. appending an element to a list). So we can distinguish between `Mutable` and `Immutable` object depending on the possibility of changing the internal state.\n",
    "\n",
    "`Immutable objects:`\n",
    "\n",
    "- Numbers\n",
    "- Strings\n",
    "- Tuples (if contains mutable elements, es. lists, those remain mutable)\n",
    "- Frozen Sets\n",
    "- User_Defined Classes (if so defined)\n",
    "\n",
    "`Mutable objects:`\n",
    "\n",
    "- Lists\n",
    "- Sets\n",
    "- Dictionaries\n",
    "- User_Defined Classes (if so defined)\n",
    "\n",
    "Care must be taken when we talk about immutability of an object that is given to a function as an argument. We have to distinguish between the `module scope` and the `function scope`.\n",
    "When we pass an object to a function we are in reality passing the `reference` of the object itself. So if we are passing to a function an immutable object, say a string, at the beginning both the module scope and the function scope point to the same memory reference, but as soon as the function modify the string (es. concatenating another string), then a new object with a new reference is created. If the object is mutable, say a list, and the function modify the list (es. appending an element), then python doesn't create a new object but simply modify the internal state of the existing memory reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('world', 'helloworld', 140326588266544, 140326588259184)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMMUTABLE\n",
    "def process_str(s):\n",
    "  # s has still the same memory reference as my_string\n",
    "  s = 'hello' + s\n",
    "  # now s has been modified, and since it was an immutable object, a new object with a new reference is created.\n",
    "  return s\n",
    "\n",
    "my_string = 'world'\n",
    "\n",
    "my_string, process_str(my_string), id(my_string), id(process_str(my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 5, 5], [1, 2, 3, 5, 5], 140326588253184, 140326588253184)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUTABLE\n",
    "def process_lst(lst):\n",
    "  # lst has the same memory reference as my_list\n",
    "  lst.append(5)\n",
    "  # since lst is a mutable object, only the internal state is changed but the memory reference is still the same.\n",
    "  return lst\n",
    "\n",
    "my_list = [1,2,3]\n",
    "\n",
    "my_list, process_lst(my_list), id(my_list), id(process_lst(my_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Equality\n",
    "There are two ways to verify the equality of two variables in python: the `is` and the `==` operators, respectively the identity and equality operators. While the identity operator compares the memory reference of two objects, the equality operator compares their internal state (data). Their negation are `is not` and `!=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = a\n",
    "print(a is b) # True since the memory address is the same (int are immutable objects)\n",
    "print(a == b) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = 500\n",
    "b = 500\n",
    "\n",
    "print(a is b) # False, preloaded integers are in the range [-5, 256] see Interning\n",
    "print(a == b) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,2,3]\n",
    "\n",
    "print(a is b) # False, different memory address\n",
    "print(a == b) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = 'hello'\n",
    "b = 'hello'\n",
    "\n",
    "print(a is b) # True, but not always! only if strings are stored as Singleton\n",
    "print(a == b) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = 10.0\n",
    "b = 10\n",
    "\n",
    "print(a is b) # False, float and int are different objects\n",
    "print(a == b) # True, python recognize the have the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in methods\n",
    "---\n",
    "\n",
    "## `isinstance(object, class)`\n",
    "Return `True` if an object is an instance of a particular class, `False` otherwise.\n",
    "\n",
    "## `issubclass(subclass, class)`\n",
    "Return `True` if class inherits from another upper class, `False` otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Types\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integers\n",
    "Integer are represented internally using base-2 digits (binary representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# es. binary representation of 19\n",
    "\n",
    " 0   0   0   1   0   0   1   1 \n",
    "--- --- --- --- --- --- --- ---   -> max num of bits = 8\n",
    "2^7 2^6 2^5 2^4 2^3 2^2 2^1 2^0\n",
    "\n",
    "1x2^4 + 0x2^3 + 0x2^2 + 1*2^1 + 1*2^0 = 16 + 2 + 1 = 19\n",
    "\n",
    "(10011)base_2 = (19)base_10\n",
    "\n",
    "To represent the number 19 are required 5 digits, hence 5 bits.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But which is the largest number we can store depending on the number of bits we want to store? It depends whether or not we care about negative values, since in order to store the sign we have to allocate one bit.\n",
    "The general formula is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```py\n",
    "max_unsigned_digit = 2^n -1 # where n is the number of bits\n",
    "max_signed_digits = [-2^(n-1), 2^(n-1)-1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side-Note: a 32 bit Operative system can store 2^32 unsigned integers (roughly 4Gb) and this limits also the number of memory address that can be stored at the same time. This is why having more than 4Gb of ram on a 32 bit OS is essentially useless since the machine can't store more than 4 Gb at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "How mod `//` (floor division) and div `%` (modulo) operators works in python? mod returns the floor division (rounded to the smaller integer) while div return the remainder. They have to satisfy the following equation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```py\n",
    "n = d * (n // d) + (n % d) # where n is the numerator and d is the denominator\n",
    "\n",
    "#n.b. the `floor` of a real number `a` is the largest integer `<= a` \n",
    "floor(3.14) = 3\n",
    "floor(-3.1) = 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base\n",
    "We can create an `int` object by calling the `int()` constructor; this has an optional parameter that is the base that python has to use to translate the argument (it may also be a string). The default values is base=10, since it is the way we are use to read numbers (while machines works in binary, so base=2). If the base is greater then 10 that the numbers start to be encoded with letters ( base[0, 10] = [0, 10], base[11, 27] = [A, Z])\n",
    "\n",
    "Python has some built-in function to translate the most common base like `bin()` for binary `hex()` for hexadecimal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```py\n",
    "bin(10) = 0b1010 # the 0b is telling us that the base 2 (binary)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rational Numbers\n",
    "Rational numbers are those number which are not integer and can be represented with a finite number of digits or translated into a fraction of rational numbers. The module `fraction` can be used to represent rational numbers, since the float representation can be misleading due to machine precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```py\n",
    "from fraction import Fraction\n",
    "\n",
    "Fraction(1,2) #(numerator, denominator)\n",
    "Fraction('0.125')\n",
    "Fraction(22/7)\n",
    "\n",
    "# CAVEAT\n",
    "'''\n",
    "Some numbers can't have a finite representation due to machine precision. For example 0.3 it is actually an approximation. The problem is that we have to look at something like the 20th decimal position in order to realize that this is the case. If we pass this number to Fraction() we would imagine to receive 3/10 as output; instead we would get a fraction of very huge numbers that best approximate that imprecision in the machine representation of 0.3 (0.2999999999999999999998977..)\n",
    "'''\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floats (Real Numbers)\n",
    "In CPython floats are implemented as `C double` which implements the `binary64` (IEEE 754).\n",
    "Floats use a fixed size of 64 bits divide as follow:\n",
    "\n",
    "- sign -> 1 bit\n",
    "- exponent (in the range[-1022, 1023]) -> 11bit\n",
    "- significant digits -> 52 bit (15-17 significant digit in base_10)\n",
    "\n",
    "To have a precise representation of real numbers (since float may be effected by machine precision), we can use the `decimal` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# decimal representation of a real number\n",
    "123.45 = 1*10^3 + 2*10^1 + 3*10^0 + 4*10^-1 + 5*10^-2 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equality\n",
    "Care must be taken when looking at the equality of floats since there are some decimal numbers that cannot be represented by a finite binary representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004441\n",
      "0.29999999999999998890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_10(0.1) = base_2(0.0 0011 0011 0011 ...) \n",
    "# therefore\n",
    "x = 0.1 + 0.1 + 0.1 \n",
    "y = 0.3 \n",
    "\n",
    "print(f'{x:.20f}')\n",
    "print(f'{y:.20f}') \n",
    "x == y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One workaround is to set a range delta (es. a percentage of the size of the larger number involved in the equality operation) as discriminant values to determine if two numbers are equal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "|a - b| < epsilon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pythonic way to approach this problem is to use the module `math.isclose` with the care of specifying appropriate relative and absolute tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# math.isclose(x, y, rel_tol, abs_tol)\n",
    "math.isclose(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Booleans PEP 285\n",
    "Booleans are a subclass of the int class (i.e. it inherits all its methods). Two constant are defined: `True` (int = 1) and `False` (int = 0). They are Singleton objects, i.e. the point to a fixed address in memory and can be compared with the identity and the equality operator aswell. N.b. even if True and False evaluates to the int 1 and 0 respectively, they don't point ot the same memory address, since they are not the same type of object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(True) == 1 and int(False) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(True) != id(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(False) != id(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects have an associated `truth value`, meaning that they have a defined truth state. In particular, every object will evaluate to `True` by default except for:\n",
    "\n",
    "- None\n",
    "- False\n",
    "- 0 (in any numeric type, float, complex ..)\n",
    "- empty sequence (list, tuple string..)\n",
    "- empty mapping (dictionary, sets..)\n",
    "- implementing __bool__ or __len__ in a custom class\n",
    "\n",
    "Has a matter of fact, when we call `bool()` on an object, it will look for the definition of the dunder method `__bool__`, if this is not defined, then it will look for the `__len__` method and if also this is not defined the it will evaluate to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es. __bool__ implementation for the int class\n",
    "def __bool__(self):\n",
    "  return self != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booleans operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Truth Table\n",
    "X Y  not X  X and Y  X or Y\n",
    "0 0    1       0       0\n",
    "0 1    1       0       1\n",
    "1 0    0       0       1 \n",
    "1 1    0       1       1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# De Morgan's Theorem\n",
    "not(A or B) == (not A) and (not B)\n",
    "not(A and B) == (not A) or (not B)\n",
    "\n",
    "# Operations precedence (in descending order)\n",
    "< > <= >= == != in is\n",
    "not\n",
    "and\n",
    "or\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-Circuiting\n",
    "Looking at a truth table there are two case in which the program can simply is job evaluating only part of a boolean statement. Thi is called `short-circuiting`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True or Y # -> True \n",
    "# with an or statement, id the first argument is True it doesn't matter whether the second argument is True or False, the operation will always evaluate to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False and Y # -> False\n",
    "# with an and statement, id the first argument is False it doesn't matter whether the second argument is True or False, the operation will always evaluate to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very useful when we have to concatenate two conditions together, one of the which may results in rising an exception (and breaking the code) if evaluated. With short-circuiting we can add a first statement that check for a particular exception that may rise with second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = 'ciao'\n",
    "if 'a' == my_string[0]:\n",
    "  pass\n",
    "# we are checking if 'a' is the first letter of my_string. But what happen if my_string is empty? the code breaks. We can solve this with short-circuiting the first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_6828/1172889395.py\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmy_string\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mif\u001B[0m \u001B[0;34m'a'\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mmy_string\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "my_string = ''\n",
    "if 'a' == my_string[0]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if my_string and 'a' == my_string[0]:\n",
    "  pass\n",
    "# Since an \"and\" expression will evaluate to True only if the two members are True, we can safeguard our code from breaking checking first if my_string evaluates to False (i.e. if is an empty string);\n",
    "# if it is so, the second part of the \"and\" statement won't be executed thus safeguarding our code of breaking due to IndexError exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterable and Iterators\n",
    "---\n",
    "Iterables are, in essence, containers that can be iterated, nothing more. For example, sequences are particular iterables upon which we can iterate on an index-base, but this is not always the case for an iterbale. An iterable only need the concept of `next` item, meaning that it should return another element from the container, without any ordering implied. Moreover iterators needs to keep track of the elements that have been handed out, since we don't want the same element twice, and a stopping creterion is needed to tell us when the container is exhausted and no more elements are available. Other features are: having a finite number of elements in the container, the possibility to \"start from the beginning\" i.e. reuse the iterable if needed, the use of list comprehension etc..\n",
    "\n",
    "Python has a special method called `next()` that lives under `__next__` and fit exaclty the purpose of iteraring over an iterable, i.e handing out a new elemnt each time it is called on an iterable. We also have a built-in exception that is made to check if the iterable is exhausted, this is `StopIteration`.\n",
    "\n",
    "Let's imagine we want to create our custom iterable class, how can we tell Python how to behave properly on the `__next__` method? how to not exhaust our class instance once we have iterated over all the elements? \n",
    "To answer this we need to use a **`protocol`** i.e. a way to tell to the python interpreter that our class has to implement certain functionality.\n",
    "\n",
    "In particular, for an `iterator` we need a `iterator protocol` that requires the class to have 2 methods:\n",
    "* `__iter__` a method that should only return the class instance (why???)\n",
    "* `__next__` to handle the elements return and the eventual raise of StopIteration\n",
    "\n",
    "If our class satify this prerequisite than we have an **`iterator`** upon which we can use for loops, list comprehension etc.. except for the reusability of the iterable, once it is consumed it has to be reinstaciated.\n",
    "\n",
    "At the end, the solution to our problem is to have two distinct objects: the `iterable` that is the collection of the elements, it never get exhausted, it is just a container; and the `iterator`, a copy of the iterable object that is responsible for iterating over the elements. the iterable is created once while the iterator is created any time we want to restart the iterating process. This is a good deal form amny reasons, for example we may have a use set of data stored in our iteable and we don't want to reload them each time we need to restart the iteration!\n",
    "\n",
    "As a formal distinction, an `iterable` is a python object that implement an `iterable protocol` that only requires the definition of the `__iter__` method which will return a new instance of the iterator each time is called:\n",
    "\n",
    "```py\n",
    "class Iterator\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __iter__(self): # return itself\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        # criterion for StopIteration\n",
    "\n",
    "class Iterable:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __iter__(self): # return the iterator\n",
    "        return Iterator(self)\n",
    "```\n",
    "\n",
    "Basically, we are not iterating over the instance of the iterable object but over the iterator generated by the iterable itself! \n",
    "\n",
    "N.B. when we ask python to iterate over an object it will first look for the `__iter__` method and only after for the `__getitem__` method. This means that if we implement both in our custom class, during iteration python will use the former.\n",
    "\n",
    "Python has different built-in (lazy) iterable and iterators and it is fundamental to know who is what now that we know the difference between the two constructs:\n",
    "\n",
    "* `range()` -> iterable\n",
    "* `dict.keys()` / `values()` / `items()` -> iterable\n",
    "* `zip()` -> iterator\n",
    "* `enumerate()` -> iterator\n",
    "* `open()` -> iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume iterators manually\n",
    "Imagine we need to parse a csv file  where the first two lines are the header and the data types. We could essentially do a for loop with nested if condition to split the header and the data type from the actual data or we could apply what just learned about iterators. Essentially we want to create and iterator from the iterable (the file to be parsed) and assign directly the headers and the data types to a variable and performe a for loop only on the data rows.\n",
    "\n",
    "```py\n",
    "with open(data_file.csv) as file:\n",
    "    file_iter = iter(file)\n",
    "    header = next(file_iter) # first row\n",
    "    data_types = next(file_iter) # second row\n",
    "    data = [row for row in file_iter] # from the third row and on\n",
    "```\n",
    "\n",
    "The results is a very clean and efficient way to parse the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Iterables\n",
    "Let's first define what is a **Lazy Evaluation**: it is often referred to class properties that are not directly evaluated when the instance of the class is created but are computed, and becomes available, only when the propety is requested. Once the propery is requested for the first time, its value is cached in the instance state, therefore it doesn't need to be computed again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "class Circle:\n",
    "    def __init__(self, r):\n",
    "        self.radius = r # using the setter\n",
    "        self._area = None # Lazy area\n",
    "        \n",
    "    @property\n",
    "    def radius(self):\n",
    "        return self._radius\n",
    "    \n",
    "    @radius.setter\n",
    "    def radius(self, r):\n",
    "        self._radius = r\n",
    "        self._area = None # reset area when the radius is changed\n",
    "        \n",
    "    @property\n",
    "    def area(self):\n",
    "        if self._area is None:\n",
    "            print('Calculating area')\n",
    "            self._area = pi*self.radius**2\n",
    "            #return self._area\n",
    "    \n",
    "        return self._area # if area was not None we already have the cached value   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = Circle(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if called again\n",
    "c1.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the result is cached\n",
    "c1.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the radius is changed\n",
    "c1.radius = 3\n",
    "c1.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concept can be applied also to iterables, and in fact it is something used very often in python `generators`; essentially, the element is returned only when `next()` is called. Fro example we can create an infinte lazy iterable that will compute factorial only when requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "class Factorial:\n",
    "    def __iter__(self):\n",
    "        return self.FactIter()\n",
    "    \n",
    "    class FactIter:\n",
    "        def __init__(self):\n",
    "            self.i = 0 # initializer\n",
    "            \n",
    "        def __iter__(self):\n",
    "            return self\n",
    "        \n",
    "        def __next__(self):\n",
    "            f = factorial(self.i)\n",
    "            self.i += 1\n",
    "            return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = iter(Factorial())\n",
    "for _ in range(10):\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code we can se that the iterable has almost nothing to do, all the computation is carried out in the iterator class. `enumerate()` and `zip()` are built-in lazy iterator of the python stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iter() method\n",
    "What happens when we call `iter(obj)`? Python essentialy first look for an `__iter__` method defined in the object class, if not, it look for the `__getitem__` method where basically the iterator is created over a sequence that is iterated with `next` as if it were a while loop, catching the exception for `IndexError` and raising the `StopIteration`. Folloqiong an example of a Sequence-Iterator class (something very similar to what python does when we call `iter()` on an object that has only `__getitem__` defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqIter:\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "        self.index = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            item = self.seq[self.index]\n",
    "            self.index += 1\n",
    "            return item\n",
    "        except IndexError:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter() with callables\n",
    "There is a second form of the `iter()` method that is useful to iterate over callables. In the form `iter(callable, sentinel)` we can specify a sentinel argument wich is the criterion to call the StopIteration on the iterable. Of course, if the sentinel value is never met, we'll generate an infinite iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call():\n",
    "    i = 0\n",
    "    \n",
    "    def inner():\n",
    "        nonlocal i\n",
    "        i += 1\n",
    "        return i\n",
    "    return inner\n",
    "\n",
    "inner = call()\n",
    "iter_call = iter(inner, 5) # set the sentinel value to 5\n",
    "\n",
    "for c in iter_call:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delegating Iterators\n",
    "Let's imagine we have a class that is storying a list of elements in a pseudo-private variable (i.e. beginning with \\_). Once an instance of the class is created, unless specifically implemented, we cannot iterate over this list (unless we are aware of the private variable, but aniway shouldn't be the case). On option could be to add the capability to the class to generate an iterator out of that list, but it is code that we dont want to handle directly. What we can do is to `delegate` the generation of the iterator to the `iter` method itself; in this way we only need to implement the `__iter__` method in our class. As a matter of fact, in real practice we will often not create a custom iterable, unless we need specific functionality, but we'll leverage the `iter()` method, delegating to it theduty of returning an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Delegate:\n",
    "    def __init__(self, some_list):\n",
    "        self._somelist = some_list\n",
    "        # do something to this list\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self._somelist)\n",
    "    \n",
    "# since some_list is already an iterable (a list) we can create easily an iterator with iter().\n",
    "# In this way the class Delegate is simply trasformed in an iterable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reversed Iteration\n",
    "It may happen to need to iterate over an iterable in reversed order, and what we usually do is to use a for loop and slicing like:\n",
    "```py\n",
    "for i in lst[::-1]:\n",
    "    print(i)\n",
    "```\n",
    "In this way we are creating a copy of the list, maybe only to iterate over some few elements in a huge sequence! A quite bad looking sintax solution would be to iterate backwards knowing the length of the sequence:\n",
    "```py\n",
    "for i in range(len(lst)):\n",
    "    print(lst[-i-1])\n",
    "```\n",
    "However, the best approach, both in terms of efficiency and readability, is tu tranform the list in a `reversed iterator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [i for i in range(5)]\n",
    "for s in reversed(seq):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the method `reversed()` is creating an iterator of the iterable seq, therefore we are not creating a copy of the sequence. What Python does behinde the scene is to call the `__reversed__` method, and this is it , at least for sequence type.\n",
    "\n",
    "However, the same does not apply if we want to reverse iterate over a general iterable that doesn't support indexing. In this case we need the `__reversed__` method to return an iterator. As for the `iter()` method, python will first search for the `__reversed__` method, and if don't find it it wil look for both the `__getitem__` and `__len__` methods (basically to do what we have shown before with the for loop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats: usign iterators as function arguments\n",
    "Due to the fact that we can only iterate once over an iterator before it is exhausted, we need to be careful to use it as a function argument. \n",
    "\n",
    "Let's immagine that we created a class that returns an iterator consisting in a list of numbers: now suppose we need to find the max and the min betweeen these. If we call, for example the `min()` method we will get the minimum values, but to get this python had to iterate over the iterator exhausting it! Therefore, if now we ask for `max()` we'll incurr in a `ValueError` since our iterator has reached the `StopIteration` condition with the `min()` method and now it is nothing more than an empty sequence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators\n",
    "---\n",
    "Generators are functions that contain at least one `yield` statement, are essentially iterator (iterato protocol implemented), and are inherently lazy.\n",
    "\n",
    "Let's imagine we want to create a function that can be stopped during execution (like a for loop that is stopped at each iteration) so that we can handle the data to do something else before the function is exhausted. In python there is a particular keyword called `yield` which emits a value and pause the function execution; than it can be resumed calling `next`. Once the function execution is finished, a `StopIteration` exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es.\n",
    "def using_yield():\n",
    "    print('Name')\n",
    "    yield 'Giovanni'\n",
    "    print('Surname')\n",
    "    yield 'Frison'   \n",
    "    \n",
    "test = using_yield()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `test` is not a function but a `generator object` upon which we can call the `next()` method to execute the body until a `yield` statement is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for an iterator, when the function body is exahusted, i.e. no yield statment remains, the `StopIteration` is raised.\n",
    "\n",
    "A function that use the `yield` statment in his body is called `generator function` and is essentially a `generator factory`. The `using_yield` function above is just a regular function, but since it contains a `yield` in the body, python compiler knows that it is a generator function/factory, and each time it is called a generator is created. In this way we are able to execute the function piece-wise, one `yield` statement at the time, exiting and re-entering the fucntion in different part of our code, calling `next` on the generator function.\n",
    "\n",
    "The generator behave like an iterator because it is an iterator! it has the iterator protocol implemented (`__iter__` and `__next__` methods). As a matter of fact, generatos are powerful tools to create iterator in a very effective way. Es. looking at the Factorial implementation that can be found in [Lazy Iterables](#Lazy-Iterables) section, we can easily re-implementing it using a yield statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def factorials(n):\n",
    "    for i in range(n):\n",
    "        yield math.factorial(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in factorials(5):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterables from generators\n",
    "Being an iterator, a genrator has the same caveat, meaning that once it is exhausted it has no use and sometimes this can lead to unwanted bugs in our code. However, there is a solution to this, we could make an iterable from our generator simply by defining an iterable class that in the `__iter__` method returns not `self` but an instance of our generator. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_gen(n):\n",
    "    for i in range(n):\n",
    "        yield i ** 2\n",
    "\n",
    "sq = square_gen(4)\n",
    "for s in sq:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we created a generator factory (`square_gen`) and a generator from it (`sq`) that it now exhausted due to the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can define a custom iterable ourself (implementing the iterable protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square:\n",
    "    def __init__(self, n:int):\n",
    "        self.n : int = n\n",
    "            \n",
    "    # iterable protocol\n",
    "    def __iter__(self):\n",
    "        # we need to retunr an iterator or in this case \n",
    "        # our generato function\n",
    "        return Square.square_gen(self.n) \n",
    "    \n",
    "    @staticmethod # it doesn't use any of the class properties\n",
    "    def square_gen(n):\n",
    "        for i in range(n):\n",
    "            yield i ** 2   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call `sq` as many time as we want since it time is automatically returning a new instance of the generator function `square_gen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = Square(4)\n",
    "for s in sq:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator expressions\n",
    "Using the same sintax of list comprehension we can create generator expression, the only difference is in that generators wants round brackets `()`. The advantage of generator expressions is that they are ineherently `lazy`, meaning that the expressions are not evaluated until requested by the `next()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [i ** 2 for i in range(5)] # iterable\n",
    "gen = (i **2 for i in range(5)) # iterator\n",
    "\n",
    "lst, gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing list and generator comprehension we have:\n",
    "* list takes longer to create since they have to evaluate each expression, while generator are returned immediately\n",
    "* list iteration is faster since the object has already been created\n",
    "\n",
    "Therefore, we canc conclude that:\n",
    "* if we need to iterate over all the elements, the  time performance is almost the same, but generators occupy less space since once iterated are destroyed\n",
    "* If we need to iterate only on some elements then generators are the way to go, since we don't compute all the unwanted iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield from\n",
    "In it easiest application, `Yield from` is just a way to replace a for loop over an iterable inside a generator expression. Let's take for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_on_nested_generator(n):\n",
    "    gen = ((i*j for i in range(1,n+1)) for j in range(1,n+1))\n",
    "    for row in gen:\n",
    "        for item in row:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = loop_on_nested_generator(2)\n",
    "for l in loop:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we can sustitute the last for loop with a `yield from` an achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_from_nested_generator(n):\n",
    "    gen = ((i*j for i in range(1,n+1)) for j in range(1,n+1))\n",
    "    for row in gen:\n",
    "        yield from row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = yield_from_nested_generator(2)\n",
    "for l in loop:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Type\n",
    "---\n",
    "In math terms a sequence is nothing more than a countable group of items that have a positional ordering, meaning that each element can be accessed by an index representing its position. In Python, a `list` is a sequence type while a `set` is not since it doesn't have a positional order.\n",
    "A sequence can be `mutable` (list, bytearrays) or `immutable` (string, tuples, range, bytes). \n",
    "Again, sequence can be `homogeneous`, if they held elements of the same type (like strings) or `heterogenuos` (lists). \n",
    "A sequence is also an `iterable type` since we can reach each element one-by-one hence iterating over the sequence (n.b. an iterbale is not always a sequence, set is an example).\n",
    "\n",
    "Common methods on sequence are:\n",
    "* `in` and `not in`\n",
    "* `+` for concatenation (not for range type)\n",
    "* `* int` for repetition (not for range type)\n",
    "* len() to retrieve the length of the sequence\n",
    "* index(x) to retrieve the occurence of element x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutating sequence\n",
    "\n",
    "With mutation in Python we refer to the change in the internal state of a mutable object without modifying it memory address. For example list concatenation is not a mutation since a new object is created while usign the method `append` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenation\n",
    "l = ['Giovanni']\n",
    "l_id = id(l)\n",
    "l = l + ['Frison']\n",
    "id(l) == l_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending\n",
    "l = ['Giovanni']\n",
    "l_id = id(l)\n",
    "l.append('Frison')\n",
    "id(l) == l_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other mutation can be achieved with:\n",
    "* slicing assignation -> `s[i] = x`\n",
    "* delete of elements -> `del s[i]`\n",
    "* removing all the objects in the container -> `s.clear()`\n",
    "* inserting elements -> `s.insert(i, val)`\n",
    "* extend with another iterable -> `s.extend(iterable)`\n",
    "* pop (return and remove the element at index i) -> `s.pop(i)`\n",
    "* remove the first occurrence of x -> `s.remove(i)`\n",
    "* reverse in-place -> `s.reverse()`\n",
    "* and many more ..\n",
    "\n",
    "N.B. not all the sequence type must have this methods, in particular if custom made by us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in-place concatenation and repetition\n",
    "Sequence can be concatenate or repeted using the `+` or the `*`. If We do it in the stadard way we obtain a new object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2,3]\n",
    "l2 = [4,5,6]\n",
    "l1_prev_id = id(l1) \n",
    "l1 = l1 + l2\n",
    "\n",
    "id(l1) == l1_prev_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however, if the sequence is mutable, we use inplace concatenation `+=` or repetition `*=` we are mutating the object, therefore the `id` remain the same. If the sequence is immutable, like a tuple, a new object will be created anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2,3]\n",
    "l2 = [4,5,6]\n",
    "l1_prev_id = id(l1) \n",
    "l1 += l2\n",
    "\n",
    "id(l1) == l1_prev_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutation by assignment\n",
    "Some mutable sequence, like lists, support the assignment via index, meaning that we can replace elements by assigning new values to the list. This works also with slices provided that the we provide as substituing value an iterable, and it doesn't even need to be of the same length of the slice we are replacing! We can aslo have stepwise slices as a replacement, but in that case the length of the iterable must match the number of element selected. In the same way we can delete elements just by replacing a slice with an empty list. At last, usign a trick we are also able to insert an iterable inside a sequence: first we need to create an empy assignation to  a slice on the same index e.g. `l[1:1] = []`, and then we can assign an iterable to that slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing a slice with as many elements as we want\n",
    "l = [1,2,3,4,5,6,7,8,9,10]\n",
    "l[:2] = ['a', 'b', 'c', 'd'] # place 4 elements instead of the first 2\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing a stepwise slice with the same number of element selected\n",
    "l = [1,2,3,4,5,6,7,8,9,10]\n",
    "l[::2] = ['a', 'b', 'c', 'd', 'e'] # 5 element change with 5 element\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete elements\n",
    "l = [1,2,3,4,5,6,7,8,9,10]\n",
    "l[:5] = []\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert elements\n",
    "l = [1,2,3,4,5,6,7,8,9,10]\n",
    "l[1:1] = []\n",
    "l[1:1] = [1,2,3]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Never return a mutated object\n",
    "If we write a plain function it is best practice not to modify the element we are passing as argument but return a modified copy of it. So called `in-place methods` are generally bouded to classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we should do\n",
    "def reverse(s):\n",
    "    s.reverse()\n",
    "\n",
    "# what we shoudn't do\n",
    "def reverse(s):\n",
    "    s.reverse()\n",
    "    return s\n",
    "\n",
    "s1 = [1,2,3]\n",
    "s2 = reverse(s1)\n",
    "\n",
    "s1, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Sequences\n",
    "\n",
    "While copying immutable sequence is in general a safe procedure, the same cannot be stated about mutable ones. A trivial example comes from care concatenation and repetition, because the repeting/concatenating will create a copy of the object and if we then modify one of the copy, the same happen to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow copies\n",
    "We created a function that apply an in-place method to the argument `s` of the function and then `return s`. The user then expect to use that return and assign the function call to a variable `s2` thinking that he created a new object while instead `s2` is now poiting to the same object of `s1` that as been modified as well. What we should do is **not** returning the function.\n",
    "\n",
    "Even better, it would be not to do in-place modification to our objects, but create a copy first and than pass it to the function that modifies it. To copy a sequence, or any objects, there are a variety of ways, some more pythonic then others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ways of coopying a list creting a new object but leaving the same\n",
    "# memory reference to the elements inside the list\n",
    "\n",
    "s = [1, 2, 3]\n",
    "\n",
    "# 1. simple loop (horrible)\n",
    "cp = []\n",
    "for i in s:\n",
    "    cp.append(i)\n",
    "    \n",
    "# 2. list comprehension\n",
    "cp = [i for i in s]\n",
    "\n",
    "# 3. copy method (not for immutable sequence like tuple or strings)\n",
    "cp = s.copy()\n",
    "\n",
    "# 4. slicing (with tuple the same element is returned)\n",
    "cp = s[:len(s)]\n",
    "\n",
    "# 5. list method (with tuple the same element is returned)\n",
    "cp = list(s)\n",
    "\n",
    "\n",
    "# therefore we have\n",
    "s == cp,  s is cp,  [s[i] is cp[i] for i in range(len(s))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have performed are called `shallow copies`, i.e. we have created a copy only of the sequence object but the elements inside have the same memory reference. If these elements are immutable objects than the copy is safe, meaning that we can modifying it without affecting the source. However, if the sequence contains mutable objects then a shallow copy may not be enough. Lets see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [[1, 2], [3, 4]]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `l` is a sequence that contains 2 mutable objects. We can create a copy and try to sustitute one of its element and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = l.copy()\n",
    "cp[0] = 'python'\n",
    "cp is l, cp, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The copy `cp` is actually a new object, and when we say `cp[0] = 'python'` we are actually mutating this object, without affecting the original list `l`. but what happens if we try to modify the mutable objects inside `cp` (which share the same memory reference with the one contained in `l`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = l.copy()\n",
    "cp[0][0] = 100\n",
    "cp, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see noth the inner list in `cp` and `l` have been modified, this because the `l.copy()` is a shallow copy and effetcs only the outer object. As a matter of fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp[0] is l[0], cp[1] is l[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Copies\n",
    "\n",
    "To performe a copy at the deepest level of an object a recursive approach, able to handle circular references, is needed. Python has the built-in module `copy` to carry out a deep copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "cp = deepcopy(l)\n",
    "l, cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp[0] is l[0], cp[1] is l[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the deepcopy is intelligent enough to retin references also after the deep copy. Lets see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "        \n",
    "x = MyClass(100)\n",
    "y = MyClass(x)\n",
    "lst = [x, y]\n",
    "\n",
    "x is y.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `lst` is a sequence that contains two elements, `x` and `y` that has an attribute `y.a` that point to `x` (not a circular reference. Now if we performe a deepcopy, python will create new objects for each element, but will retain the relationship between `y.a` and `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = deepcopy(lst)\n",
    "# now even if we have different objects\n",
    "x is cp[0], y is cp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relationship is retained\n",
    "cp[0] is cp[1].a # same of x is y.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing\n",
    "\n",
    "Slicing is an opertaion that works with indexing, therefore it is appliecable only to sequence type objects. A `slice` is an object of type slice that can also be created, and assigned to a varible, with the keyword `slice()`.\n",
    "Slicing always return a new object\n",
    "\n",
    "The esiest way to slice is specify the star and stop:\n",
    "* `l[i:j]` with i included and j excluded\n",
    "\n",
    "Slice are independente from the sequence they are slicing, therefore even if the stop of the slice is out of bound for the sequence, python won't throw an error but slice to the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,4,5]\n",
    "l[0:3], l[2:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible also to specify a third argument for the slice which is the step (or stride), default to 1. Moreover, if the stop argument is not given, python automatically considere the `len` of the sequence as stopping point; same goes for the start argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[::2] # start:stop:step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conseguence, it is easy to reverse a list with slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can have arguments of th slice that are greater the the length of the sequence or even negative. To understand which range are we actually taking into account we need to remeber the folloqing rules:\n",
    "\n",
    "given `l[i:j:k]`:\n",
    "\n",
    "* if `k > 0`:\n",
    "    * if `i, j > len(seq)` -> `len(seq)`\n",
    "    * if `i, j < 0` -> `max(0, len(seq) + i(or j)`\n",
    "    * if `i` is omitted or `None` -> `0`\n",
    "    * if `j` is omitted or `None` -> `len(seq)`\n",
    "* if `k < 0`:\n",
    "    * if `i, j > len(seq)` -> `len(seq) - 1`\n",
    "    * if `i, j < 0` -> `max(-1, len(seq) + i(or j)`\n",
    "    * if `i` is omitted or `None` -> `len(seq) - 1`\n",
    "    * if `j` is omitted or `None` -> `-1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in the end we are not sure of the slices we are doing, or we don't remember this rules, we can create a slice object with `i,j,k` needed and the use the method `indices()` that will return the indices `i,j,k` corresponding to the `range(i,j,k)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice(0,6,2).indices(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is equivalent to\n",
    "l, list(map(lambda x: l[x], range(0, 5, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Sequences\n",
    "An essential feature that as to be implemented in a custom sequence object is the `__getitem__` method since it is the one that make possible to iterate over the sequence enabling all sort of iterations, from list comprehension to for loops. The `__getitem__` method should be coded in order to handle positive and negative integers as well as slice objects. An idea of implementation could be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class MySequence(object):\n",
    "        \n",
    "    def __init__(self, length):\n",
    "        if isinstance(length, int):\n",
    "            self.length = length\n",
    "            self.sequence = [i for i in range(length)]\n",
    "        else:\n",
    "            raise TypeError('Length must be an integer.')\n",
    "            \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(length={self.length})'\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            if index < 0: # handle negative index\n",
    "                index = self.length + index\n",
    "            if index < 0 or index > self.length: # handle out of bound index\n",
    "                raise IndexError\n",
    "            else:\n",
    "                return self.sequence[index]\n",
    "\n",
    "        elif isinstance(index, slice):\n",
    "            start, stop, step = index.indices(self.length)\n",
    "            rng = range(start, stop, step)\n",
    "            return [self.sequence[i] for i in rng]\n",
    "\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = MySequence(10)\n",
    "list(seq), seq[0], seq[-1], seq[1:3],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutation in custom sequences\n",
    "We have seen that mutating a sequence means to change it without creating a new object. Example of mutation can be concatenation or repetition and these can happen also in-place. To add this capability to our custom sequence we need to `overload` the definition of the symbols `+, +=, *, *=` by implementing the methods `__add__` and `__iadd__` or `__mul__` and `__imul__`. \n",
    "\n",
    "Other common methods that we could implement in our sequence are:\n",
    "* `__setitem__` as the complement of `__getitem__`\n",
    "* `__contains__` to add the `in` functionality to check if an element is in the sequence\n",
    "* `__delitem__` to delete elements with the keyword `del`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting sequences\n",
    "Python as a built-in sorting method called `sorted()` with a default ascending order and an optional parameter `reverse=False`. When talking about ordering we have to take into consideration that, excecpt for numbers, it is not trivial to have a ordering criterion for each type of object. An easy example is with strings, that can be ordered lexicographically, but what about lower and upper case? In python for example, the convention is that lower cases come first, i.e. `'a' > 'A'` (ASCII charchters have a code that can be retrieved with `ord(str)`).\n",
    "\n",
    "In some case, when the arguments are not directly comparable because they dont have a natural ordering, we need to create a `sorting key`, ie.e a rule that will help python understand which is the order it has to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,'a', 'x', 'Z', '?', 100, 'A']\n",
    "sorted(l) # the key argument is not provided hence python will try to sort in natural order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_func = lambda x: ord(str(x)) if isinstance(x, str) else x \n",
    "sorted(l, key=l_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting return a copy of the iterable with the sorted elements in a list, use the TimSort algorithm (after Tim Peters, author of `import this` the zen of python). Also, it is a `stable sort`, meaning that if, given a key or not, there is equality in the order of two elements, the one that appear first before the sorting will be the first also after the sorting. Lists object have a `sort()` method that instead is an in-place sorting. They have the same algorithm but the `sorted()` method have a greater overhaed since it has to create a copy of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-based Index\n",
    "\n",
    "Why are python sequence indexed starting with `0` and not `1` ?  \n",
    "\n",
    "Essentially Because:\n",
    "* we want to describe a range of indices using `range(l, u)` with `l <= n < u`\n",
    "* in this way the length (number of elements) is precisely the upper bound of the sequence (`l - u`)\n",
    "* if we want to know how many elements precede the element `s[i]`, in a base-0 system is exaclty `i`\n",
    "* the only starnage behavior is the last index of a sequence which is `len(s) - 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application - `Polygon`\n",
    "Link to [Polygon_Class](Polygon_Class.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists vs Tuples\n",
    "\n",
    "Lets look at the difference between lists and tuples (as immutable data-structure) and in particular why tuples are more efficient and should be used instead of lists if the mutability is an attribute not required.\n",
    "\n",
    "To do this, we have first to definte what `constant folding` is: the process of recognize and evauating constant expressione at compile time and not at run time as computation.\n",
    "\n",
    "To look at the different compilation of lists and tuples we'll make use of the `dis` module that essentially disassemble the steps that the python compiler execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dis import dis\n",
    "\n",
    "# let's compile a tuple and a list and disassemble the process\n",
    "list_dis = dis(compile(\"[1,2,3,'a']\", 'string', 'eval'))\n",
    "tuple_dis = dis(compile(\"(1,2,3,'a')\", 'string', 'eval'))\n",
    "\n",
    "list_dis, tuple_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the huge difference in compilation between tuples and list; the first load only one constant representing all the elements while the latter load one element at the time. In this case both the containers have immutable elements, but if the tuple contains a mutable object (es. a list) then the compilation advantage is lost, since first python build the list and only after the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis(compile(\"(1,2,3,['a'])\", 'string', 'eval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying\n",
    "\n",
    "There is a difference in copying a list and a tuple since one is mutable and the other not. If for example we do a shallow copy of a list, a new object is created, with a tuple instead, since it doesn't make sense for python to have two identical immutable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2,3,4]\n",
    "t1 = (1,2,3,4)\n",
    "l2 = list(l1) # shallow copy\n",
    "t2 = tuple(t1)\n",
    "\n",
    "l1 is l2, t1 is t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Efficiency\n",
    "\n",
    "From Python 3.8 there is not much difference in storing efficiency between tuple and list if the dimension of the final object is known. therefore there is little difference in storage between doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "l = list(range(100))\n",
    "t = tuple(range(100))\n",
    "\n",
    "sys.getsizeof(l), sys.getsizeof(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the final dimension of the sequence is unkown, i.e. we append elements to the list, then the list constructor allocate extra memory when it sees that the size is being filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list()\n",
    "size_prev = sys.getsizeof(l) # to catch the overhead of list creation\n",
    "\n",
    "for i in range(10):\n",
    "    l.append(i)\n",
    "    size_l = sys.getsizeof(l)\n",
    "    delta, size_prev = size_l - size_prev, size_l\n",
    "    print(f' nÂ° item: {i+1}, list size: {size_l}, delta:{delta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration Tools - The itertools module\n",
    "---\n",
    "The itertools module is a collection of lazy iterator functions (i.e. very efficient) that can be very usefull in different situations. In this section we will look to several functions that can leverage the use of the itertools module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregators\n",
    "aggregators are function that iterate over an iterable and return a bulk summary of the its content as a singel value. Example are the functions `max()`, `min()`, `sum()` etc.. \n",
    "\n",
    "The function `any()` and `all()` are usefull agregator that looks at the truth values of the elements inside an iterator. `any()` will return `True` if at least one element evaluates to `True` while `all()` will evaluate to `True` only if all the elements evaulate to `True`.\n",
    "\n",
    "N.B. remember that in Python every object has an associated truth value that by default evaluate to `True`. Only elements like `0`, `''`, `None`, `[]` evaluate to `False`. The truth values can be coded also in a custom class by defininig a specific rule in the `__bool__` method. If the bool method is ont defined, python will look for the `__len__` method, where 0 evaulate to `False`. If neither of the two is defined, the custom object will evaulate to `True` by default.\n",
    "\n",
    "A `predicate` is a function that takes a single argument and return `True` or `False`, like `bool()`, and can be used in conjunction with `all()` or `any()`. Let's say for example that we want to find if all the elements in a list are greater then 0. We could for sure iterate over the whole list, but a more clever way is to apply a predicate to the list and then check the content with `all()`. In this way if, for example there is an element < 0 in the first position, the program won't waste resources iterating over the whole list, saving memory and time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [0, -1, 10, 5, -3, 6]\n",
    "gen = (l >= 0 for l in lst)\n",
    "all(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iSlicing\n",
    "Slicing is the operation of cutting a sequence type object in different ways. The classical notation is composed by up to three terms `[i:j:k]` respectively the start, stop and step parameters. However, with classical slicing is not possible to slice iterables. To do this, we have the `itertools.islice` method.\n",
    "\n",
    "Of course, `islice` is a lazy iterators, that iterate over an iterable and return a lazy evaluated slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "result = islice(lst, 0, 3)\n",
    "result, list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result of `islice` is an iterator, once we have mapped it into a list, it is exahusted, therefore we won't be able to use it again. As a matter of fact, calling `list` again will result in an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Filtering\n",
    "\n",
    "Python has a builting function `filter` that takes an iterable and a predicate and return an iterator. What it doesn under the hood is basically forming a generator expression that loops over the iterable veryfing the predicate on each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1, 2, 3, 4, 5]\n",
    "list((l for l in lst if l>3)), list(filter(lambda x: x>3, lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as expected the two methods are exactly equivalent, both return an iterator that once iterated is exausted. \n",
    "\n",
    "From the itertools module we have some lazy version of the filter function, for example `filterfalse` that will essentially filter the negation of the predicate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import filterfalse\n",
    "lst = [0, '', 'ciao', 1]\n",
    "list(filterfalse(None, lst)) \n",
    "# N.B. if None is supplied as predicate python will look at the truth values of each element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another usefull itertools function for selecting and filtering is `compress` wich takes two iterables, one with the data to be selected and one with a series of selector (values that explicitly evaluates to True or False), adn maps the two. The result is a lazy iterator that contains only the elements in the first iterable that were in the position that evaluated to True in the second one. Better an example than 1000 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "lst = [1, 2, 3, 4 ,5, 6]\n",
    "test = [True, False, None, 0, 1]\n",
    "list(compress(lst, test))\n",
    "# N.B. if the lst has more elemtns than test, the remaining are evalutated to None and therefore discarted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `takewhile` function takes an iterable and a predicates as arguments and returns an iterator that yield values until the predicate evaluates to `True`; when a `False` evalutation is encountered, at that point the iterator is exahusted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import takewhile\n",
    "lst = [1,2,3,5,2,1]\n",
    "list(takewhile(lambda x: x<5, lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when `takewhile` encoutered a predicated that evaluated to `False` (5<5) it stopped yielding eveen if the last elemebts would evaluate to `True`.\n",
    "\n",
    "Similarly `dropwhile` will do the opposite, it will start yield values from the iterable as soon as the predicated evaluate to `False` the first time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import dropwhile\n",
    "lst = [1,2,3,5,2,1]\n",
    "list(dropwhile(lambda x: x<5, lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite iterators\n",
    "From the itertools module we also have a set of infinite iterators that can comes handy. `count` for example  is a function similar to `range`, since we can define a start and a step, but it has no stop parameter. Moreover, start and step can be an numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count, takewhile\n",
    "list(takewhile(lambda x: x < 11,count(10, 0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cycle` function let us iterate over an iterable or an iterator (yes, also an iterator) over and over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "lst = [1, 2, 3, 4]\n",
    "match  = ['a', 'b']\n",
    "for i, j in zip(lst, cycle(match)):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `repeat` instead simply yields the same element indefinitely, or a defined number of time if specified. N.B. the elemetn that is repeated is actually alwasy the same object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "ciao = repeat('ciao')\n",
    "for _ in range(3):\n",
    "    print(next(ciao))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining and Teeing\n",
    "\n",
    "Chaining is essentially the operation of concatenate multiple iterables together, something that we can easily fo with the `+` sign. What `itertools.chain` add is the ability to concatenate iterators lazily. `chain(*args)` takes a variable number of arguments that can be iterable or iterators. However, there is a caveat: imagine we have list `l` containing 3 iterators that we want to chain, if we pass `l` with unpacking we are losing the lazyness since unpacking with `*` is an eager procedure (it requires python to iterate over the object to unpack, and if it is an iterator it will exhaust it). In order to pass directly an iterable there is a dedicated method in the chain module: `itertools.chain.from_iterable`, another lazy iterators!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "l1 = (i**2 for i in range(3))\n",
    "l2 = (i**3 for i in range(3))\n",
    "l3 = (i**4 for i in range(3))\n",
    "\n",
    "list(chain(l1,l2,l3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to use an iterable of iterators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(i**j for i in range(3)) for j in range(2,5)]\n",
    "list(chain(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we get back is a list of the generators created inside `l` and not their chaining.\n",
    "\n",
    "Instead if we use the `.from_iterable` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since iterator are one-time-use it can be beneficial to be able to create copies if we need to use them more than once in our code. The simplest way would be to use a for loop to populate, let's say, an empty list with an arbitrary number of calls of our generator function, however python has a smarter way to do it. The operation is called  \"Teeing\" and is performed by the `itertools.tee` function that will take 2 arguments, an iterable/iterator and the number of time we want to copy it. The copies will be independent object with different memory address.\n",
    "\n",
    "N.B. what come back from `tee` is always an iterator even if the object passed was an iterable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "def square(n):\n",
    "    for _ in range(n):\n",
    "        yield n**2\n",
    "        \n",
    "single_iterator = square(5)\n",
    "\n",
    "multiple_iterators = tee(single_iterator, 5)\n",
    "\n",
    "multiple_iterators\n",
    "# each object in the tuple has a different memory address!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping and Accumulation\n",
    "Mapping is essentially the application of a callable (a function) to every element of an iterable, something that can be easily achieved with the `map` function (return a lazy iterator), a list comprehension or a generator function.\n",
    "\n",
    "Of course, itertools has its own mapping function to work with iterators. `starmap` is similar to map but it is able to unpack every sub element of the iterable and mapping to a function. Moreover, we can pass to `starmap` a function that takes multiple argument, ideally a number equal to the number of elements in the nested iterables. As for every function in the itertools module, what is returned by starmap is a lazy iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import starmap\n",
    "l = [[1,2,3], [3,4,5]]\n",
    "list(starmap(lambda x, y, z: x + y + z, l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulation is a process that reduce an iterable to a single value; es. the `sum` function or more generally the `reduce` function (lazy iterator), which has the advantage to take an arbitrary function to apply to each element of the iterable and also to specify an initializer (see [Partial functions](#Partial-functions))\n",
    "\n",
    "Itertools has a function similar to `reduce` that is called `accumulate` which take an iterable and a function as arguments (it doesn't support an initializer) and returns (lazily) each intermediate result of the accumulation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "lst = [i for i in range(1,5)]\n",
    "\n",
    "reduce(operator.mul, lst), list(accumulate(lst, operator.mul))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipping\n",
    "The classic built-in `zip` function is a lazy iterator that takes an arbitrary number of iterables and return an iterator that produces tuples. If the iterables passed have different length, the shortes will command the lenght of the resulting iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shortest iterable command the iterator length\n",
    "list(zip([1,2], ['a', 'b', 'c'], [10, 20, 30 , 40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we may want to zip on the longest iterable, filling the holes with a predetermined value. To do this we have the `itertools.zip_longest` which takes a variable number of iterators as well, but it let us specify a `fillvalue` (defaulted to `None`) that will serve as a placeholder for the shorter iterables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "list(zip_longest([1,2], ['a', 'b', 'c'], [10, 20, 30 , 40], fillvalue='Filled'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "Sometimes, while iterating over an iterable, let's say a list of tuples, we may need to group the elements based on a specific pattern or a key. To do this there is the function `itertools.groupby` wich takes an iterable, a key function and returns a lazy iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "iterable = [(1, 10, 100), (1, 11, 101), (1, 12, 102),\n",
    "            (2, 20, 200), (2, 21, 201),\n",
    "            (3, 30, 300), (3, 31, 301), (3, 32, 302)]\n",
    "\n",
    "groups = groupby(iterable, lambda x: x[0]) # key function groubing based on the first element of the tuples\n",
    "for group in groups:\n",
    "    print(f'key is {group[0]}')\n",
    "    print(f'resulting in group:\\n{list(group[1])}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling next on the sub iterator `group[1]` will actually consumes the original iterator created upond the iterable passed as argument to `groupby`. However, if we decide to skip iterating, let's say, on the second group of elements, when we call next on the third group, python automatically iterates also over the second in order to return in output the elements of the correct group.\n",
    "\n",
    "N.B. `groupby` will create groups with elements that consecutively have the same key, it doesn't sort the iterbale first, therefore, depending on the need, it migh be needed to pre-sort the iterables before passing it to `groupby`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat: lazy iterators in I/O operation\n",
    "Imagine we want to read a csv file that contains a list of products (brand, product) in rows; we want to group this data by the the brand and we opt to use the `itertools.groupby function`. We can immagine to structure something like this:\n",
    "\n",
    "```python\n",
    "from itertools import groupby\n",
    "\n",
    "with open('my_file.csv') as f:\n",
    "    grouped = groupby(f, lambda x: x[0])\n",
    "```\n",
    "\n",
    "Now we surely would expect to be able to look at the grouped iterator by, for example, castin it to a list:\n",
    "\n",
    "```python\n",
    "list(grouped)\n",
    "```\n",
    "\n",
    "however, what we would have in return is a `ValueError: I/O operation on closed file.`. this is because `groupby` is a lazy iterator and therefore it won't actually evaluate its content untill requested, that in our case is outside the `with` statement, i.e. when the file `my_file.csv` has already been closed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinatorics\n",
    "The itertools module has also some useful functions realted to combinatorics; as a matter of fact we have `permutations`, `combinations` and `cartesian product` of multiple iterables, all returning a lazy iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian Product\n",
    "The cartesian product is the conbination of all the elements in 2 or more sets (don't need to be of the same length). In 2 dimension is something that we do preatty often, and can be achieved easily with a nested for loop, but when the dimension start to increase it can become messy. To easily handle an `n-dimensional` cartesian product we can use the `itertools.product` which takes an arbitrary number of arguments and return lazily their cartesina product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "l1 = [1,2,3]\n",
    "l2 = [1,2,3]\n",
    "\n",
    "list(product(l1, l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutations\n",
    "Statistically speaking, simple permutations are all the possible combinations (without repetition, i.e. no duplicates element are present in the set) of all the elements in a set. Givena set of dimension `n` the toal number of permutation is given by `n!`. To performe a permutation in python we can rely on the `itertools.permutations` function wich takes as argument an iterable and, optionally, the length of the permutation. There is a caveat thou, since all the elements in an iterable, say a list, are distinct objects even if they have the same value; this means that, in case of duplicated values in an iterable, the permutation will contains an apparent repetition, which in reality is not because the object underlying is being switched in potition generating *de facto* a new permutation. **Elements are unique based on their position, not their value!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "string = 'abc'\n",
    "\n",
    "list(permutations(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but, be careful, uniqueness is given by position not values, therefore if we a duplicate value in a different position, this result in a new object, therefore, not counting as a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'aba'\n",
    "\n",
    "list(permutations(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations\n",
    "Unlike permutations, combinations don't care about the order of the elements (i.e `'ab'=='ba'`). Combinations can be defined `without replacement`, meaning that once an element is picked from a set it cannot be picked again, and `with replacement`. To perform combinations in python we can use the `itertools.combinations` or i`tertools.combinations_with_replacement`; both functions take an iterable and an the optional length of the combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "l1 = [1,2,3]\n",
    "\n",
    "list(combinations(l1, r=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "l1 = [1,2,3]\n",
    "\n",
    "list(combinations_with_replacement(l1, r=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Managers PEP 343\n",
    "----\n",
    "The definition of 'Context' in Python is the `the state surrounding a section of code`, we can think it as the `scope` in wich the code is referencing its variable while running. A classical example is the `with` statement used while, for example, open a file; in this case we create a `context` that it is specifically design to handle the file, we `enter` the context opening the file, we `work` in the contex while manipulating the file and we `exit` the context closing the file (escaping the 'with' indentation).\n",
    "\n",
    "```python\n",
    "with open('file.txt', 'r') as f:  # Entering the context\n",
    "    print(f.readlines()) # Working in the context\n",
    "print('We are out of the context') # Exiting the context\n",
    "```\n",
    "\n",
    "We could have use a `try-except` expression to handle possible error inside the context 'file-handling' but this is a much cleaner approach.\n",
    "\n",
    "We can resume the context manager as the process to manage data in our scope giving an on-entry/on-exit functionality (open and closing file in the example above). Other classical example are querying a database, set a tolerance and reset it back, lock and release a thread etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try..Except..Finally\n",
    "Let's first look at the most direct way to create a context manager, i.e. a section of code that provides entering and exiting functionality. By this mean, the most important part is given by the `finally` statement since it is always excuted even if an exception is raised (i.e. it is our `on-exit` event). However, handling each time a try-except statement can become cumbersome and messy, therefore there is a better way to handling this process.\n",
    "\n",
    "Basically, the pattern we want to reproduce is:\n",
    "* create an object\n",
    "    * work with the object\n",
    "* clean up the object after the work is done (we want to do this automatically!)\n",
    "\n",
    "The PEP 343 introduces the `with` statement, the keyword that is used to enter a context manager:\n",
    "\n",
    "```python\n",
    "with open('file.txt', 'r') as f:\n",
    "    ''' ENTERING THE CONTEXT\n",
    "    'with ... as f' are the keyword that allows us to enter the context manager\n",
    "    created by 'open()'. 'as f' is optional and returns and object from the context manager\n",
    "    (an alias to for the 'filename')\n",
    "    '''\n",
    "    print(f.readlines()) # Working in the context\n",
    "    ''' WORKING IN THE CONTEXT\n",
    "    We are in the 'with' block, inside the context, where we can manipulate the object 'f' (optional)\n",
    "    '''\n",
    "    \n",
    "print('We are out of the context') # Exiting the context\n",
    "''' EXITING THE CONTEX\n",
    "without the need of any 'f.close', when we exit the indentation of the block, the context manager\n",
    "is outomatically closed, and its reference cleaned up\n",
    "'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The context management protocol\n",
    "Has we have seen for iterators, the context management is nothing more than a protocol, meaning that we can implement our custom class that implements the context management protocol by providing two special function: `__enter__` which handles the setup, and `__exit__` which handles the cleanup. Let's try to understand step-by-step what's happening:\n",
    "\n",
    "```python\n",
    "with CtxManager() as obj: ''' An instance of the CtxManager is created (it is called with open-close parenthesis).\n",
    "                              This is equivalent to say '''\n",
    "                              manager = CtxManager()\n",
    "                          ''' but instead we dont have a variable name assign to the call, and we actually don't need it.\n",
    "                              The 'with' statement call the '__enter__' method on the context manager and if 'as obj' is\n",
    "                              specified (it is optional) it is used as variable:'''\n",
    "                              obj = manager.__enter__()\n",
    "                          ''' Then we need to handle the work we will do inside the context, beign sure that we reach the\n",
    "                              exit condition even if an exception is encoutered. this traduces in a try-finally statement\n",
    "                              under the hood '''\n",
    "                              try:\n",
    "                                # do something\n",
    "                              except:\n",
    "                                # possible handling of some exception\n",
    "                              finally:\n",
    "                                # we are done with the context\n",
    "                                manager.__exit__()\n",
    "```\n",
    "\n",
    "The `CtxManager` in the example above is simply a class that implements the context management protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        # print('initialization')   \n",
    "        pass\n",
    "    def __enter__(self):    \n",
    "        return obj\n",
    "    def __exit__(self, *args):\n",
    "        # print('clean up obj') \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is equivalent to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj = MyClass()\n",
    "\n",
    "with MyClass() as obj:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that in the second case, we don't have a direct handlying to the instance of Myclass() that is created (in the first case is the variable 'my_obj'), but no worries, python has! \n",
    "\n",
    "The `with` look for the `__enter__` mehtod inside MyClass and if founded, it calls it `manager.__enter__()` ('manager' id just a fictitious name to indicate the instance of MyClass created under the hood by the `with` statement).\n",
    "\n",
    "Whetever is returned by the `__enter__()` method is assigned to `obj` (if `as obj` is provided). N.B. `obj` is not the instance of MyClass that was created by the `with` statement! (actually it can be but only if the `__enter__` method return `self`, i.e. the instance itself of the context manager we hve just entered.\n",
    "\n",
    "Whetever we exit the `with` block, or an exception occur, the MyClass `__exit__` method is called (`manager.__exit__()`), as it would happen with `finally` in a try-except."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'with' Scope\n",
    "The with block, unlike funciton or list comprehension, has not its own scope. It lives in the scope where it is running, local if it inside a function or global otherwise, and the same apply to the object is returned by the `__enter__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \\_\\_enter\\_\\_ mehtod\n",
    "The enter method is preatty straigthforward, it just need to handle what we need to happen when we enter the context manager, and optionally it can return an object (the one referred in the with statement as 'as obj')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \\_\\_exit\\_\\_ mehtod\n",
    "The exit method is similar to the `finally` statement in a try-except block, it has to be executed always even if an exception occur, but more than that; we may want to be able to handle whichever exception has been raised and modify on its basis the behavior of the `__exit__` method. Therefore, the `__exit__` method needs to knwo which exception occurred and has to tell python if he has to silence it (the program keeps running) or let it propagate (the exception is raised and the program interrupted).\n",
    "\n",
    "The exit method needs 3 argument:\n",
    "* the esception type that occurred (if any, None otherwise)\n",
    "* the exception value that occurred (if any, None otherwise)\n",
    "* the traceback object if an exception occurred (if any, None otherwise)\n",
    "\n",
    "It has to return `True` or `False`:\n",
    "* True if the exception raised must be silenced\n",
    "* False the exception has to be raised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __exit__(self, exc_type, exc_value, exc_trace):\n",
    "    # do the cleanup\n",
    "    return True # silence eventual exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Manager Class\n",
    "Now, let's try to build a context manager class, i.e. a class that implement the `__enter__` and `__exit__` method, with some print statement that will help us understand how the process is handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtXManager:\n",
    "    def __init__(self):\n",
    "        self.obj = None\n",
    "        print('Ctx initialized...')\n",
    "        \n",
    "    def __enter__(self):\n",
    "        print('__enter__: entering context...')\n",
    "        obj = 'Obj returned by __enter__'\n",
    "        return obj\n",
    "    \n",
    "    def __exit__(self, exception_type, exception_value, exception_traceback):\n",
    "        print('__exit__: exiting context...')\n",
    "        if exception_type: # if an exception is raised\n",
    "            print(f'An exception has been raised and handled by __exit__')\n",
    "            print(f'Exception type: {exception_type}, Exception value: {exception_value}')\n",
    "        return False # False tell python to not silence the exception       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use equivalently two approaches: initialize the CtXManager class and the entering the context manager with the `with` statement, or directly call the `with` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = CtXManager()\n",
    "print('***Context Ready***')\n",
    "with ctx as obj:\n",
    "#with CtXManager() as obj:\n",
    "    print(f'Entering the with block with the {obj}...')\n",
    "    raise ValueError('A dummy error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat with Lazy Iterator\n",
    "Care must be taken when returning a lazy iterator from a context manager, for example when reading a file. The reason is that, if we don't `yield from` the ctx manager (in this way we dont exit the with block until the generator is exhausted) but we simply `return` an object, and the aim is to use that object in a second stage outside the with block, we will get an error a `ValueError: I/O operation on a closed file`. This is because, once outside the ctx manager, the exit method is called and therefore the file is closed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional use of Context Manager\n",
    "Context Managers can be used for a lot more stuff than just opening and closing files. Folloqing some example of application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redirect Standard Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class OutToFile:\n",
    "    def __init__(self, fname):\n",
    "        self._fname = fname\n",
    "        self._curren_stdout = sys.stdout\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self._file = open(self._fname, 'w')\n",
    "        sys.stdout = self._file\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        sys.stdout = self._curren_stdout\n",
    "        self._file.close()\n",
    "        return False        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter, sleep\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.elapsed = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = perf_counter()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.stop = perf_counter()\n",
    "        self.elapsed = self.stop - self.start\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Manager Decorator\n",
    "One of the driver that pushed the Python community do develope the context management constructor was the ability to handle the context of generator functions. Essentially what we need to create a context manager from a generator function is to trap the entering and exiting of the context with a tyr-finally , where 'try' has the duty to yield values from the generator and 'finally' has to clean up the context. We can than build a context manager class that takes as argument an instance of our generator function and implementxs the context manager protocol; on enter we will call `next` on the generator to open the file, on exit we need another try-except to call again next to exhaust the generator, therefore closing the file, and trap the `StopIteration` exception that the generator would naturally raise. This is it, we are able to use a generator with a proper context manager. To simplify the process, python has a built-in standard library `contextlib.contextmanager`, which is a decorator that turns generator functions into context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def open_file_gen(f_name): # our generator function\n",
    "    f = open(f_name)\n",
    "    try:\n",
    "        print('yielding...')\n",
    "        yield f\n",
    "    finally:\n",
    "        print('closing file...')\n",
    "        f.close()\n",
    "        \n",
    "# now we can use the generato function as a context manager\n",
    "with open_file_gen('nyc_parking_tickets_extract.csv') as gen:\n",
    "    for _ in range(3):\n",
    "        print(next(gen).strip('\\n').split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Context Manager: ExitStack\n",
    "We can easily manually nest multiple context managers simply keeping the indentation growth, and this is totally fine when the number of context manager is known and small, but what if this is not the case? The `contextlib.ExitStack` is the built-in python class that allows us to handle a variable number of context managers. Basically, `ExitStack` on enter return itself and with a dedicate function stores the `exit` method of each context manager passed. On exit it simply call the exit methods stored in reversed order (to close the context from the most inner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager, ExitStack\n",
    "f_names = 'file1.txt', 'file2.txt', 'file3.txt'\n",
    "\n",
    "@contextmanager\n",
    "def create_and_destroy(f_name):\n",
    "    f = open(f_name, 'w')\n",
    "    try:\n",
    "        f.write('Hello')\n",
    "        yield f\n",
    "    finally:\n",
    "        print('Destroy the secret!')\n",
    "        f.close()\n",
    "        os.remove(f_name)\n",
    "           \n",
    "with ExitStack() as stack:\n",
    "    files = [stack.enter_context(create_and_destroy(f)) for f in f_names]\n",
    "    print('working in all the files at once...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings\n",
    "---\n",
    "\n",
    "Strings are immutable object of the sequence type, therefore they have indices and can be used as an iterator. String are an homogeneous containers with fixed length and order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods\n",
    "- isalpha() -> check if is alphanumeric\n",
    "- isprintable() -> check if is printable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lists\n",
    "---\n",
    "List are mutable object 0f the sequence type... (to be extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List comprehension\n",
    "The goal of list comprehension is to **generate a list by `tranforming`, and optionally `filtering`, another iterable**.\n",
    "\n",
    "Like a function, list comprehension have a localscope (what is inside the sqaured brackets is essentially the body of a function) but they can freely access also the globalscope. As a matter of facts, when python compile the list comprehension rhs, it creates a temporary function which is essentially the equivalent of a for loop (with eventual if statements). This also implies that, after the excution, the varibale inside the local scope are deleted and can't be retrieve in the global namespace.\n",
    "\n",
    "We can try to disassemble a list comprehension to see whats happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dis import dis\n",
    "\n",
    "compiled = compile('[i**2 for i in (1,2,3)]', filename='string', mode='eval')\n",
    "dis(compiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, python is create a function `4 MAKE_FUNCTION` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehension can be nested one inside the other, creating closures betwen themself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[i*j for i in range(5)] for j in range(3)]\n",
    "lst "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehension can have as many nested for loop as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            l.append((i,j,k))\n",
    "            \n",
    "            \n",
    "l_1 = [(i, j, k) for i in range(2) for j in range(2) for k in range(2)]\n",
    "\n",
    "print(l)\n",
    "print(l_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the order in the for loop is the same in the list comprehension\n",
    "\n",
    "We can also add if statement inside the for loop and of course the order matter! They have to be referenced after the for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        if i == j:\n",
    "            l.append((i,j))\n",
    "            \n",
    "l_1 = [(i, j) for i in range(2) for j in range(2)  if i == j]\n",
    "\n",
    "print(l)\n",
    "print(l_1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add an else statement, but in this case the `if..else` statement must come before the for loop. since it acts like a filter for the for expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        if i == j:\n",
    "            l.append((i,j))\n",
    "        else:\n",
    "            l.append('else')\n",
    "            \n",
    "l_1 = [(i, j) if i == j else 'else' for i in range(2) for j in range(2)  ]\n",
    "\n",
    "print(l)\n",
    "print(l_1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_1 = [(i, j)  for i in range(2) for j in range(2) if i == j else 'else' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuples\n",
    "---\n",
    "Tuple are known as immutable list, they are object of the sequence type, therefore they have indices and can be used as an iterator. Tuples can be homogeneous or heterogeneous containers. Together with immutability, in comparison with lists, the main difference are that tuples have a fixed length and a fixed order (cannot be im-placed sorted or reversed like lists).\n",
    "\n",
    "Due to this property, we can think of tuples as data records, where the position of the data, one define have a precise meaning:\n",
    "\n",
    "```py\n",
    "# Circle(x, y, radius)\n",
    "circ1 = (0, 0, 10)\n",
    "```\n",
    "Once we have define the structure of our container, tuples can be used to store data efficiently, since once created we are sure that nobody will be able to accidentally modify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Tuples\n",
    "Sometimes, defining a custom class can be an excessive effort if what we want to achieve is simply storing data in a custom data structure. Classes require at least some methods to be implemented to be property employed in our code (such the `__eq__` and `__repr__` method for example); moreover the instance of a class is not immutable and can lead to potential errors. On the opposite, plain tuples are immutable and well opt to store data, but accessing properties with indices can be troublesome for the user or even for other developer to read. \n",
    "\n",
    "```py\n",
    "# Class vs tuple approach\n",
    "class Person:\n",
    "\n",
    "    def __init__(self, name, age)\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person('Luca', 44)\n",
    "p1.name, p1.age # 'Luca', 44\n",
    "\n",
    "p1 = ('Luca', 44)\n",
    "name = p1[0]\n",
    "age = p1[1]\n",
    "```\n",
    "\n",
    "Of course, python as a perfect solution to this kind of problem, i.e. named tuples. `namedtuples` are functions that comes shipped in the `collenctions` standard library; they are a subclass of the `tuple` type but they are not a type them self. Instead, namedtuple is a function that generate a new class (`class factory`) which can assign property names to positional elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Pt2D = namedtuple('Point2D', ['x', 'y'])\n",
    "\n",
    "# Pt2D is a variable alias of the class `Point2D` generated by the class factory namedtuple\n",
    "\n",
    "pt = Pt2D(x=10,y=20)\n",
    "pt, pt.x, pt.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we call the `Pt2D` functions, python is using the `__new__` method of the `Point2D` class to create a new instance of that object and return the tuple.\n",
    "\n",
    "There are several ways in which we can pass the arguments to the namedtuple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pt2D = namedtuple('Point2D', ['x', 'y']) # list\n",
    "Pt2D = namedtuple('Point2D', ('x', 'y')) # tuple\n",
    "Pt2D = namedtuple('Point2D', 'x, y') # comma separated strings\n",
    "Pt2D = namedtuple('Point2D', 'x y') # whitespace separated strings\n",
    "\n",
    "# and remember, namedtuple are subclasse of the tuple type\n",
    "pt = Pt2D(x=10,y=20)\n",
    "isinstance(pt , tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from a class instance, since `pt` is a tuple, it is immutable, i.e. we cannot modify its attribute (in a class object we could)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.x = 100 # cannot do! it is a tuple -> immutable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B namedtuple arguments name CANNOT contains underscore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pt2D = namedtuple('Point2D', 'x _y') # ERROR!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unless we provide the keyword `rename=True`, in which case the namedtuple will convert the erroneous name into the positional number of the argument preceded by an underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pt2D = namedtuple('Point2D', 'x _y', rename=True)\n",
    "pt = Pt2D(10, 20)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspection\n",
    "The namedtuple generated classes that are shipped with some methods that can help ud in the introspection of our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pt2D._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt._asdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify and Extending\n",
    "Namedtuple are immutable in essence but they come shipped with some methods that helps us handling arguments substitutions and extending. Basically the original tuple is overwritten and associated to a new memory address. Looking to Pt2D as example, we can modify its parameters with the method `_replace()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt right now is Point2D(x=10, _1=20)\n",
    "pt = pt._replace(x=50)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, if we want to extend the namedtuple, adding more argument we can use the the `_fields` property of the existing namedtuple (which is a tuple), adding the element/s we want and create the new namedtuple with extended fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pt2D = namedtuple('Point2D', 'x, y')\n",
    "old_fields = Pt2D._fields # is a tuple\n",
    "new_fields = old_fields + ('z', ) # concatenation of two tuple\n",
    "Pt3D = namedtuple('Point3D', new_fields) # equal to say Pt3D = namedtuple('Point3D', (x,y,z)) \n",
    "pt = Pt3D(10, 20, 30)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docstring\n",
    "The namedtuple is shipped with a set of precompiled docstrings that can be access as always with the `help()` or the `__doc__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Pt3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults values\n",
    "When we create a namedtuple, we cannot specify default arguments. One way to circumvent this would be to define an instance of the namedtuple (e.g. setting all parameters to 0) and then use the `._replace()` method to specify only those arguments that we want to have a default value. Alternatively we can use the `__defaults__` method (it can be use in the same way on any function). To use this last approach on the namedtuple we first need to create a new instance of the class using the `__new__` method and then call the `__defaults__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `__defaults__` on a generic function\n",
    "def func(x, y, z):\n",
    "    print(x, y, z)\n",
    "\n",
    "func.__defaults__ = (10, 20) # N.B. The replacement starts from the last parameter\n",
    "func(x=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the Pt3D function setting a default values fro the argument z\n",
    "Pt3D.__new__.__defaults__ = (0,)\n",
    "pt = Pt3D(x=10, y=10)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associative arrays\n",
    "Associative arrays are abstract data structure that can be implemented in different way but that shares the concept of a `collection of key-value pairs`. Dictionaries are associative arrays and in python they have extra functionalities but, in general, to be and associative arrays the data structure should implement:\n",
    "* adding/removing elements\n",
    "* modifying elements\n",
    "* looking up values via key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Maps\n",
    "Hash maps are one of the possible implementation of associative arrays. They are based on the concept of `hash function` that is a mathematical expression that maps from a set of arbitrary size (thoretically infinite) to another smaller set of fixed size. Moreover we want the output of the has function to be as equally distributed as possible along the dimension of the hash table (a table that associate to each slots a key, that will be the result of the hash function). This is to avoid collisions, meaning that the result of the hash function is an already occupied key. this problem is directly related to the initial size of the hash table: as a matter of fact, if the table is very large we will have no problem of collisions but we will have a huge memory constrain on a data-structure that maybe in only partially in use; on the opposite we could choose to start with a small table and then resizing at need, but resizing is an expensive operation since we have to recompute the hashes and move data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dictionaries implementation (PEP 412)\n",
    "In Python, dictionaries are probably the most importatn data structure, since most of the objects in it are related to dictionaries type (namespaces, classes, modules, functions, sets ..).\n",
    "\n",
    "From Python 3.6 we had a major modification in dictionaries implementation that, as  side effect, also preserv the order of insertion of the key-value pairs. The details and the motivation aroud its implementation can be found in PEP 412."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python `hash()`\n",
    "Python has ofc a built-in hash function that always return an int of fixed size (depending on the operating system and the python version you aree running. If two variable holds the same value (i.e a == b is True) then also their hash comparison is also True. However, the hash of the same values can differ from one round to the other (they might but it is not guaranteed, for security reasons that I dont know ;D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.hash_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not everything is hashable in python, for example list and dictionaries are *not* hashable (because they are **mutable objects**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Python 3.6, dictionaries are ordered hasmaps, containing (key,values) pairs. To be noted that while the order of insertion is retained now, if you print a dictionaries, it will be sorted in lexicografical order, therefore dont count on that!\n",
    "\n",
    "Before 3.6, to have an ordered dictionary we would have used the `collections.Orderdict`. this is not completely surpassed since it has some usefull functionalities like `move_to_end` and `popitem(last=False)` to move/pop a key in front or at the end of the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionaries\n",
    "As a recap, dictionaries are associative arrays that therefore rely on the `key-value` pairs structure. The `value` can be any king of python object while the `key` has to be hasable, therefore, it has to be a constant, i.e. an immutable object (list and sets are not hashable for example). More in general an object is hashable if it returns an integer values and if two object compare to equal (==) also their hashes must be equal.\n",
    "\n",
    "Following a list of ways to create a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the curly brakets notation keys can be any hashable object\n",
    "dict1 = {'key1':1, 222:2, print:1, (0,1):tuple}\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case keys must be a vaid identifier, like in functions or class names\n",
    "dict2 = dict(key=1, john=list, man=5)\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict comprehension\n",
    "dict3 = {i:i**2 for i in range(5)}\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fromkeys, passing an iterable\n",
    "dict4 = dict.fromkeys(['a', (0,0), list], 'whaat?')\n",
    "dict4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common operations with dictionaries\n",
    "Given a dictionary `d` we can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'key1':1, 222:2, print:1, (0,1):tuple}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d[key] = value` - create a key-value pair if the key doesnt exist or replace its values if is already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['key2'] = 'new!'\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d[key]` - request the associated value to `key`, if `key` is not in the dictionare we will have a `KeyError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['key3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d.get(key, default)` - to avoid the `KeyError`; if `default` is not specified return `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get('key3', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d.pop(key)` returns and delete the value if the key exist, if not return `KeyError` unless a default is specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.pop('key3', 'Not Found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d.popitem()` retunr the last element inserted and remove the key-value pair in the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.popitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`d.setdefault(key, value)` test if a key exist, if not insert the `value` and return it. If the Key already exist, returns its current value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = d.setdefault('aaa',dict)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary view PEP 3106\n",
    "There are essentially three ways of getting a view of dictionaries in Python:\n",
    "* `d.keys()` - return the keys of the dict; it behave like a set since key must be unique\n",
    "* `d.values()`- return the values of the dict\n",
    "* `d.items()` - return the key-value pairs as tuple; it behave like a set since the pairs are unique\n",
    "\n",
    "Behaving like sets, `keys()` and `items()` can be manipulated with sets operation like intersection, union etc.. But be aware! while dictionaries preserve indertion order, and therefore also their views, sets don't meaning that after permorming a set operation on a dictionary view the order is no more guaranteed.\n",
    "\n",
    "Another important feature of modern dictionaries implementation is that the views are dynamic, meaning that if we store, let's say the keys of a dictionary into a variable with `key = d.keys()`, and later on the keys of the dictionary change, then also the view change.\n",
    "\n",
    "At last, if we can we should always iterate over the view of a dictionary since it is way faster that do the hash look up each time we require the value associated to a key (e.g. `d[key]`, instead is better `for k, v in d.itmes():`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Class and Hashing\n",
    "Python has a default behavior when calling the hash() function on an object: it looks for the `__hash__` method and if not defined it will hash based on the `id` of the object (this, in cascade will produce euqality on id, ==, and hash). This leads in a prticular situation to a somehow unwanted behavior.\n",
    "\n",
    "Imagine we have two instances of a class `Person` that represent actually the same person; we want to insert one of these in a dicitonary as a key and be able to retrive its value calling eithre of the two instances, since for us they represent the same person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "p1 = Person('Gio')\n",
    "p2 = Person('Gio')\n",
    "\n",
    "p1 == p2, p1 is p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, niether of the comparison evaluates to True, therefore neither their hash evaluates to True.. leading to.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {p1:10}\n",
    "d[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[p2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a `KeyError` as expected since it is not respected the hash equality property between `p1` and `p2`. \n",
    "\n",
    "However, in our aim this is unwanted, for us `Gio` is the same person, therefore the two instance of the class should be seen as the same object by the dictionary. What we can do to tweak this behavior is to implement in the class person the  `__eq__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Person):\n",
    "            return self.name == other.name\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "p1 = Person('Gio')\n",
    "p2 = Person('Gio')\n",
    "\n",
    "p1 == p2, p1 is p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, even if the two object are still different in `id`, they can be compared with `==` according to the `name` property. Probelm solved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {p1:10}\n",
    "d[p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! we get a new error; now the class `Person` is unhashable, this because when implementing the `__eq__` method, python disable the default hash protocol (ofc.. it is based on the object `id`). So what to do?\n",
    "\n",
    "By the way, if we want explicitly to tell python that our object is not hashable we can set the `__hash__` attribute to `None` and since returning an integer is a ,mandatory proprerty of an has function, python will understand that the object cannot be hashed. An this is exaclty what python does when we define only the `__eq__`  method: it sets `__hash__ = None` in the class.\n",
    "\n",
    "What we need to do is defining our custom hash method for the class and make it return what is appropriate for our purpose. In the case of our `Person` class, we just need to hash the name attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Person):\n",
    "            return self.name == other.name\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # __hash__ = None -> if we want the class to be non-hashable\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "p1 = Person('Gio')\n",
    "p2 = Person('Gio')\n",
    "\n",
    "p1 == p2, p1 is p2, hash(p1) == hash(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use p1 and p2 as the same key for our dictionary since they are equal and have the same hash. From a dictionary prespective, p1 and p2 are exaclty the same even if they are differente instance of the same class, i.e. different objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {p1:10}\n",
    "d[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[p2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaultdict\n",
    "\n",
    "Defaultdicts solves the problem of getting a `keyError` back when trying to access a key in a dictionary that doesn't exist. In reality, there is already a built-in way to do this but not sistematically, and can get messy in a bigger code, i.e. using the `get(key, default)` method of diciontaries. However we need to specify the get each time we do a key search.\n",
    "\n",
    "The pythonic way to do it is to use a specilized dictionaries: `collections.defaultdict`. defaultdict is a subclass of `dict` and is defined with a callable as argument, i.e. a function that gets called whenever the default value is required. The callable has to take zero arguments and if not specified will return to `None`. Whiele the get method just returns a default values that can be eventually stored in the dictionary, the default dict automatically create an entry for the not-found key with the default values specified in the callable.\n",
    "\n",
    "The callable function doesn't need to be deterministic, it could be an API call that returns a particular value in different situations; the importat thing is that it need to takes no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(lambda: 'notFound')\n",
    "d.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('test', 'notFound')])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['test']\n",
    "d.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, an example from Fred of how to leverage the poiwer of defaultdict. Essentially he creates a function that contains a decorator that create a defaultdict that stores the number of tiume and the utcnow() of the first call of a function... amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.function_stats.<locals>.decorator(fn)>,\n",
       " defaultdict(<function __main__.function_stats.<locals>.<lambda>()>, {}))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "\n",
    "def function_stats():\n",
    "    d = defaultdict(lambda: {'count':0, 'first_called': datetime.utcnow()})\n",
    "    Stats = namedtuple('Stats', ['decorator', 'data'])\n",
    "    \n",
    "    def decorator(fn):\n",
    "        @wraps(fn) # decorator for storing the metadata \n",
    "        def inner(*args, **kwargs):\n",
    "            d[fn.__name__]['count'] +=1\n",
    "            # if the function already exist in d, it only update the count,\n",
    "            # otherwise it stores also the utcnow()\n",
    "            return fn(*args, **kwargs)\n",
    "        return inner\n",
    "    \n",
    "    return Stats(decorator, d)\n",
    "\n",
    "\n",
    "stats = function_stats()\n",
    "stats.decorator, stats.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@stats.decorator\n",
    "def func1():\n",
    "    pass\n",
    "\n",
    "@stats.decorator\n",
    "def func2():\n",
    "    pass\n",
    "\n",
    "func1()\n",
    "func1()\n",
    "func2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function function_stats.<locals>.<lambda> at 0x7ffa16dd91b0>,\n",
      "            {'func1': {'count': 2,\n",
      "                       'first_called': datetime.datetime(2022, 6, 17, 11, 7, 49, 680927)},\n",
      "             'func2': {'count': 1,\n",
      "                       'first_called': datetime.datetime(2022, 6, 17, 11, 7, 49, 681008)}})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(stats.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counters\n",
    "\n",
    "One common application of dictionaries is to use them as counters, and we have seen how can we solve the problem of missing keys with the `get()` method or leveraging the `defaultdicts` module. However, if we have multiple diciontaries this can become tedious, therefore pytho has a specialized dictionary for this case: `collenctions.Counter` the python implementation of `Multi-Sets` (?).\n",
    "\n",
    "So the `Counter` is a specialized dictionary that:\n",
    "* acts like a defaultdict with default = 0\n",
    "* is a subclass od dict and therore inherits most of its class methods (not 'fromkeys', and 'update' become an inplace addition to the count\n",
    "* has additional functionality to generate `frequency tables`, i.e. count how many time a key appears\n",
    "* supports addition, subtraction and sets operations betweeen Counters object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 3, 'b': 2, 'c': 1, 'd': 3, 'f': 2, 'r': 2})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c1 = Counter('aaabbcdddffrr')\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('d', 3), ('b', 2), ('f', 2), ('r', 2), ('c', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.total()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChainMap\n",
    "\n",
    "ChainMap is a module from the `collections` package that works similarly to the `itertools.chain` module, but specialized for dictionaries. Essentially it creates a view (no storage, required, nothing new is created when chaining) of the combination of multiple dictionaries. Unlike the `dict.items()` view, a chainmap sees chnges in the underlying dicts and can also be updated.\n",
    "\n",
    "While a classical `chain` returns a lazy iterable, the `chainmap` returns a map, therefore keys must be unique. So, if the chained dictionaries shares a key, the chainmap will view only its first appereance (i.e. the order of chaining is very important). N.B. the opposite happens when unpacking two dictionaries to create a combination of the two, there the last instance of a key is retained (d3 = {\\*\\*d1, \\*\\*d2})\n",
    "\n",
    "**N.B. the key ordering in a chainmap is not guaranteed, therefore, unlike standard dictionaries (post 3.6) we can't rely on insertion order**\n",
    "\n",
    "Chainmap are usually thinked of as `Parent-Child` relationship, where the child is the first element to be chained and the parents all the others. this is because the child is the only one assured to have all its keys retained, while cascading, the parents might have their keys overrided. With this structure in mind, chainmpas have ad hoc methods line `d.parents` to select only the parents oin the chainmap or `d.new_child(d1)` to add a new dictionary in front of the others as the new child.\n",
    "\n",
    "To modify a chainmap, the best way it to use the `d.maps` method which returns a `mutable list` of the dicionaries in the chain (retaining the child-parent order). The `d.maps` being a mutable list has the methods `append`, `insert` that mutate chaimap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChainMap({'a': 10, 'b': 100, 'c': 30}, {'e': 10, 'b': 5, 'f': 60}, {'a': 1000, 'e': 3, 'l': 300})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = dict(a=10, b=100, c=30)\n",
    "d2 = dict(e=10, b=5, f=60)\n",
    "d3 = dict(a=1000, e=3, l=300)\n",
    "\n",
    "d = ChainMap(d1, d2, d3)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first occurence of the key \"a\" is 10\n"
     ]
    }
   ],
   "source": [
    "print(f'The first occurence of the key \"a\" is {d[\"a\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 10, 'b': 100, 'c': 30},\n",
       " {'e': 10, 'b': 5, 'f': 60},\n",
       " {'a': 1000, 'e': 3, 'l': 300}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also modify the source dictionaries from the chaimap, for example associating a new value to an existing key, however **the changes will affect only the child map**, therefore if a key is updated but is in a parent maps, a new entry in the child will be created. Similarly, if we call a `del` on a key, it will only look in the child map, and if there is another appereance of that key in a parent maps, it will be retained; moreover, if we try to delete a key that is in a parent maps but not in the child, we will get a `KeyError` exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChainMap({'a': 10, 'b': 100, 'c': 30, 'l': 1}, {'e': 10, 'b': 5, 'f': 60}, {'a': 1000, 'e': 3, 'l': 300})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['l'] = 1 # the changes happen in the child map!\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cool usecase of chainmaps could be to store a reference immutable dictionary (like a set of default options), and chain it with an empty dictionary that will be the *user_configuration*.. es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChainMap({}, {'channel': 5, 'volume': 55, 'user_name': 'admin'})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_config = {'channel':5, 'volume':55, 'user_name':'admin'}\n",
    "user_config = ChainMap({}, default_config)\n",
    "user_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChainMap({'volume': 70, 'user_name': 'Giovanni'}, {'channel': 5, 'volume': 55, 'user_name': 'admin'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_config['volume'] = 70\n",
    "user_config['user_name'] = 'Giovanni'\n",
    "user_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UserDict\n",
    "\n",
    "Sometimes we may want to have a dictionary with specific usecase restrictions, like accepting only certaing keys, limiting the values to a certain range etc.. \n",
    "\n",
    "We could implement our own class, specifying the `__getitem__` and `__setitem__` methods but we won't inherit the whole functionality of dicts. Moereover, even if we subclass the `dict` class, some of its special methods, like `.get` are called from the higly optimize C library and therefore there is no guarantee that our redefined method will be used (e.g. if we redefine the `__len__` method and we call `len(string)` python won't use `__len__`, instead, under the hood it will find the reference to the C-wrtitten function optimizew to find the length of an array.\n",
    "\n",
    "This is why the `collections.UserDict` was implemented, which is not a subclass of dict but use dictionary as a backing data stucture (e.g. items(), keys(), values()). Here we are sure that our specilized method in the custom dict class should work as expected.\n",
    "\n",
    "Let's see an example where we build a class that stores RGB colors, therefore the keys can only be 'R' or 'G' or 'B' and the values must span between 0 and 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R': 0, 'G': 0, 'B': 0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import UserDict\n",
    "\n",
    "class LimitedDict(UserDict):\n",
    "    def __init__(self, keyset, min_value, max_value, *args, **kwargs):\n",
    "        self._keyset = keyset\n",
    "        self._min_value = min_value\n",
    "        self._max_value = max_value\n",
    "        # for the rest use the standar implementation of UserDict\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __setitem__ (self, key, value):\n",
    "        if key not in self._keyset:\n",
    "            raise KeyError('Invalid Key name.')\n",
    "        if not isinstance(value, int):\n",
    "            raise ValueError('Value must be integer type')\n",
    "        if value < self._min_value or value > self._max_value:\n",
    "            raise ValueError(f'Values must be between {self._min_value} and {self._max_value}')\n",
    "        # once the exception are stated, leave to the super class the duty to insert the element\n",
    "        super().__setitem__(key, value)\n",
    "\n",
    "rgb = LimitedDict(set('RGB'), 0, 255, R=0, G=0, B=0)\n",
    "rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Y'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35187/2843379146.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrgb\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Y'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/lib/python3.10/collections/__init__.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"__missing__\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1105\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__missing__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1106\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1108\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__setitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Y'"
     ]
    }
   ],
   "source": [
    "rgb['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Values must be between 0 and 255",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35187/2432782120.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrgb\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'R'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m300\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_35187/2528293951.py\u001B[0m in \u001B[0;36m__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m     15\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Value must be integer type'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_min_value\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_max_value\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Values must be between {self._min_value} and {self._max_value}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m         \u001B[0;31m# once the exception are stated, leave to the super class the duty to insert the element\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Values must be between 0 and 255"
     ]
    }
   ],
   "source": [
    "rgb['R'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MappingProxy\n",
    "\n",
    "Is probably the fastes way to create an immutable and persistent view of a dictionary which will reflects any update on the source dict. It is part of the `types` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'a': 1, 'b': 10})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import MappingProxyType\n",
    "\n",
    "d = {'a':1, 'b':10}\n",
    "mp = MappingProxyType(d)\n",
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'mappingproxy' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35187/2879245441.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: 'mappingproxy' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "mp['a'] = 'cant reassing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'mappingproxy' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35187/2245984712.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'c'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'cant insert'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: 'mappingproxy' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "mp['c'] = 'cant insert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'mappingproxy' object does not support item deletion",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_35187/2484332592.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mdel\u001B[0m \u001B[0mmp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# cant delete\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: 'mappingproxy' object does not support item deletion"
     ]
    }
   ],
   "source": [
    "del mp['a'] # cant delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'a': 1, 'b': 10, 'c': 'new_key'})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['c'] = 'new_key'\n",
    "mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sets\n",
    "---\n",
    "\n",
    "Matematically speaking, a set is a gathering (unordered ensamble) of unique elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Set Theory\n",
    "\n",
    "Given two sets `s1` and `s2`:\n",
    "\n",
    "* **Union**: the elements contained in s1 `or` s2 but not repeated (or in python is `s1 | s2` but we also have the method `s1.union(s2, ...)`)\n",
    "\n",
    "* **Interesection**: the elements that are both in s1 `and` s2 (and in python is `s1 & s2` but we also have the method `s1.intersect(s2, ...`)\n",
    "\n",
    "* **Difference**: the elements that are in s1 `and not` in s2 (in python we use the `-` sign since it is overloaded or the  method `s1.difference(s2, ...)`); difference is not commutative. Two sets are **disjoint** there are no elements in their intersection (`len(s1 & s2) == 0` or `s1.isdisjoint(s2)`)\n",
    "\n",
    "* **Symmetric Difference**: the union - the intersection, i.e. what is in s1 and in s2 but not in both (in python we use `s1 ^ s2` or the method `s1.simmetric_difference(s2)`.\n",
    "\n",
    "* The **Cardinality** of a set is the number of elements it contains; the empty set contains no elements, thus its cardinality is 0. To create an empty sets in python we need to use the `set()` function because empty curly brackets `{}` will create an empty dictionary.\n",
    "\n",
    "* s1 is a **subset** of s2 it means that all the elements in s1 are contained in s2 (in python `s1 <= s2` or `s1.issubset(s2)`. we can also say that s2 is a **superset** of s1 (in python `s2 <= s1` or `s2.issuperset(s1)`\n",
    "\n",
    "All the above-mentioned operation on set in python will mutate the original set object (meaning that a new object is created. However, there is the possibility to update the sets (same as for lists, adding the `=` sign after each operation (e.g. `&=` or `|=` etc..) or by using the methods with the `_update` ending (e.g. `s1.intersection_update(s2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python implementation of sets\n",
    "Python has the datatype `set` implemented (once was very close to dictionaries implementation) based on hashmaps that only contain keys; as a metter of fact, set's elements must be hashable and distinct (in the sense that they don't compare equal with `==`); no order is guaranteed among the elements (while from 3.6 dictionaries new implementation retain insertion order).\n",
    "\n",
    "A set is a mutable object, since we can add and remove elements, and therefore it is not hashable; for the same reason it can't be the key of a dictionaries and cannot contains another set.\n",
    "\n",
    "Since sets are essentially hash tables, the memebership testing is very efficient (`in` or `not in`) in particular compared to a list, where we need to scan all the elements. The tradeof is that sets have an higher storage cost due to the precaution that hash tables need to have to avoid collisions as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets creation\n",
    "\n",
    "There are few ways to create sets:\n",
    "\n",
    "* using a literal expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 44, 'a'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = {'a', 1, 44}\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* using the set method passing an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set('abc')\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is also the only way to create an empty set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set()\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* set comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h', 'n', 'o', 'p', 't', 'y'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = {i for i in 'python'}\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets can also be unpacked with `*` but the order of unpacking depends on the hash table, therefore, it won't probably be what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 't', 'o', 'h', 'y', 'n']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [*s1]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we can use unpacking to create the union of two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd', 'e', 'f'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set('abcd')\n",
    "s2 = set('cdef')\n",
    "s3 = {*s1, *s2}\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common operation in sets\n",
    "\n",
    "Following, a list of the most common operations on sets:\n",
    "\n",
    "* adding an elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd', 'e', 'f', 'g'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.add('g')\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* removing elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'c', 'd', 'e', 'f', 'g'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.remove('a')\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however, if we try to remove an element that doesn't existi we get a `KeyException`; this can be avoided using the `discard` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.discard('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove elements we could also use the `pop` method, but the element popped will be pseudo-random, and we will get a `keyError` exception if the set is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, if we want to empty the set we have the `clear` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.clear()\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To copy a set, as for any iterable, we have methods for shallow copies like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s1.copy()\n",
    "s2 = set(s1)\n",
    "s2 = {*s1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, for a deep copy, we have the `copy.deepcopy` function which takes care of all the check and recursions required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Sets\n",
    "\n",
    "Frozen sets are the immutable version of a set; as a matter of fact they are hashable and, as a consequence, can be used as keys in dictionaries and can contain other frozen sets.\n",
    "\n",
    "To create a frozenset we simply need to call the corresponding method `frozenset`.\n",
    "\n",
    "We can carried out sets operation between sets and frozensets, and the resulttant type will dipend on the first operand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization and Deserialization\n",
    "---\n",
    "\n",
    "Serialization is a procedure useful to store and trasmit data both during and after the program execution. A classical example is a REST API where data are trasmitted in JSON format and therefore have to be serialized in that format and eventualy deserialized for performing operations on the data. In principles, we can serialize any kind of object and this mainly depend by the serialization mechanism we will adopt. Python for example has a specific serialization mechanism called `pickle` which use a binary representaion (non-human readable). Other example are databases (sql and NOsql) or JSON which are essentially text representation and therefore more limited in the datatypes they can serialize/deserialize, but with the advantage of being human readable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle\n",
    "Pickle is a Python specific serialization/deserialization (`marshalling`) tool which creare and load objects representation in binary format. \n",
    "\n",
    "A side effect of pickling/unpickling is that we loose the objects id, therefore the same object after a pickle will still compare equal `==` but not identical `is`.\n",
    "\n",
    "Some objects may have some property that cannot be natively serialized by pickle, like open file hadle; to solve this problem we need to create a custom class that hold that unserializable operation and code ourself the `__reduce__` method that will essentially tell pickle how to treat that unknown property/object.\n",
    "\n",
    "N.B. when deserializing, pickle can actually execute code, therefore it is important to unpickle only data that we trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "JSON stands for **J**ava**S**cript **O**bject **N**otation and it is a text-based object seriablization, thus human readable. It is the standard for API and web/systems communication in general.\n",
    "\n",
    "Being string based, it has limitation of the data types it can serialize:\n",
    "\n",
    "* strings: **Only** with double quotes -> \"python\"\n",
    "* numbers: without any distinction on integer, exponentials floats etc.. (all are floats even if many JSON deserializer have the ability to recognize more sepcific datatypes)\n",
    "* booleans: true, false\n",
    "* arrays: list essentially, the order matters!\n",
    "* dictionaries: the keys must be string and the values can be any supported datatype (unordered)\n",
    "* empty value: `null`\n",
    "\n",
    "\n",
    "The fact that JSON format is a string with limited datatypes and we may want to serialize different type of objects can create problems. How can we serialize a python set? Python dictionaries' keys only require to be hashable, not to be string, is this incompatibel with the JSON dictionary? To solve this problem we will need to create our `custom encoding rules`, and can get complicate. This is way we usually fall back in third-party libraries that have already sorted out this problem, like `marshmallow`.\n",
    "\n",
    "To work with JSON in python we need to import the `json` module and then use the `dump` and `load` methods for serialize and deserialize (or `loads`, and `dumps` if we want to work with string representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "json_data = '''\n",
    "{\"menu\": {\n",
    "  \"id\": \"file\",\n",
    "  \"value\": \"File\",\n",
    "  \"popup\": {\n",
    "    \"menuitem\": [\n",
    "      {\"value\": \"New\", \"onclick\": \"CreateNewDoc()\"},\n",
    "      {\"value\": \"Open\", \"onclick\": \"OpenDoc()\"},\n",
    "      {\"value\": \"Close\", \"onclick\": \"CloseDoc()\"}\n",
    "    ]\n",
    "  }\n",
    "}}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'menu': {'id': 'file',\n",
      "          'popup': {'menuitem': [{'onclick': 'CreateNewDoc()', 'value': 'New'},\n",
      "                                 {'onclick': 'OpenDoc()', 'value': 'Open'},\n",
      "                                 {'onclick': 'CloseDoc()', 'value': 'Close'}]},\n",
      "          'value': 'File'}}\n"
     ]
    }
   ],
   "source": [
    "json_py = json.loads(json_data)\n",
    "pprint(json_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Encoding\n",
    "So what if we want to JSON-encode something that is not JSON supported, let's say an instance of a custom class? Well, essentially we need to create our custom encoding function. The `dump` method has an option called `default` which takes a single-argument-callable as argument and gets called any time python encounter any type that cannot be serialized to JSON. \n",
    "\n",
    "Lets see an example: imagine we want ot serialize a date generated by the datetime module, an object that cannot be serialized in JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 6, 9, 14, 31, 10, 833635)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError : Object of type datetime is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test = json.dumps(now)\n",
    "except TypeError as exc:\n",
    "    print(f'{exc.__class__.__name__} : {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therefore, we need to create our custom encoder for datetime format `YYYY-MM-DDTHH:MM:SS` year-month-day (T is a spacer for Time) hours\\:minutes\\:seconds.\n",
    "\n",
    "This is the format that we actually get if we call the string representation of datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-06-09 14:31:10.833635'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are gonna define a single argument function that conver our datetime object and pass it as default to the json encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_iso(dt: datetime):\n",
    "    return dt.strftime('%Y-%m-%dT%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"2022-06-09T14:31:10\"\n"
     ]
    }
   ],
   "source": [
    "test = json.dumps(now, default=format_iso)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we need to consider more than one custom encoder at the time? Then we have two alternative, define a function that check the instance of the argument passed and collect the proper custom encoding or, more efficienlty, we may use the [`functools.singledispatch`](#Single-Dispatch-Generic-Functions) decorator. \n",
    "\n",
    "Beyond the `default` argument, there are others tweaks we can use with the `dump` method:\n",
    "* `skipkeys`: default to False; if True will skip from encoding dictionary keys that otherwise will trown a TypeError\n",
    "* `indent`: specify the idnentation for imporve human readability\n",
    "* `separators`: default is `(', '; ': ')` -> comma with space, semicolon with space, colon with space. All this to imporve human readability, but for example we may want to remove the spaces to save space in the transmission of the json.\n",
    "* `sort_keys`: default is False, if True sort alphanumerically the keys of our dictionary.\n",
    "* `cls`: itlet us specify our custom `JSONEncoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSONEncoder Class\n",
    "Python uses an instance of the `JSONEncoder` class, inside the `json` module, to serialize data. It shares basically all the arguments with the `dump` method but it become usefull as sort of  context manager. Imagine that in our file we have many dumping command and some times later we need to change something in our encoding strategy, let's say the indentation for whetever reason. In this case we would need to modify each j.son.dump entry. The smart approach instead is to always specify the `cls` argument, to point to a variable that hold the JSONEncoder (even the default one); in this way, if in a second time we need to change the encoding strategy, we only need tpo modify in one place, i.e. creating our custom JSONEncoder.\n",
    "\n",
    "To create a custom JSONEncoder we just need to subclass it, modify the arguments we need to and falls back on the default class for the others. Lets see an example where we use the default JSONEncoder as a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 2, 3]'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "default_encoder = json.JSONEncoder\n",
    "json.dumps((1,2,3), cls=default_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a custom encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    # args and kwargs are needed to silently pass all the argument from the parent class\n",
    "    # also those we are not using, otherwise python will complain\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        # redefine our custom arguments\n",
    "        super().__init__(skipkeys=True,\n",
    "                         allow_nan=False,\n",
    "                         indent='---',\n",
    "                         separators=('', ' = '))\n",
    "        \n",
    "    # define the custom encoding rules\n",
    "    def default(self, arg):\n",
    "        if isinstance(arg, datetime):\n",
    "            return arg.isoformat()\n",
    "        else:\n",
    "            # delegate back to parent class\n",
    "            return super().default(arg)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "---\"time\" = \"2022-06-09T14:31:14.307764\"\n",
      "---\"name\" = \"Python\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    'time': datetime.utcnow(),\n",
    "     1+1j: 'complex will be skipped by skikeys=True',\n",
    "     'name': 'Python'\n",
    "    }\n",
    "\n",
    "print(json.dumps(d, cls=CustomEncoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Decoding\n",
    "Decoding, or deserialization, is performed with the `json.load` method and it works out-of-the-box with standas JSON datatypes but won't work with specialized object, like datetime for example, for which it will retunr only a string. However, if we want to retain particular object while decoding we need do work around the json limitations.\n",
    "\n",
    "One approach is to encode using a scheme that define not only the value but also its type, in order to have that information ready when deconding. We could encode a single object as a dictionary that contains two keys, one for the value and one for the object type. When deconding to a dictionary, then we can iterate over and replace the the two key-value pairs with the exact object. But this is a tedious approach and difficult to carry out recursively.\n",
    "\n",
    "A semplification of this approach is using the `object_hook` argument in the `load` method; this take a function that will be called each time the decoder encounter a dictionary, from the most inner to the outer (root), handling the dicionray and then returning a substitue value/object. In this way the recursion is hadled for us.\n",
    "\n",
    "An alternative is to use `object-pairs_hook` (is an alternative to *object_hook*, we cannot specify both the arguments); the difference is that instead of passing the deserialized dictionary (for whom the keys order is not guaranteed) it pass a list o tuples that contains the key-value paris, and since list are guaranteed to retin order, so it will be for the keys in the tuples.\n",
    "\n",
    "There are other usefull arguments of the `load` method that are supposed to take care of specific deconding; these can be used together with *object_hook* but will be execute first since the hook return an already parsed object.\n",
    "* parse_float\n",
    "* parse_int\n",
    "* parse constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSONDecoder Class\n",
    "Like for the encoder, we have a similar class for decoding that we can customize and use as `cls` arugment in the `load` function. However, while for encoding we overrided the `default` function only for the specific type we wanted to custom-encode while leaving to the default class the rest, in decoding we receive as argument the JSON string as a whole; therefore, we need to fully parse the text and return the object we want.. much more work to do! A samrt way to approach this is to write our custom decoding class and inside it use the default `json.loads` as a first step to return al least the basic python objects (list, dictionaries etc..) and then work on this result leveragin python methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Schema\n",
    "https://json-schema.org/\n",
    "\n",
    "Aside from third party libraries, specialized in deconding/endcoding JSONs, the best way to approach a decoding is to define a schema, i.e. a pattern that will be consistent with the data we are expecting to receive (e.g. an API call). \n",
    "\n",
    "A JSON schema is essentially a dictionary that describe each component of the data we are expecting to decode. Let's see an example; imagine we are receiving from an API call a series of information that describe a person and that we want to decod it in a python object with some properties, and the following is the expected template:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"firstName\": \"...\",\n",
    "    \"lastName\":\"...\",\n",
    "    \"age\":\"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Given this expected incoming data format we can create our dictionary schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_schema = {\n",
    "    \"type\":\"object\",\n",
    "    \"properties\":{\n",
    "        \"firstName\":{\"type\":\"string\"},\n",
    "        \"lastName\":{\"type\":\"string\"},\n",
    "        \"age\":{\"type\":\"number\"},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we are still very generic in our schema: what if an empty string is passed as a name? what if a negative number is passed as age? Well we can specofy even further the schema requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_schema = {\n",
    "    \"type\":\"object\",\n",
    "    \"properties\":{\n",
    "        \"firstName\":{\"type\":\"string\",\n",
    "                    \"minLength\":1\n",
    "        },\n",
    "        \"lastName\":{\"type\":\"string\",\n",
    "                   \"minLength\":1\n",
    "        },\n",
    "        \"age\":{\"type\":\"integer\",\n",
    "              \"minimum\":0\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"firstName\", \"lastName\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are many more properties, limitation and specifications that can be added to our schema, depending on the usecase.\n",
    "\n",
    "Ok, now that we have our JSON schema setup, we need to use it! To do this we levarage the module `jsonschema`. Given a dummy API call that result in a person, we will try to validate it with our schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''{\n",
    "    \"firstName\": \"Giovanni\",\n",
    "    \"lastName\":\"Frison\",\n",
    "    \"age\": 32\n",
    "}'''\n",
    "\n",
    "\n",
    "p2 =  '''{\n",
    "    \"firstName\": \"Giovanni\",\n",
    "    \"lastName\":\"Frison\",\n",
    "    \"age\": \"Unknown\"\n",
    "}'''\n",
    "\n",
    "p3 =  '''{\n",
    "    \"firstName\": \"Giovanni\",\n",
    "    \"age\": 32\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonschema import validate\n",
    "from jsonschema.exceptions import ValidationError\n",
    "from json import loads, dumps, JSONDecodeError\n",
    "\n",
    "def my_json_schema(json_doc):\n",
    "    try:\n",
    "        '''\n",
    "        validate() wants a dictionary, therefore we need to first\n",
    "        '''\n",
    "        validate(loads(json_doc), person_schema)\n",
    "        '''\n",
    "        We need to catch some possible error; first if the file is not a proper json file,\n",
    "        second if the file is not compliant with our schema\n",
    "        '''\n",
    "    except JSONDecodeError as ex:\n",
    "        print(f'Invalid JSON: {ex}')\n",
    "    except ValidationError as ex:\n",
    "        print(f'Validation error: {ex}')\n",
    "    else:\n",
    "        print(json_doc)\n",
    "        print('JSON is a valid and compliant schema')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"firstName\": \"Giovanni\",\n",
      "    \"lastName\":\"Frison\",\n",
      "    \"age\": 32\n",
      "}\n",
      "JSON is a valid and compliant schema\n"
     ]
    }
   ],
   "source": [
    "my_json_schema(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 'Unknown' is not of type 'integer'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['age']:\n",
      "    {'minimum': 0, 'type': 'integer'}\n",
      "\n",
      "On instance['age']:\n",
      "    'Unknown'\n"
     ]
    }
   ],
   "source": [
    "my_json_schema(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 'lastName' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'properties': {'age': {'minimum': 0, 'type': 'integer'},\n",
      "                    'firstName': {'minLength': 1, 'type': 'string'},\n",
      "                    'lastName': {'minLength': 1, 'type': 'string'}},\n",
      "     'required': ['firstName', 'lastName'],\n",
      "     'type': 'object'}\n",
      "\n",
      "On instance:\n",
      "    {'age': 32, 'firstName': 'Giovanni'}\n"
     ]
    }
   ],
   "source": [
    "my_json_schema(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, but with this approach, as soon as an exception is raised the decoding stops, hence we are able to see only the first error and we might need to reiterate many time untill we have a proper schema to fit our data. To solve this, in the jsonschema library there is the module `Draft4Validator` that assist us inspecting all the errors that occur at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'32' is not of type 'integer'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['age']:\n",
      "    {'minimum': 0, 'type': 'integer'}\n",
      "\n",
      "On instance['age']:\n",
      "    '32'\n",
      "------------------\n",
      "'lastName' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'properties': {'age': {'minimum': 0, 'type': 'integer'},\n",
      "                    'firstName': {'minLength': 1, 'type': 'string'},\n",
      "                    'lastName': {'minLength': 1, 'type': 'string'}},\n",
      "     'required': ['firstName', 'lastName'],\n",
      "     'type': 'object'}\n",
      "\n",
      "On instance:\n",
      "    {'age': '32', 'firstName': 'Giovanni'}\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "p4 =  '''{\n",
    "    \"firstName\": \"Giovanni\",\n",
    "    \"age\": \"32\"\n",
    "}'''\n",
    "\n",
    "from jsonschema import Draft4Validator\n",
    "\n",
    "validator = Draft4Validator(person_schema)\n",
    "\n",
    "for error in validator.iter_errors(loads(p4)):\n",
    "    print(error, end= '\\n------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marshmallow\n",
    "\n",
    "https://marshmallow.readthedocs.io/en/stable/\n",
    "\n",
    "Marshmallow is a thrid party library devoted to encoding and deconding (not only JSONs). It does a ton aof thinks, but essentially it has a lot of builtin utility to built specific schema clas to handle JSONs.\n",
    "\n",
    "**Re-watch lecture 57 of deep dive part 3 for more info**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyYaml\n",
    "\n",
    "https://pyyaml.org/wiki/PyYAMLDocumentation\n",
    "\n",
    "Library to work with .yml files. Be carefull, it uses pickling therefore work only with files you knwo the source of.\n",
    "\n",
    "**Re-watch lecture 58 of deep dive part 3 for more info**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serpy\n",
    "\n",
    "https://serpy.readthedocs.io/en/latest/\n",
    "\n",
    "doeas only the serialization but mush faster than marshmallow\n",
    "\n",
    "**Re-watch lecture 59 of deep dive part 3 for more info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpacking iterables\n",
    "Iterables are `packed` structures that bundle values together (list, tuple, strings, set, dictionary..). As per the world meaning, `unpacking` is an operation that assigned the packed values to variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = [1,2,3]\n",
    "\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = 'ciao'\n",
    "\n",
    "a, b, c, d\n",
    "# N.B. we can unpack in the same way a dictionary or a set but in that case the order of assignment will be casual because these are unordered types of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It comes handy when we want to swap values between variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "b = 20\n",
    "\n",
    "a, b = b, a\n",
    "\n",
    "a, b\n",
    "# this works in python because the RHS is evaluated first, where the memory address of \"b\" and \"a\" is copied in a tuple and only after assigned to the new swap) variables \"a\" and \"b\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking with *\n",
    "We may want to unpack an iterable in more than one variable, and in this case it comes handy the `*` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,4]\n",
    "# we want to unpack the first element of l and the others apart\n",
    "#we could do it simply with list slicing and unpacking the\n",
    "a, b = l[0], l[1:]\n",
    "\n",
    "# or in a more elegant way with the * operator\n",
    "a, *b = l # a=1, b =[2,3,4] \n",
    "\n",
    "a, *b, c = l # a=1, b=[2,3], c=4\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, *b, *c = l # ERROR we can unpack with only one *, otherwise python won't understand who assign to whom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of the `*` operator is that can be used also with objects that don't support slicing (like sets or dictionaries, since they have no ordering). N.B. if more than one element is unpacked with *, it will always end up in a list (even if, for example, the item unpacked is a tuple).\n",
    "\n",
    "The `*` operator can be used also for unpacking objects on the RHS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2]\n",
    "l2 = [3,4]\n",
    "l = [*l1, *l2] # l=[1,2,3,4]\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dictionaries we have both keys and values that can be unpacked (unordered unpacking since there is no order!). With the `*` operator we unpack the keys of the dict only, while with the `**` we can unpack both keys and values (N.B. `**` can be used only on the RHS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'a': 1, 'b': 2}\n",
    "d2 = {'b': 3, 'c': 4}\n",
    "d = {*d1, *d2} # d={'a','b','c'} -> n.b. b was not repeated because keys are unique in sets and dictionaries.\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {**d1, **d2} # d={'a': 1, 'b': 3, 'c': 4} -> n.b. b has the values contained in d2 since it was unpacked for second and overwrite the keys from d1.\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested unpacking\n",
    "We can unpack also nested structures, such as list of lists, with the same operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, *b, (c, *d) = [1, 2, 3, 'python']\n",
    "\n",
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While loop\n",
    "to generate an infinite loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print('Infinite Loop')\n",
    "    break # to stop infinite loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`else` statement is executed after a while loop only if it terminates without a `break`\n",
    "\n",
    "`continue` is used to interrupt the execution of the current iteration are restart the loop with the next iteration. Only `finally` statement is executed after a continue statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try statement\n",
    "test a code block.\n",
    "\n",
    "`except` is used to captures errors and handle exceptions\n",
    "\n",
    "`finally` is a code block that is always executed, whether an exception or a break are invoked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common exceptions\n",
    "\n",
    "- ZeroDivisionError\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A first semantic difference in functions is the definition of `parameters` and `arguments`; the first is referred to the variables in the function definition while the second is refereed to the variables passed to the instance of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b): # a and b are the parameters of the function\n",
    "  pass\n",
    "\n",
    "x = 10\n",
    "y = 'a'\n",
    "\n",
    "my_func(x, y) # x and y are the arguments of the function\n",
    "\n",
    "my_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be noted that x and y are passed by `reference` to `my_func`, i.e. the memory addresses of x and y are stored into a and b.\n",
    "Therefore, the `Function Scope` contains the memory addresses of the variables that are passed to the function (x and y in the example).\n",
    "\n",
    "Another pythonic difference is the definition of `functions` and `methods`. They are defined in the same way but a method is bound to a class, it is an attribute of the class that is callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import ismethod, isfunction\n",
    "\n",
    "ismethod, isfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstrings and annotations (PEP 257)\n",
    "N.B. Docstring has to the first line of code in the function/class definition, otherwise it won't be inserted into the `__doc__` method and won't be displayed with the `help()` function.\n",
    "\n",
    "Docstrings (single quote or triple quote) are the way to generate documentation inside the python code. They are different from comments (#) since the former are actually compiled by the interpreter and stored in the `__doc__` property of functions and classes. The `__doc__` property can be invoked with the `help()` function on any object that implements it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func():\n",
    "  '''\n",
    "  Here it goes the doctring that contains\n",
    "  the instruction on function usage and arguments types and boundaries.\n",
    "  This will be displayed invocking the `__doc__` method\n",
    "  with the `help()` function.\n",
    "  '''\n",
    "  pass\n",
    "\n",
    "help(my_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to document our code is to use annotations. These are not stored in the `__doc__` method but can be invoked by the `help()` function. Annotations can be also functions that are evaluated as constant during first compilation; however they don't bind the code to a specific behavior (a: int -> doesn't bind a to be an int), they are only metadata stored in teh `__annotations__` method which is a dictionary with parameters as key and annotations as values. These can be used by external modules like `Sphinx` to automatically generate documentation for our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a: 'string', b: 'integer') -> 'a string':\n",
    "  return a*b\n",
    "  \n",
    "my_func.__annotations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lambda` expression\n",
    "`lambda` expressions are another way to create function without the `def` statement. They are also referred as to `anonymous functions`. It has to be a single expression, therefore no assignment is allowed aswell as no type hinting (annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda [parameters list]: expression\n",
    "lambda x: x**2\n",
    "lambda x, y: x + y\n",
    "lambda : 'hello' # we can assign 0 parameters and just return a constant\n",
    "\n",
    "# we can assign lambda function to variable and later call it\n",
    "my_func = lambda x: x**2\n",
    "\n",
    "type(my_func), my_func(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lambda expression generates a `function object` that returns the expression when called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Intorspection\n",
    "Function are first-class objects that, when created are shipped with a series of default dunder methods in additions to the ones that we implemented. To look at all the function attributes we can use the built-in function `dir()`. Among the dunders method we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be shown with the module\n",
    "# inspect.getcomments()\n",
    "def func(a, b, c='hello'):\n",
    "  pass\n",
    "\n",
    "introspection = {\n",
    "'name' : func.__name__, # the name of the function\n",
    "'default args' : func.__defaults__, # tuple containing default positional parameters\n",
    "'deafult kwargs' : func.__kwdefaults__, # dictionary containing default keyword parameters\n",
    "'code object' : func.__code__, # return a code object which has its own methods:\n",
    "'variables names' : func.__code__.co_varnames, # return the paramters and then the local variables (defined inside the function scope) of the function\n",
    "'num of arguments' : func.__code__.co_argcount, # return the number of parameters except *args and **kwargs\n",
    "}\n",
    "\n",
    "for key, arg in introspection.items():\n",
    "  print(f'{key} : {arg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the module `inspect` can be used to retrieve information about the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "inspect.getcomments(func) # returns the comment just above the function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*args and **kwargs\n",
    "Unpacking can be done also in functions parameters in order to specify a variable number of arguments as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b, *args): # N.B. the name 'args' is just a conventions\n",
    "  return a, b, args\n",
    "\n",
    "a, b, c = my_func(1,2,3,4) # a=1, b=2, c=(3,4)\n",
    "# note that inside function scope, arguments are unpacked into tuples e not lists\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positional argument constructor `*args` has to be the last positional argument in the function since it exhaust all the non-assigned positional arguments; after that only keyword arguments are allowed and these can be unpacked with the `**kwargs` parameter. The `*` and `**` operators can be used to limit the use of positional or keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(*, name):\n",
    "  # in this way my_func doesn't allow positional arguments.\n",
    "  # name is automatically a keyword argument.\n",
    "  pass\n",
    "\n",
    "def my_func(a, *, name):\n",
    "  # in this way my_func allow only one positional argument `a` and one keyword argument `name'.\n",
    "  # since 'name' is placed after * it means it is a keyword argument\n",
    "  pass\n",
    "\n",
    "\n",
    "def my_func(*, name, **kwarg): # OK\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(*, **kwarg): # ERROR an explicit keyword argument is required after the `*`\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters default\n",
    "Care must be taken when assigning default values to functions arguments, in particular if these are mutable objects. Wehn Python compile the script it stores in memory the function definition and any argument with default values. This means that each time that function is called, if the default parameters is left unchanged, it will use the specified values. In some case it may results in unwanted behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that store a message in a log file with the datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def log(msg, *, dt=datetime.utcnow()):\n",
    "  print(f'{dt}: {msg}')\n",
    "\n",
    "# Now, since the value of dt is stored at runtime, each time we call\n",
    "log('first log')\n",
    "time.sleep(3)\n",
    "log('first log')\n",
    "# we will see that the time printed is alway the same since it has been stored at compilation time as a CONSTANT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def log(msg, *, dt=None):\n",
    "  dt = dt or datetime.utcnow() # if dt is false (None) the 'or' statement is executed\n",
    "  print(f'{dt}: {msg}')\n",
    "# we can set dt=None and check if the user actually input a values for dt. If not the function will call datetime.utcnow().\n",
    "# Since this call is performed in the function scope, it gets executed each time the function is called. \n",
    "\n",
    "log('first log')\n",
    "time.sleep(3)\n",
    "log('first log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example is when we create a mutable object directly as argument of a function. Also in this case, that object is evaluated as a constant at compilation time and reused as reference each time the function is called. This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that store values into a list\n",
    "def add_item(item, func_list=[]):\n",
    "  func_list.append(item)\n",
    "  return func_list\n",
    "\n",
    "my_list1 = add_item('banana') # a list is created that references to `func_list`\n",
    "# so if i create another list of items\n",
    "my_list2 = add_item('coca')\n",
    "# now we have:\n",
    "# my_list1 = ['banana', 'coca']\n",
    "# my_list2 = ['banana', 'coca']\n",
    "# because they are both referencing to func_list!\n",
    "\n",
    "my_list1, my_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def add_item(item, func_list=None):\n",
    "  func_list = func_list or list() # short-circuit\n",
    "  func_list.append(item)\n",
    "  return func_list\n",
    "\n",
    "my_list1 = add_item('banana')\n",
    "my_list2 = add_item('coca')\n",
    "\n",
    "my_list1, my_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KEY TAKE-AWAY`: Never use mutable objects as `default` arguments. Instead use None and create the object in the function scope. The only time it can come in handy is when using `memoization` to cache values from a function that is executed multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map, Filter and Zip functions\n",
    "N.B. Map anf Filter have been mostly replaced by list comprehension and generator functions.\n",
    "\n",
    "These are `higher order functions` (i.e. function that takes a function as parameter and/or returns a function), `map` return an `iterator` that applies the function to each element in the iterable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map(func, *iterables)\n",
    "\n",
    "def sq(x):\n",
    "  return x**2\n",
    "\n",
    "l = [1, 2, 3]\n",
    "\n",
    "list(map(sq, l)), list(map(lambda x: x**2, l))\n",
    "# map returns a generator, therefore we need to pass it to list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of iterables that are provided to map() is determined by the function that it is passed; if more than one iterable is provided, only the length of the shortest will be mapped. `filter` is a function that takes a function and a single iterable and returns the elements of the iterable that satisfies the condition given in the function, i.e. it filters the iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0,1,2,3,4,5]\n",
    "\n",
    "list(filter(lambda n: n % 2 == 0, l)) # [0, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Functions\n",
    "Also called, accumulators, aggregators or folding functions; are functions that recombine an iterable recursively, returning a single value (es. finding the max in an array, or summing up its elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a reducing function to compute max, min and sum of an iterable.\n",
    "l = [5, 8, 6, 10, 9]\n",
    "\n",
    "add = lambda a, b: a+b\n",
    "find_max = lambda a, b: a if a > b else b\n",
    "find_min = lambda a, b: a if a < b else b\n",
    "\n",
    "def _reduce(fn, sequence):\n",
    "  result = sequence[0]\n",
    "  for x in sequence[1:]:\n",
    "    result = fn(result, x)\n",
    "  return result\n",
    "\n",
    "_reduce(find_max, l), _reduce(find_min, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `_reduce` takes two arguments, a function (add, find_max or find_min), and sequence of numbers. It applies the function recursively a return a single value (the sum, the max or the min) depending on the function passed.\n",
    "Python has a builtin modules that contain the function `reduce` similar to the one defined above, but that works on any iterables, also non index ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "l = [5, 8, 6, 10, 9]\n",
    "\n",
    "reduce(find_max, l), reduce(lambda a, b: a if a > b else b, l) # find max of l\n",
    "\n",
    "#Reduce has a third argument called 'initializer' that serve as first value for the reduced function. This is to avoid runtime error in the case, for example of trying to apply sum-reduce to an empty list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other builtin reducing function in python are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_func = {\n",
    "'max': max(l),\n",
    "'min': min(l),\n",
    "'sum': sum(l),\n",
    "'any': any(l), # return True if at least one element in the sequence evaluates to True\n",
    "'all': all(l), # return True if all the elements in the sequence evaluates to True\n",
    "} \n",
    "\n",
    "red_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial functions\n",
    "Partial functions are a way to reduce the number of argument required by a function, setting some of them as default. We could write ourself a wrapper to a function ore use the builtin `functools.partial` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that compute the power of a number\n",
    "\n",
    "def pow(base, exponent):\n",
    "  return base ** exponent\n",
    "\n",
    "# we can create a partial function that specify the exponent so that it computes alway the square\n",
    "def square(base):\n",
    "  return pow(base, exponent=2)\n",
    "\n",
    "# or we can use the functools.partial module\n",
    "from functools import partial\n",
    "\n",
    "square = partial(pow, exponent=2)\n",
    "\n",
    "'''\n",
    "N.B. if we define a variable prior to the partial definition, and we assign that variable as argument, the argument won't point to the variable but to the values associated with its memory address. therefore even if we change the variable after, the value at which the partial function is pointing will remain the same\n",
    "'''\n",
    "\n",
    "a = 2\n",
    "square = partial(pow, exponent=a)\n",
    "# exponent is pointing at the same memory address of 'a' (the value 2) and not to a itself\n",
    "a = 5\n",
    "\n",
    "square(5) # the value assigned in the square function remain the same (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `operator` module\n",
    "The `operator` module is a builtin suits shipped with standard python installation. Its main purpose is to construct functional equivalents to arithmetic operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "# we have seen how we can use lambda expression together with reduce to create recursive functions on sequences\n",
    "reduce(lambda x, y: x*y, [1,2,3,4]) # return the product of the elements of the list\n",
    "# the same can be achieved with the operator.mul\n",
    "multiplication = reduce(mul, [1,2,3,4])\n",
    "\n",
    "multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a variety of different methods in the operator module, for arithmetic/boolean operations (`mul()`, `add()`, `le()`, `is_()` ..), for sequences handling (`getitem()`, `setitem()`, `delitem()` ...) and for handling functions (`itemgetter()`, `attrgetter()`, `methodgetter()`...). These last ones don't return values but instances of the method called; they become an operator thyself (a function essentially) to be call on another objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "l = [5, 8, 6, 10, 9]\n",
    "f = itemgetter(1, 3) # create a function that return the item at index 1 and 3\n",
    "\n",
    "f(l) # -> (8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "---\n",
    "We have seen as everything in python is a object and Classes are the quintessence of object! We can think of classes like containers that stores data (state and attributes) and functionality (behavior and methods). We can thing of a class as a blueprint of an object, and the object that will be created from that class are referred as `instance` of that class; as a consequence there is a huge difference when talking about the state and the behavior of a class or of an instance of that class (actually the creation of an instance is a behavior of the class!). \n",
    "\n",
    "Finally, since class are object that generates objects, how are class created in the first place? the answer is in the `metaclass` type, an advance concept (metaprogramming) that we will investigate later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes of classes\n",
    "\n",
    "Classes, once created, are shipped with a series of default attributes that python generates for us, like `__name__` or `__doc__`; however we can easily add properties to the a class simply declaring variables inside its definition. We can easily access these variable with the dot notation or with the `getattr` function to which we can specify a default value in case we request an attribute that soesn't exist (otherwise we would incurr in an `AttributeError`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Python', 3.6, 'Not an attribute')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClass:\n",
    "    language = 'Python'\n",
    "    version = 3.6\n",
    "\n",
    "MyClass.language, getattr(MyClass, 'version', 'Not an attribute'), getattr(MyClass, 'name', 'Not an attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can set an attribute to the class with the dot notation or with the `settattr` function; if the attribute already existing we are goin to mutate the state of the class, otherwise we are going to add a new one (ofc, python is a dynamic language!).\n",
    "\n",
    "To delete class attributes we can simply use the `del` or the `delattr` methods (with the same exception handling problem of getattr if an attribute doesn't exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('G.V.Rossum', 3.7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.inventor = 'G.V.Rossum'\n",
    "setattr(MyClass, 'version', 3.7)\n",
    "\n",
    "MyClass.inventor, MyClass.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of a class is stored inside a dictionary, under the builtin property `__dict__` which return a [mappingproxy](#MappingProxy) (forcing attributes to be strings essentially), essentially an immutable view of key-value pairs that represents attributes and their stored value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              'language': 'Python',\n",
       "              'version': 3.7,\n",
       "              '__dict__': <attribute '__dict__' of 'MyClass' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'MyClass' objects>,\n",
       "              '__doc__': None,\n",
       "              'inventor': 'G.V.Rossum'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes can be of any type, for example a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from PyFry!\n"
     ]
    }
   ],
   "source": [
    "def say_hello():\n",
    "    print('hello from PyFry!')\n",
    "    \n",
    "setattr(MyClass, 'say_hello', say_hello)\n",
    "\n",
    "MyClass.say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we have said, class are callables, meaning that python automatically creates the `__call__` method for us when creating a class. We a class is called, it return an object of the type of the class itself that is called an `instance of that class`. The instance of a class is an object itself and as such it has its own namespace and own `__dict__`.\n",
    "\n",
    "To recap, classes are callable (of type() == `type`) that returns an instance of themself which has the type corresponding to the class from which it is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(type, __main__.MyClass, True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myObj = MyClass()\n",
    "# m is a instance of the class MyClass  \n",
    "\n",
    "type(MyClass), type(myObj), isinstance(myObj, MyClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, 'Python')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myObj.__dict__, myObj.language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the instance dict we notice something strange: the dict itself is not a mappingproxy (but a simple dict that can be direclty manipulated) and it is empty, however we are able to retrieve the attribute *language* from the parent class. This is because python first look at the instance namespace, and if it finds the attribute it returns it, if not look one level above, i.e. in the namespace of the parent class.\n",
    "\n",
    "If we try to set an attribute to the instance instead, we are going to populate the instance dict an not the class one, i.e. creating an `instance attribute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'language': 'Java'}, 'Java')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myObj.language = 'Java'\n",
    "\n",
    "myObj.__dict__, myObj.language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as expected, python retrieves the instance attributes since it looks first at the instance namespace. However, if we create a new instanc eof the class, its dictionary would be empty since it is a different object not related to the other instances of the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions as instance attribute\n",
    "\n",
    "We have seen that we can set as class attribute also a function, however, unlike other data-like attributes, function are not passed directly to instances of that class. Instead something else is passed to the instance, i.e. a `bound method` to the parent class and a reference to a specific id of the instance itself. But lets see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.say_hello()>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.say_hello # a simple function, an attribute of the class MyClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method MyClass.say_hello of <__main__.MyClass object at 0x000001EE58F3F0D0>>,\n",
       " '0x1ee58f3f0d0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myObj.say_hello, hex(id(myObj)) # a method bounded to the instance myObj of the class MyClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(function, method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(MyClass.say_hello), type(myObj.say_hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a more unexpect behavior happen when trying to call the function! If we call it on the class, where it is defined, than we have seen that the function is executed correclty; If we try to execute it on the instance we get a `TypeError`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "say_hello() missing 1 required positional argument: 'name'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_7880\\540095145.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmyObj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msay_hello\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m: say_hello() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": [
    "myObj.say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python says that the function takes no argument, and this is true in its definition, but that we passed one argument! Whats happening? and what it is this `bound method` that python created on the instance of the class??\n",
    "\n",
    "`method` is a actual object type that like a function is callable but it is bound to some object, **and that same object is passed to the method as first argument**. \n",
    "\n",
    "Therefore when we are calling **myObj.say_hello()** we have that **say_hello** is a method bounded to the object **myObj** that is injected as a first parameter in the method itself!?!?! essentially what we are doing is similar to:\n",
    "\n",
    "```py\n",
    "MyClass.say_hello(myObj)\n",
    "```\n",
    "But of course this is now what we want to achive, and we will sort this soon.\n",
    "However, the utility of having the method bounded to the object is clear: now **say_hello** has access to the object namespace.\n",
    "\n",
    "Being objects, `methods` have their own attributes like `__self__` , which is the instance to which the method is bound to, and `__func__` which is the original function defined in the parent class. calling `myObj.say_hello(args)` traduces essentially in calling `say_hello.__func__(say_hello.__self__, args)` where:\n",
    "\n",
    "* `say_hello.__func__` is the function say_hello coded in the class MyClass\n",
    "* `say_hello.__self__` is the instance to which the method is bounded, i.e. `myObj`\n",
    "* `args` are any other parameters that the function may take as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.MyClass.say_hello(instance_obj, name)>,\n",
       " <__main__.MyClass at 0x1ee57cfdf10>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myObj.say_hello.__func__ , myObj.say_hello.__self__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a way to take into account that extra argument `say_hello.__self__` that is automatically passed to the method when it is called by the instance, and this has to be done inside the class definition, otherwise we wont be able to call the function form the instance. To do this we need to define the function inside the class with at least one argument that represents the instance of the class that will be passed to the instance method once it will be bounded to the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from PyFry!\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def say_hello(instance_obj):\n",
    "        print('hello from PyFry!')\n",
    "    \n",
    "myObj = MyClass()\n",
    "myObj.say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function inside the class is often called `instance method` but in realty, until is not bounded to an instance of the class, is just a regular function. When we create `myObj` the function `say_hello` becomes a method bounded to that instance, and since it need an argument (called `obj` in his case as an example), it can be called from the instance without receiving the error:\n",
    "\n",
    "**TypeError: say_hello() takes 0 positional arguments but 1 was given** because now we have one positional argument that is filled by the bound instance (the instance bounded to the method) itself `say_hello.__self__`.\n",
    "\n",
    "This is the same as calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from PyFry!\n"
     ]
    }
   ],
   "source": [
    "MyClass.say_hello(myObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the functions defined inside the class can have their own parameter, even if the first one will always need to be the bounded instance. the power of the instance method is that can access both the instance and the class namespaces thanks to the bounded instance that is automatically passed as first argument, making possible the communication between the instance and the parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Giovanni, greating from Pyfry!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, {})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClass:\n",
    "    master = 'Pyfry'\n",
    "    def say_hello(instance_obj, name):\n",
    "        print(f'Hi {name}, greating from {instance_obj.master}!')\n",
    "    \n",
    "myObj = MyClass()\n",
    "myObj.say_hello('Giovanni'), myObj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, python looked for the attribute master first in the instance namespace, since it doesn't find it, looks up in the parent class namespace. If we were to redifine the state of the attirbute *master* then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Giovanni, greating from God!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, {'master': 'God'})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myObj.master = 'God'\n",
    "myObj.say_hello('Giovanni'), myObj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Worldwide convention is to call the first argument of a class function, the one referring to the bounded instance **`self`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B if at runtime we assign a function to an instance of a class, this will be only a plain function and not a bounded method**. We can create bounded method at runtime but we need a special function from the `type` module called `MethodType`, this takes two arguments: the function and the instance object to which we want to bound it. Ofc, in this case only the namespace of the particular instance is affected, if we create new instances they wont see nothing more than what is defined inside the parent class. This can become handy when we have different instance of the same class that for some reason needs a different behavior; monkey-patching a bounded method at runtime can help us in this kind of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class initialization\n",
    "\n",
    "When creating an instance of the class python does two things: creates the new instance and initialize its namespace. We can, and we usually do, override the default python behavior creating a custom initializer. This is design to work as a bound instance method and has the special name `__init__`. Like any function defined inside the class, the *init* is a class attribute, i.e. a function that is in the class namespace. Only when we instanciate the class, the instance call the `__init__` as a bounded method, as always with the first argument equal to the instance itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing <__main__.MyClass object at 0x000001EE57AE3F40>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm an instance of <class '__main__.MyClass'>\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClass:\n",
    "    attribute = 'a class attribute'\n",
    "    \n",
    "    def __init__(self, whoAmI):\n",
    "        print(f'Initializing {self}')\n",
    "        self.whoAmI = whoAmI\n",
    "        \n",
    "obj = MyClass(f\"I'm an instance of {MyClass}\")\n",
    "obj.whoAmI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike any other function, the *init* is a special method and gets called automatically by python each time an instance of the class is created.\n",
    "\n",
    "*whoAmI* is an `instance attribute`, inside the class is assigned to `self`, i.e. to the future instances of the class MyClass; as a matter of fact *whoAmI* is not inside the class namespace but only in the instance one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(mappingproxy({'__module__': '__main__',\n",
       "               'attribute': 'a class attribute',\n",
       "               '__init__': <function __main__.MyClass.__init__(self, whoAmI)>,\n",
       "               '__dict__': <attribute '__dict__' of 'MyClass' objects>,\n",
       "               '__weakref__': <attribute '__weakref__' of 'MyClass' objects>,\n",
       "               '__doc__': None}),\n",
       " {'whoAmI': \"I'm an instance of <class '__main__.MyClass'>\"})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.__dict__, obj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats happening under the hood is that python has already created the object ans its namespace before the *init* is executed; only in this way the *init* can be called as a bound method to the newly created instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Properties\n",
    "\n",
    "We have seen how we can define bare attributes to class instances by assigning them as *self.attribute* in the *init* method or, at runtime, with the *setter* or dot method. These attribute are freely accessible from the user since in python, unlike other programming OOP languages like JAVA or C#, deosn't have the concept of private variable. as a matter of fact, not all the attributes we define in a class should be accessible to the user since their value can affect in an unexpeded way other behaviors o fthe class itself; where private variable exist, usually no attributes are left open access in classes; instead they are defined as private and two methods called *getter* and *setter* that are pubblic and permit an indirect access to the private variable.\n",
    "\n",
    "Even if in python we dont have the concept of private variable, there is a common rule between pythonista that is **if an attribute starts with an underscore, it means that is protected e.g. self._attribute**. However, this is just a convention, and nothing strictly prohibits the user to access it. To declarea **private** attribute double underscore **self.__attribute** are used, but again the privacy is only apparent since python is only a `name mangling`, i.e. it is replacing the attribute name with `_className__attributeName`; of course the protection on that attribute is stronger, but we are still not strickly talking about a private variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Giovanni'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, name, surname):\n",
    "        self._name = name\n",
    "        self.__surname = surname\n",
    "\n",
    "obj = MyClass('Giovanni', 'Frison')\n",
    "obj._name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyClass' object has no attribute '__surname'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_7880\\3220063278.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__surname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: 'MyClass' object has no attribute '__surname'"
     ]
    }
   ],
   "source": [
    "obj.__surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frison'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj._MyClass__surname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, the corret approach to handle a private variable is to create methods that explicity set or get that attribute, and also in python, even if the concept of privacy is ficticious, we can declare a varible as protected (`sef._attribute`) and create a setter and a getter function suited to read and modify the protected variable. We can always access the attribute directly but to other programmers or to our future self we are explicitly telling \"use that attribute only trough the setter and the getter method!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Giovanni'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mimic JAVA\n",
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "       \n",
    "    def get_name(self):\n",
    "        return self._name\n",
    "    \n",
    "    def set_name(self, name):\n",
    "        self._name = name\n",
    "        \n",
    "obj = MyClass('Giovanni')\n",
    "\n",
    "obj.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gianni'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.set_name('Gianni')\n",
    "obj.get_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getter and Setter are more then simple way to protect an attribute, they are ment to provide control over tha class attributes themself giving extra capabilities. For example we may add a validation step in the setter method or perform a computation in the getter, and that's why we call them properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a validation that we can carry out with a setter method\n",
    "def set_name(self, name):\n",
    "    if isinstance(name, str) and len(name) > 0:\n",
    "        self._name = name.strip()\n",
    "    else:\n",
    "        raise ValueError('name must be a non-empty string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, since in the setter we are validating the bare atribute that is assigned for the first time in the *init*, why dont use it directly in the *init*? In this way the validation will occur also at the creation of the instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        # self._name = name\n",
    "        self.set_name(name)\n",
    "    \n",
    "    def set_name(self, name):\n",
    "        if isinstance(name, str) and len(name) > 0:\n",
    "            self._name = name.strip()\n",
    "        else:\n",
    "            raise ValueError('name must be a non-empty string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smart!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @**property** class\n",
    "\n",
    "Python has a solution to mimic in a more consistent way the JAVA-like approach to setter and getter: the `property` class. The `property` class takes two arguments `fget` and `fset` that are the two class functions that will be called when the property to which it referes is called (other parameters are `fdel` to specify the function to call when we want to delete the instance property and `doc` to specify a docstring)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setter called\n",
      "getter called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'God'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "       \n",
    "    def get_name(self):\n",
    "        print('getter called')\n",
    "        return self._name\n",
    "    \n",
    "    def set_name(self, name):\n",
    "        print('setter called')\n",
    "        self._name = name\n",
    "        \n",
    "    name = property(fget=get_name, fset=set_name)\n",
    "    \n",
    "obj = MyClass('Giovanni')\n",
    "obj.name = 'God'\n",
    "obj.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to access the attribute using the dot notation but in reality we are invocking the *fget* and *fset* method of the property class. This helps us in the code structure; in fact, in python we usually start writing bear attribute, and if needed we can create a property without breaking the interface of the class, but simply defining a getter and a setter method. \n",
    "\n",
    "Another way to use the `property` class type is to call it and assign to creare a property object and then assign to it the `getter, setter and deleter` methods:\n",
    "\n",
    "```py\n",
    "x = property()\n",
    "x = x.getter(get_x)\n",
    "x = x.setter(set_x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property Decorator\n",
    "\n",
    "Following the example above, the use of a property to create a property object and then assign to it the getter and setter methods is the same pattern used for decorators (a decorator takes the original function symbol and add to it some functionality, like a property class take a function as argument and returns a property object with the functionality of the functions passed as argument). As a matter of fact the most used way to assign a property to a class is trough decorators.\n",
    "\n",
    "So to create a `property` we simply decorate any function with the `@property` decorator, and this is the same effect of creating a property object with the same name of the function and assign to it a getter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getter called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Giovanni'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "     \n",
    "    @property\n",
    "    def name(self):\n",
    "        print('getter called')\n",
    "        return self._name\n",
    "    \n",
    "obj = MyClass('Giovanni')\n",
    "obj.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function *name* is a method and an instance property.\n",
    "\n",
    "Then, if we want to define a `setter` we can refer to the *name* function that has been decorated as a property and decorate a setter function adding the setter functionality to the alredy created *name* property: `@name.setter` (**the name of the setter has to correspond to the name of the function which has been decorated with @property**this because when creating the setter i dont want to create a new property but override the first one adding the setter functionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setter called\n",
      "getter called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'God'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "     \n",
    "    @property\n",
    "    def name(self):\n",
    "        print('getter called')\n",
    "        return self._name\n",
    "    \n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        print('setter called')\n",
    "        self._name = value   \n",
    "    \n",
    "    \n",
    "obj = MyClass('Giovanni')\n",
    "obj.name = 'God'\n",
    "obj.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS**: if we want to define a docstring to the property using the decorator approach, we just need to insert the docstring in the getter function (i.e. the one decorated with @property)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-Only and Computed Properties\n",
    "\n",
    "In general, we want our python objects to be as lazy as possible, meaning that we want to compute at initialization only what is striclty necessary and leave to a later computation attributes that may or may not be requested by the user. Moreover, once these attributes are requested, we want to store their results, until one of the input property change, in order to safe computation if the request has to be done multiple times.\n",
    "\n",
    "Looking at the example belowe, we have a circle class that require the radius as argument. The area of the circle is needed only when the user request it, therefore it is an optimal candidate for a  **read-only property**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing area..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.274333882308138"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import pi\n",
    "class Circle:\n",
    "    \n",
    "    def __init__(self, r):\n",
    "        self.r = r\n",
    "        \n",
    "    @property\n",
    "    def area(self):\n",
    "        print('computing area..')\n",
    "        return pi*self.r**2\n",
    "    \n",
    "c1 = Circle(3)\n",
    "c1.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, each time we are calling the property area we are computing its value even if the radius hasn't change, and this is a waste of resources. What we can do is to set initially the area value to None at initialization and each time the radius is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing area..\n",
      "setting new radius..\n",
      "computing area..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.26548245743669"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import pi\n",
    "class Circle:\n",
    "    \n",
    "    def __init__(self, r):\n",
    "        self._r = r\n",
    "        self._area = None\n",
    "    \n",
    "    @property\n",
    "    def radius(self):\n",
    "        return self._r\n",
    "    \n",
    "    @radius.setter\n",
    "    def radius(self, value):\n",
    "        print('setting new radius..')\n",
    "        if isinstance(value, int) or isinstance(value, float):\n",
    "            self._r = value\n",
    "            self._area = None\n",
    "    \n",
    "    @property\n",
    "    def area(self):\n",
    "        if not self._area:\n",
    "            print('computing area..')\n",
    "            self._area = pi*self.radius**2\n",
    "        return self._area\n",
    "    \n",
    "c1 = Circle(3)\n",
    "c1.area\n",
    "c1.radius = 4\n",
    "c1.area\n",
    "c1.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! the last time we asked for the circle's area we retrieved the value cached at the prior iteration without wasting computational resources.\n",
    "\n",
    "**N.B. in *area* property we used the getter method *radius* to retrieve the circle's radius instead of the semi-private attribute *self._r* and this is a good practice, since usually getter and setter method are enhanced with additional funcitonalities, such as data validation, and calling the property we are ensuring that we keep our pipeline consistent**\n",
    "\n",
    "This is so true that in reality we couls have define direclty the radius property in the initialization using the setter method instead the semi-private attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting new radius..\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "class Circle:\n",
    "    \n",
    "    def __init__(self, r):\n",
    "        self.radius = r\n",
    "        self._area = None\n",
    "    \n",
    "    @property\n",
    "    def radius(self):\n",
    "        return self._r\n",
    "    \n",
    "    @radius.setter\n",
    "    def radius(self, value):\n",
    "        print('setting new radius..')\n",
    "        if isinstance(value, int) or isinstance(value, float):\n",
    "            self._r = value\n",
    "            self._area = None\n",
    "            \n",
    "    @property\n",
    "    def area(self):\n",
    "        if not self._area:\n",
    "            print('computing area..')\n",
    "            self._area = pi*self.radius**2\n",
    "        return self._area\n",
    "            \n",
    "c1 = Circle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class and Static Methods\n",
    "\n",
    "**Class methods** are, as the name suggest, method that are bounded to the class also when called from an instance of the class itself. We have seen how normally, when we define a function inside the class, we need to pass at least the *self* argument for the instance to be able to call it. With class method instead, the call of the function will be always referred to the class that generated it, not its instance; therefore, calling the function from the instance or from the class will have the same output.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def hello():\n",
    "        return 'hello'\n",
    "    \n",
    "    def instance_hello(self):\n",
    "        return f'hello form {self}'\n",
    "        \n",
    "    @classmethod\n",
    "    def class_hello(cls):\n",
    "        return f'hello from {cls}'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the function *hello()* will work only on the class, but when called from the instance will throw an error due to the missing *self*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hello() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16608\\1523650409.py\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmC\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMyClass\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmC\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhello\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m: hello() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "mC = MyClass()\n",
    "mC.hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it will work fine if called from the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the instance and the class method instead will have different behavior: the instance one won't work if called from the class since it is a bounded method, while the class one will always refer to the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.MyClass.instance_hello(self)>,\n",
       " 'hello form <__main__.MyClass object at 0x0000023A612A77C0>')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.instance_hello, mC.instance_hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"hello from <class '__main__.MyClass'>\",\n",
       " \"hello from <class '__main__.MyClass'>\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.class_hello(), mC.class_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Static Method** instead are plain function that are won't never be bounded to an instance of the class or to the class itself. There isn't a real use case for statich method, if not the need to have a function enclosed in the class definition for completeness or logical order (but it could be defined at module level so.. why bother??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    \n",
    "    @staticmethod\n",
    "    def hello():\n",
    "        return 'hello'\n",
    "    \n",
    "mC = MyClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello', 'hello')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyClass.hello(), mC.hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(function, function)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(MyClass.hello), type(mC.hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the *type* of the function, both on the class and on the instance, is a *function* an not a bounded method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class body scope\n",
    "\n",
    "We knwo the difference between global and local scope, but what about the class scope, i.e. whats defined inside the class body? All the variables defined inside the class body are actually part of the class scope, therefore also the pointers to the methods defined inside the class belong to the class namespace. However, what about the methods inner scope? Methods are essentially functions therefore they have their own local scope; is this a nested scope inside the class scope? \n",
    "\n",
    "The answer is **NO**! the functions themself belong in the scope that contain the class itself, therefore be aware that **Python won't look in the class scope for any symbols unless it is clearly referenced the class scope by self**.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONST=2\n",
    "\n",
    "class ClassBodyScope:\n",
    "    CONST = 1\n",
    "    \n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "        \n",
    "    def add_to_var_1(self):\n",
    "        return self.var + self.CONST\n",
    "    \n",
    "    @classmethod\n",
    "    def add_to_var_2(cls, var):\n",
    "        return var + cls.CONST\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_to_var_3(var):\n",
    "        return var + ClassBodyScope.CONST\n",
    "    \n",
    "    def add_to_var_globalscope(self):\n",
    "        return self.var + CONST\n",
    "    \n",
    "test = ClassBodyScope(1)\n",
    "\n",
    "test.add_to_var_1(), test.add_to_var_2(1), test.add_to_var_3(1), test.add_to_var_globalscope()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example above we can see that the name 'CONST' is defined inside the class scope and can be accessed by `add_to_var` function from 1 to 3 becasue we are explicitly telling python where to look for the variable 'CONST' (i.e. inside the class). In the last method however, python will look in the parent scope of the function for the variable 'CONST' and this is the globalscope, where it is defined with a different value.\n",
    "\n",
    "**N.B. a list comprehension is nothing more than a function, therefore if we use it inside a class, referencing a variable to loop on, if not explicitly stated (with self or cls), it will look for a reference for that variable inside the globalscope!! BUUGSS!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 got: ['Luca', 'Luca', 'Luca']\n",
      "test 2 got: ['Giovanni', 'Giovanni', 'Giovanni']\n"
     ]
    }
   ],
   "source": [
    "name = 'Giovanni'\n",
    "\n",
    "class Parrot:\n",
    "    name = 'Luca'\n",
    "    test1 = [name]*3\n",
    "    test2 = [name for _ in range(3)] # This is a function! its outer scope is the global scope!\n",
    "    \n",
    "    @classmethod   \n",
    "    def exec_test(cls):\n",
    "        print(f'test 1 got: {cls.test1}')\n",
    "        print(f'test 2 got: {cls.test2}')\n",
    "        \n",
    "Parrot.exec_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polymorphism\n",
    "\n",
    "`Polymorphism` is the ability to define a behavior that will change when applied to different type of objects. Python is very polyphormin in nature since it supports the paradigm of `ducke typing`:\n",
    "\n",
    "*\"If i walks like a duck andquacks like a duck then it is a duck!\"*\n",
    "\n",
    "Meaning that we don't care to explicity name a specifi object for what it is but for its functionality! In the section about iterator we see how we can implement our own iterator easy enough so that we can iterate over its elements; we don't care if the collection is a list a tuble of a dictionary, we care about iterability only!\n",
    "\n",
    "Other example are the operators such as `+ and *` which behave differently wheter they work with strings of list or numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Methods\n",
    "\n",
    "`Special Methods` are those defined with a double underscore at the beginning and at the end of the name (e.g. \\_\\_init\\_\\_). These are private names that should be reserved to python core only since they serves for the implementation of a lot of functionalities (e.g. iterators, context manager, arithmetic operations etc..)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_str\\_\\_ method\n",
    "\n",
    "The `str` method is aspecial method that will invocked by the print function on the object; it is tipically used for displaying the purpose of the object or some information to the user. If not defined python will look for the `repr` method, if not found it will print the source of the object together with its memory address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rectangle: width:10, height:20'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Rectangle:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Rectangle: width:{self.width}, height:{self.height}'\n",
    "\n",
    "r1 = Rectangle(10,20)\n",
    "str(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_repr\\_\\_ method\n",
    "\n",
    "The `repr` method is a special method similar to `str` method but its use is more developer-oriented. The repr method should return the string representation of the class instance called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rectangle(10, 20)'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Rectangle:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Rectangle({self.width}, {self.height})'\n",
    "\n",
    "r1 = Rectangle(10,20)\n",
    "repr(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic operators\n",
    "\n",
    "Arithmetic operations can be defined for each type of object, also in our custom class. There are many flavors of operators, such as standard, inplace, reversed.\n",
    "\n",
    "for example standard arithmetic operations are:\n",
    "* `__add__`: +\n",
    "* `__sub__`: -\n",
    "* `__mul__`: *\n",
    "* `__truediv__`: /\n",
    "* `__floordiv__`: //\n",
    "* `__mod__`: %\n",
    "* `__pow__`: **\n",
    "\n",
    "\n",
    "more info can be found in DEEP Dive part 4 Lesson 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truth value\n",
    "\n",
    "We should know by now that every python object has a truth value, actually we can say that any object returns to `True` if evaluated as a boolean, execpt for some special cases like 0, None, '' etc..\n",
    "\n",
    "When called `bool` on an object python will first look for the definition of the `__bool__` method and if not found it will look for the `__len__` method, if this returns 0 than the result is `False`, and `True` in any other case. If bool and len are not defined, python will return True no matter what.\n",
    "\n",
    "In our custom class we can override the bool behavior with our custom rules but it always need to return True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callables\n",
    "\n",
    "The `__call__` sepcial methods is used to create classes that are callables, meaning that they can generate instance that are themself callable.\n",
    "\n",
    "It is a very commond practice in python developmente to create callable class and use them, for example, like decorator class.\n",
    "\n",
    "**For some example see Deep Dive 4 Lesson 58**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class Person generates callable now\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    def __call__(self):\n",
    "        print(f\"The class {self.__class__.__name__} generates callable now\")\n",
    "        \n",
    "p = Person()\n",
    "p()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Inheritance\n",
    "\n",
    "`Inheritance` is a fundamental concept in OOP; since classes define properties and methods, these can be inherited from child classes, forming a natural hierarchy. For example we could have the class **Shape** which has 3 child classes that inherit from it that could be **Polygon**, **Line** and **Ellipse**; **Polygon** in turn has two `child classes`: **Quadrilateral** and **Triangle** and so on. \n",
    "In the same way as a triangle is a polygon and also a shape, we can immagine having a class the inherit, and potentially extend or override, the characteristics (state and behavior) of one or more parent class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instance vs type\n",
    "\n",
    "There is a subtle difference when looking at instances of child classes; in fatc, if we look at the `type()` of an instance we will get back the class from which it was generated and no relation with a potential parent class; with the `isinstance()` method instead, we get `True` also if we check the instance against the parent class (which is not directly the generator of the instance itself). We will more often use *isinstance()* method since we are usually more interested in knowing wheter an object has inehirted certain behaviors (from the list or dict class for example).\n",
    "\n",
    "We can also look at the relationship between classes direclty, instead of looking at instances, using the `issubclass()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *object* class\n",
    "\n",
    "When we create a class, even if we are not inheriting explicitly from another class, python is subclassing a special built-in class called `object`. N.B. lowercase classes derives from a Cython implementation.\n",
    "\n",
    "Also modules and functions are class that are contained in the `types` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AsyncGeneratorType',\n",
       " 'BuiltinFunctionType',\n",
       " 'BuiltinMethodType',\n",
       " 'CellType',\n",
       " 'ClassMethodDescriptorType',\n",
       " 'CodeType',\n",
       " 'CoroutineType',\n",
       " 'DynamicClassAttribute',\n",
       " 'EllipsisType',\n",
       " 'FrameType',\n",
       " 'FunctionType',\n",
       " 'GeneratorType',\n",
       " 'GenericAlias',\n",
       " 'GetSetDescriptorType',\n",
       " 'LambdaType',\n",
       " 'MappingProxyType',\n",
       " 'MemberDescriptorType',\n",
       " 'MethodDescriptorType',\n",
       " 'MethodType',\n",
       " 'MethodWrapperType',\n",
       " 'ModuleType',\n",
       " 'NoneType',\n",
       " 'NotImplementedType',\n",
       " 'SimpleNamespace',\n",
       " 'TracebackType',\n",
       " 'UnionType',\n",
       " 'WrapperDescriptorType',\n",
       " '_GeneratorWrapper',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_calculate_meta',\n",
       " '_cell_factory',\n",
       " 'coroutine',\n",
       " 'new_class',\n",
       " 'prepare_class',\n",
       " 'resolve_bases']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "dir(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that, for example we define a function, it \"type\", hence its string representation, will be `function`, but in reality it descentd from the `types.FunctionType` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(function, True, True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_func():\n",
    "    pass\n",
    "\n",
    "type(my_func), types.FunctionType is type(my_func), isinstance(my_func, object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a matter of fact, the class object implements a number of dunder methods that are used by a common class, like `__init__`, `__repr__`, `__new__` etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding\n",
    "\n",
    "Any method inherited from a parent class (at any level) can be overridden to have specific functionalities fot hata class. This works when we inherit from another custom class (like, Person -> Student) but since any class inherit from `object`, we can override also those methods, something that we do almost always when we define a custom `__init__` or `__repr__` method.\n",
    "\n",
    "An example case where we leverage this behavior is in the representation method of a class, where instead of hard-coding the name of the class (that might change or can have a child class with a differente name) we can use the `__class__` property inherited from the `object` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyClass()\n",
      "MySecondClass()\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}()'\n",
    "    \n",
    "class MySecondClass(MyClass):\n",
    "    pass\n",
    "    \n",
    "aClass = MyClass()\n",
    "print(aClass)\n",
    "aSecondClass = MySecondClass()\n",
    "print(aSecondClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending\n",
    "\n",
    "The same concept of overriding but instead we are enhancing a method or a property from the parent class with extra functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delegating to parent class\n",
    "\n",
    "Often, we don't want to repeat code that is already present in our parent class; instead we want to be able to leverage it and extending where applicable. to do this we have the special method `super()`. When using `super().someMethod()`, we are delegating to the parent class the usage of that method as if we were executing it from the parent itself (careful with side effects). Moreover, `super().someMethod()` will look up in the hierarchy of the class for the method called, not only in the direct parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Im' a Person\", \"Im' a Person\", \"Im' a Person but also an Athlete!\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person:\n",
    "    def talk(self):\n",
    "        return \"Im' a Person\"\n",
    "\n",
    "class Man(Person):\n",
    "    pass\n",
    "\n",
    "class Athlete(Man):\n",
    "    def talk(self):\n",
    "        return super().talk() + \" but also an Athlete!\"\n",
    "    \n",
    "\n",
    "p = Person()\n",
    "m = Man()\n",
    "a = Athlete()\n",
    "\n",
    "p.talk(), m.talk(), a.talk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common use case of delegating is through the init method.\n",
    "\n",
    "N.B. it is always better to call the `super().__init__(*args)` method as first in the child class init, becasue you never know; the parent class might overwrite what you did before. (In Java for example it is mandatory to first init the parent class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person:\n",
    "    \n",
    "    def __init__(self, name: str, age: int):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.smoker = True\n",
    "        \n",
    "class Man(Person):\n",
    "    def __init__(self, name: str, age: int, smoker: bool):\n",
    "        self.smoker = smoker\n",
    "        super().__init__(name, age)\n",
    "    \n",
    "\n",
    "m = Man('Gio', 33, False)\n",
    "m.smoker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we set smoker=False in the class parameter, since the method super() is called after, the parent class override the value for smoke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Binding\n",
    "\n",
    "Even if we are delegating a method to a parent class, that same method is bounded to the instance of the class from which is called; therefore `self` will always be the instance of the clas we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im Man\n",
      "Im Man\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    def talk(self):\n",
    "        print( f\"Im {self.__class__.__name__}\")\n",
    "        \n",
    "class Man(Person):\n",
    "    def talk(self):\n",
    "        super().talk()\n",
    "        print(f\"Im {self.__class__.__name__}\")\n",
    "\n",
    "m = Man()\n",
    "m.talk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slots\n",
    "\n",
    "When a class is created, since it inherits from the base `object` class, it is shipped with a default dicitonary that therefore comes with some overhead in terms of memory and speed. To reduce this overhead, relative to emmory in particular, we can use slots. Defining the `__slots__` attribute we can assigne a tuble of values to our class, and these will be the only properties that the class can use. Therefore, if for example we have a mapper class that has always the same properties in it, and this class is called several times (e.g. the parsing of a db), then using slots can be an efficient way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 1, 'y': 1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "class Point_with_slots:\n",
    "    __slots__ = ('x', 'y', )\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "p = Point(1,1)\n",
    "p_slot = Point_with_slots(1,1)\n",
    "\n",
    "p.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Point_with_slots' object has no attribute '__dict__'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [47]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mp_slot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Point_with_slots' object has no attribute '__dict__'"
     ]
    }
   ],
   "source": [
    "p_slot.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using slots have some limitations and should be the choice only when there is a clear advantage in terms of performance. For example, inheritance from a parent class that uses slots might be tricky, since the new class inherit the attributes but has also a definition of the instance dictionary `__dict__` that however won't contains the atrtribute inherited from the parent class. Moreover, we don't want to redifine attributes inherited from a parent class in a child class because we would hide the parent class definition nand increase the memory usage. \n",
    "\n",
    "Slots and properties have in common that are not stored in the instance dictionary even if they are both in the class dictionary. Htey both use **data descriptors** that essentially creates properties for us (getters, setterd, deleters etc.).\n",
    "\n",
    "To summarize, slots are faster at attribute acces and use less memory, while instance dictioray are heavier but more versatile can add attributes at run time. We could use the best of both worlds usign single inheritance where the parent or the child class only implements the slots while inheriting the dictionary from the other, or in the same class adding to the slots the `__dict__` property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptors\n",
    "HARD TOPIC\n",
    "\n",
    "---\n",
    "\n",
    "https://docs.python.org/3/howto/descriptor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that to validate an attribute of a class, we usually have to define properties, getters and setters ourself which will contain our custom validation rules and determin how the user can  interact with instance of thata class. However this can become tedious pretty quickly and a lot of boilerplate code can be produced. Here is where descriptors come at hand.\n",
    "\n",
    "The idea is to be able to define class attributes that will also be bound to the instances at run-time, the same way when we define properties at class level that somehow are also bounded to the instances of the class.\n",
    "\n",
    "The `descriptor protocol` is made of 4 main methods, not all required:\n",
    "* __get__\n",
    "* __set__\n",
    "* __delete__\n",
    "* __set_name__\n",
    "\n",
    "Moreover, descriptors can be devided in:\n",
    "* `non-data descriptors` which don't implement the set\n",
    "* `data desriptors` which implements alsto set and/or delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-data descriptors\n",
    "\n",
    "Let's start with a dummy example where we want to create a class that trows 2 dices; the approach using properties would be to define at class level two function annotated with `@property`, so that they are read only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "1 4\n",
      "3 1\n"
     ]
    }
   ],
   "source": [
    "from random import choice, seed\n",
    "\n",
    "seed(123)\n",
    "\n",
    "class Dices:\n",
    "    @property\n",
    "    def dice_1(self):\n",
    "        return choice(tuple('123456'))\n",
    "    @property\n",
    "    def dice_2(self):\n",
    "        return choice(tuple('123456'))\n",
    "    \n",
    "    @property\n",
    "    def throw(self):\n",
    "        return print(self.dice_1, self.dice_2)\n",
    "    \n",
    "d = Dices()\n",
    "for _ in range(3):\n",
    "    d.throw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we could do instead is to define a descriptor class that will handle the choice mechanism and that, moreover, can re-used for other classes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Choice:\n",
    "    def __init__(self, *choices):\n",
    "        self.choices = choices\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        return choice(self.choices)\n",
    "\n",
    "class Dices:\n",
    "    dice_1 = Choice(*'123456')\n",
    "    dice_2 = Choice(*'123456')\n",
    "    \n",
    "    @property\n",
    "    def throw(self):\n",
    "        return print(self.dice_1, self.dice_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "5 5\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "d = Dices()\n",
    "for _ in range(3):\n",
    "    d.throw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getter and setter\n",
    "\n",
    "But how the `__get__` method is being called in in the Choice class? The class Dices define an instance of Choice() as class attribute and since Choiche implements the `__get__`, python will use that method when retrieving the instance attribute. We can also call the method from the class itself but we have to be careful because the *owner_class* will be diffirent in that case. Therefore, when calling a descriptor it might be important to know if it is called from an instance (or None if called from a class) and to which class the descriptors we are calling belongs (Dices in this specific case).\n",
    "\n",
    "We usually want to differentiate the behavior of the descriptor if it is called from an instance or from the class itself. Usually, if called from an instance we want the attribute value, while if called from the class we want an handler on the descriptor, i.e. an instcane of the descriptor itself. In the case of the Choice class it will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Choice:\n",
    "    def __init__(self, *choices):\n",
    "        self.choices = choices\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        if not instance:\n",
    "            return self\n",
    "        return choice(self.choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__set__` method behave quite similarly adn its signature is `(self, instance, value)` where again:\n",
    "* self is the reference to an instance of the descriptor class\n",
    "* instance is the instance of the class that implements the descritor if any, or None otherwise\n",
    "* value is the value we want to assign to the attribute\n",
    "\n",
    "There is no *owner_class* since it doesn't make sense to set an attribute to a class but only to one of its instances.\n",
    "\n",
    "However, there is a caveat with setters and deleters descriptors; since they are declared at class level, their reference is shared across the multiple instances of the class that implements the descriptor; this will of course ause problem  when the different instance of the class sohuld set different values. For this reason we need both the getter and the setter to be aware of which instance is calling them, in order to properly store the data relative to the specific instance that called the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing instance properties\n",
    "\n",
    "Now come the problem of where to store the instance variables setted through our descriptor; we could place it inside the instance dictionary, but what if we don't have one, what if the class implements slots? and evene if the instance dictionary is there, which simbol should we use to store the variable being sure that it doesn't shade an existing one?\n",
    "\n",
    "The solution is not trying to use the instance dictionary form the instances of our class but to use the instance dictionary inside the instance of the descriptor.\n",
    "\n",
    "Now the problem is how to choose the key-value pair to insert in this dictionary: the value is trivial, is simply what we set, but the key might be a problem since it has to be hashable an not all objects are. The solution is to use as key the instance that is calling the setter method!\n",
    "\n",
    "In this way we will have a common dictionary bound to the single class instance of the descriptor, an in it we will have a key-value reference that traduces in `instance -> setted value`, so that when usign the get from a particular instance we are sure to retrieve the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descripto setter called\n",
      "descripto getter called from Point2D\n",
      "descripto setter called\n",
      "descripto setter called\n",
      "descripto getter called from Point2D\n",
      "descripto getter called from type\n",
      "descripto getter called from type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({<__main__.Point2D at 0x17f249bebc0>: 10,\n",
       "  <__main__.Point2D at 0x17f249bd750>: 1},\n",
       " {<__main__.Point2D at 0x17f249bd750>: 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IntegerValue:\n",
    "    def __init__(self):\n",
    "        self.data = {} # the decorator dictionary\n",
    "    \n",
    "    def __set__(self, instance, value):\n",
    "        print(\"descripto setter called\")\n",
    "        self.data[instance] = int(value)\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        if not instance:\n",
    "            print(f\"descripto getter called from {owner_class.__class__.__name__}\")\n",
    "            return self\n",
    "        print(f\"descripto getter called from {instance.__class__.__name__}\")\n",
    "        return self.data.get(instance, None) # use get in case getter is called before setter\n",
    "    \n",
    "class Point2D:\n",
    "    x = IntegerValue()\n",
    "    y = IntegerValue()\n",
    "    \n",
    "p = Point2D()\n",
    "Point2D().x = 10\n",
    "Point2D().x\n",
    "p.x = 1\n",
    "p.y = 1\n",
    "p.x\n",
    "Point2D.x.data, Point2D.y.data # Im accessing the instance dictiorny of the descriptor instance inside the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this cause a side effect, a memory leack to be precise; when we instantiate the `p` object the refernce count is 1, when we set its x value the reference count goes up to 2 since we are storing a reference in the descriptor dictionary. Therefore, even if we delete p the garbace collector won't be able to free the memory related to `p`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong and Week references\n",
    "\n",
    "What just described above is called `strong reference` since both the instantiation of the class Point2D and the method calling of the descriptor create astrong reference to the object underlying, hence to untill at list one of the reference is alive, the garbage collector can't free the allocated memory.\n",
    "\n",
    "A `week reference` instead is a reference that doesn not impact the reference count on an object in memory; as a consequence, if we delite the strong reference, the weak reference is defined \"dead\" and automatically lost and therefore the garbage collector can do its work. This is the kind of reference that we want our descriptor to hold in its dictionary in order to avoid memory leaks.\n",
    "\n",
    "To create a weak reference python has a built.in module called `weakref`. Once a variable holds a weak reference to an object, it become a callable, and by calling it we are returning the original object to which the reference is pointing to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Point2D at 0x17f26682320>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weakref\n",
    "\n",
    "p1 = Point2D()\n",
    "p2 = weakref.ref(p1)\n",
    "\n",
    "p2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now p2 is a callable and when called is returing the object at which p1 is poiting, therefore it is returning a strong reference. So be careful, if we assign the call of p2 to another object we are creating a second strong reference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = p2() # N.B. this is a new strong reference! -> now the reference count is 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in the descriptor's dictionary we only want to have weak references, adn instead to create a weak reference key each time we can import the `WeakKeyDictionary` from the weakref module, and this special dictionary will call `weakref.ref` for us on each key stored.\n",
    "\n",
    "All great but we still have to deal with the fact that an object has to be hashable in orter to be the key to a dictionary, and not all object are. We could use the `id(instance)` as key but we might occur into another problem: first if the strong reference to the object is deleted than our key might point to an object that doesn't exist anymore; second, even if very unlikeable, there might be a new object created that obtain the same id of the previous object, so that now the dictionary contains a reference to the wrong object.\n",
    "\n",
    "Long story short, we are going to leverage the callback functionality of `weakref.ref` which automatically calls a function when the object which is pointing to is garbage callected.\n",
    "\n",
    "The final recepy is:\n",
    "- using a regular dictionary\n",
    "- storying as key the id of the instance, which is always hashable\n",
    "- store as value a tuple composed by the weak reference and the value, the weak ref will be used to remove dead entry\n",
    "\n",
    "Using descriptor like this we will achieve:\n",
    "- instance specifi storage of variables\n",
    "- instance are not direclty used for storage\n",
    "- handled non-hashable objects\n",
    "- the data storage mechanism has no leaks\n",
    "\n",
    "This is how the previous descriptor should be coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerValue:\n",
    "    def __init__(self):\n",
    "        self.data = {} \n",
    "    \n",
    "    def __set__(self, instance, value):\n",
    "        self.data[id(instance)] = (weakref.ref(instance, self._remove_object), int(value))\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        if not instance:\n",
    "            return self\n",
    "        value_tuple = self.data.get(id(instance))\n",
    "        return value_tuple[1] # the value will always be teh second element of the tuple in the dict\n",
    "    \n",
    "    def _remove_object(self, weak_ref):\n",
    "        reverse_lookup = [key for key, value in self.data.items() if value[0] is weak_ref]\n",
    "        if reverse_lookup: # if the key relative to the supposely dead instance is found, delete it\n",
    "            print(f'removing dead entry for {weak_ref}')\n",
    "            key = reverse_lookup[0]\n",
    "            del self.data[key]\n",
    "        \n",
    "class Point:\n",
    "    x = IntegerValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1645616832784: (<weakref at 0x0000017F267F1F30; to 'Point' at 0x0000017F26682110>,\n",
       "  1)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = Point()\n",
    "p1.x = 1\n",
    "Point.x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing dead entry for <weakref at 0x0000017F267F1F30; dead>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del p1\n",
    "Point.x.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. since classes that implements slot doesn't have an instance dictionary and a weak_ref, they can be subject of a week reference! The only solution, as for the instance dictionary, is to add the weeak_ref to the slots intself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    pass\n",
    "    \n",
    "class Person_slot:\n",
    "    __slots__ = ('name',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__dict__': <attribute '__dict__' of 'Person' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'Person' objects>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__slots__': ('name',),\n",
       "              'name': <member 'name' of 'Person_slot' objects>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person_slot.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Person_slot()\n",
    "hasattr(p, '__weakref__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person_slot:\n",
    "    __slots__ = ('name', '__weakref__')\n",
    "\n",
    "p = Person_slot()\n",
    "hasattr(p, '__weakref__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The set_name method\n",
    "\n",
    "N.B. we assume that the classes dont implement slots for the following to be applicable.\n",
    "\n",
    "From python 3.6 we have another method applicable to descriptors: the `__set_name__` method. We can use it to retrieve the name of the property that the descriptor instance has being assigned to in our class (e.g. x in the Point example above).\n",
    "The descriptor is instantiated at compile time because it is defined inside a class context. Lets see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__set_name__: owner=<class '__main__.Person'>, property_name=name\n"
     ]
    }
   ],
   "source": [
    "class ValidateString:\n",
    "    def __set_name__(self, owner_class, property_name):\n",
    "        print(f'__set_name__: owner={owner_class}, property_name={property_name}')\n",
    "\n",
    "class Person:\n",
    "    name = ValidateString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has we can see the set name has already been called and since it is instatiated inside the Person class, the latter is the owner class and the peroperty_name is exaclty what was set as class attribute.\n",
    "\n",
    "The next step is to store the property name in the instance dictionary of the descriptor, which is not a problem, since the instance of the class which is tight to the instance of the descriptor will always have that same name for that particular attribute. Now, if we define a getter or a setter, these will be aware of which attribute is exdaclty beign retrieved through the property_name stored in the instance dictionary of the descriptor.\n",
    "\n",
    "Now we still have to store the property name inside the instance dictionary fo our descriptor in order to be able to retrieve it and don't caus ememory leaks. An idea could be to use as a key the property name with an underscore in front of it, but this doesn't give use the certainty that the same name is not used for something else in our class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__set_name__: owner=<class '__main__.Person'>, property_name=name\n"
     ]
    }
   ],
   "source": [
    "class ValidateString:\n",
    "    def __set_name__(self, owner_class, property_name):\n",
    "        print(f'__set_name__: owner={owner_class}, property_name={property_name}')\n",
    "        self.property_name = property_name\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        print(f'__get__ called for property {self.property_name} of instance {instance}')\n",
    "        key = '_' + self.property_name\n",
    "        return getattr(instance, key, None)\n",
    "        \n",
    "    def __set__(self, instance, value):\n",
    "        if not isinstance(value, str):\n",
    "            raise ValueError(f'{self.property_name} must be a String')\n",
    "        key = '_' + self.property_name\n",
    "        setattr(instance, key, value)\n",
    "\n",
    "        \n",
    "class Person:\n",
    "    name = ValidateString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__get__ called for property name of instance <__main__.Person object at 0x000002408FCCAD40>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'_name': 'Giovanni'},\n",
       " mappingproxy({'__module__': '__main__',\n",
       "               'name': <__main__.ValidateString at 0x2408df60e50>,\n",
       "               '__dict__': <attribute '__dict__' of 'Person' objects>,\n",
       "               '__weakref__': <attribute '__weakref__' of 'Person' objects>,\n",
       "               '__doc__': None}))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Person()\n",
    "p.name = 'Giovanni'\n",
    "p.name\n",
    "p.__dict__, Person.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all, We cannot use the same name of the property name as key or we will shadow the class instance dict with the descriptor instance dict.. right?\n",
    "\n",
    "Well.. it depends! Here becomes crucial the distinciton between `data` (implements at least get and set or delete) or `non-data` (only get is defined) decriptors. \n",
    "\n",
    "**The data descriptor always override the class instance dictionary with its own instance dictionary** even if we try to hard wire a property directly into the instacne dictionary, when accessing that same property, python will look at the instance diciotnary of the descriptor, not of the class instance.\n",
    "\n",
    "The non-data descriptor instead gives precedence to the class instance dictionary, and the look up to the descriptor instance dictionary happens only if the attribute is not found in the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidateString:\n",
    "\n",
    "    def __set_name__(self, owner_class, property_name):\n",
    "        self.property_name = property_name\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        print(f'getting {self.property_name} from __get__')\n",
    "        return self.__dict__.get(self.property_name, None)\n",
    "        \n",
    "    def __set__(self, instance, value):\n",
    "        if not isinstance(value, str):\n",
    "            raise ValueError(f'{self.property_name} must be a String')\n",
    "        print(f'setting {self.property_name}={value}')\n",
    "        self.__dict__[self.property_name] = value\n",
    "        \n",
    "class Person:\n",
    "    name = ValidateString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting name=Giovanni\n"
     ]
    }
   ],
   "source": [
    "p = Person()\n",
    "p.name = 'Giovanni'\n",
    "p.__dict__['name'] = 'Fred'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a data descriptor, even if we are trying to hard wire the instance dict of the class instance, we get back the value stored in the descriptor instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting name from __get__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Giovanni', {'name': 'Fred'})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.name, p.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the class instance dictionary actually has an attribute name in it, but it gets shadowed by the descriptor instance dictionary that is called with the getter.\n",
    "\n",
    "As stated above, differnet would have been if our was a non-data descriptor; in that case python would have first looked in the class instance dictionary, and only after in the descriptor instance,\n",
    "\n",
    "We can store data both in the class instance and in the descriptor instance, without the risk of shadowing attributes, but beign careful of not incurr in infinite recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidateString:\n",
    "\n",
    "    def __set_name__(self, owner_class, property_name):\n",
    "        self.property_name = property_name\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        print(f'getting {self.property_name} from __get__')\n",
    "        return instance.__dict__.get(self.property_name, None) \n",
    "    # we use the get method to not incurr in a runtime erro in case the values has not been set yet\n",
    "        \n",
    "    def __set__(self, instance, value):\n",
    "        if not isinstance(value, str):\n",
    "            raise ValueError(f'{self.property_name} must be a String')\n",
    "        print(f'setting {self.property_name}={value}')\n",
    "        instance.__dict__[self.property_name] = value\n",
    "        #setattr(instance, self.property_name, value) # N.B. this will lead to infinite recursion!!\n",
    "        \n",
    "class Person:\n",
    "    name = ValidateString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting name=Giovanni\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'name': 'Giovanni'},\n",
       " mappingproxy({'__module__': '__main__',\n",
       "               'name': <__main__.ValidateString at 0x2408f13c6d0>,\n",
       "               '__dict__': <attribute '__dict__' of 'Person' objects>,\n",
       "               '__weakref__': <attribute '__weakref__' of 'Person' objects>,\n",
       "               '__doc__': None}))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Person()\n",
    "p.name = 'Giovanni'\n",
    "p.__dict__, Person.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting name from __get__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Giovanni'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the class instance dictionary stores the same attribute name as the descriptor dictionary, but since it is a data descriptor, the getter return comes always from the descriptor dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties are decriptors!\n",
    "\n",
    "Back to the beginning, were we start talking of `@property` annotation, how we use then in classes and how we can reduce boilerplate code usign descriptors. Well, plot twist, properties are actual data descriptors! As a matter of fact they implements getter, setters and delete methods. Whenever we use a property with the dot notation, python is calling the get/set method on the instance that is performing the call.\n",
    "\n",
    "If we define a property usign the `property()` method, so that we can assign that property to a variable, we can easily confirm with the function `hasattr` that the property contains getter, setter and deleters, even if they are not specified! Since it is a data descriptor it has all the methods implemented by default, they will simply return none if called, but they are there!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions implements the descriptor protocol!\n",
    "\n",
    "We have seen before that some magic happens when the functions defined inside a class becomes magically bounded to the instance of that class. But how does this happen? HOw can python differentiate the behavior of calling a function from the class in which it is defined or from an instance? Well.. guess what, function are objects that implements the non-data descriptors protocol! As such, thei have the `__get__` method and depending how you call it, it is gonna return the function or the bound method to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(a,b):\n",
    "    return a + b\n",
    "\n",
    "hasattr(add, '__get__'), hasattr(add, '__set__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the get method we need the instance and the owner class.\n",
    "\n",
    "To see the different behaviors of the get method we need to be able to call the add function from its owner class, i.e. in this case the module we are writing in now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "me = sys.modules['__main__']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can call the fucntion from the owner class without passing an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.add(a, b)>, True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = add.__get__(None, me)\n",
    "f, f is add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f is now exaclty the function; so the  get method return the function itself when called from the module in which it was defined (i.e. the class that contains it).\n",
    "\n",
    "Now lets see what happens whan calling a function from a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.Person.say_hello(self)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def say_hello(self):\n",
    "        return f'{self.name} says hello!'\n",
    "    \n",
    "Person.say_hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? the behavior is exaclty the same as before; when we are calling the function, python is usign the `__get__` method, with the instance value set to None, since we are calling it from the class itself, and the owner class set to Person, i.e. the class that owns that function. \n",
    "If instead we instantiate the Person class and we call the function from there (therefore with the instance value set to the instance itself, we get the bound method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Person.say_hello of <__main__.Person object at 0x000002408FD71DE0>>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Person('Giovanni')\n",
    "p.say_hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is equivalent to say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Person.say_hello of <__main__.Person object at 0x000002408FD71DE0>>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound_method = Person.say_hello.__get__(p, Person)\n",
    "bound_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get the bound method from an instance, python is actually creating a new method each time. To see to which owner class the bound method belongs we can use the special `__func__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are f1 and f2 two the same object? False, because pyton creates a new method each time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(function, method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = p.say_hello\n",
    "f2 = p.say_hello\n",
    "print(f'Are f1 and f2 two the same object? {f1 is f2}, because pyton creates a new method each time')\n",
    "type(f), type(bound_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.Person.say_hello(self)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound_method.__func__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually mimic the descriptor that is used by python to bound a function to a class using the `types.MethodType` method wichi takes 2 argument, the function and the instance that we want to bound the function to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "class MyFunc:\n",
    "    def __init__(self, func):\n",
    "        self._func = func\n",
    "        \n",
    "    def __get__(self, instance, owner_class):\n",
    "        if instance is None:\n",
    "            print(f\"Called from class={owner_class.__class__.__name__}\")\n",
    "            return self._func\n",
    "        else:\n",
    "            print(f\"Called from instance={instance.__class__.__name__}\")\n",
    "            return types.MethodType(self._func, instance)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we bound a function, defined outside the class itself, to a class but using the descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello(self):\n",
    "    print(f'{self.name} says hello')\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    say_hello = MyFunc(hello)\n",
    "    \n",
    "p = Person('Giovanni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called from class=type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<function __main__.hello(self)>,\n",
       " <bound method Person.say_hello of <__main__.Person object at 0x000002408FD71DE0>>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person.say_hello, p.say_hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scopes and Namespaces\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Scope` is the portion of the code in which a variable name is defined; it has an associated `namespace`, essentially a table that lists all the variables in the Scope and the associated memory addresses. There are different `Scope` in python and are defined in a nested structure. At the top, we have the `built-in` scope, the only truly global scopes that exists across each modules of Python, which contains the definitions of core elements such as `True`, `None`, `dict` etc. Nested inside the built-in scope there is the so called `Global` scope (even if it is not global in the sense that exist only inside a single file). Moreover, each function has its own scope, named `Local`, that is created each time the function is called (until the function is not called, the variables defined in its own scope are not compiled, therefore does not exist in the Global scope, where the function is defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module1.py\n",
    "print(True)\n",
    "# both print and True are not defined in the module1 scope, therefore python automatically goes up one level and look up for their definition in the builtin-scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_var_never_used)\n",
    "# Error! 'a' has not been defined in the module scope and neither is in the built-in scope, therefore Python trows an error `NameError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize that at `compile time` python looks at the code and predetermine which variables will eventually be in the local or global scope. When it encounter a `def` (function), it will look inside of it; if there are assignations (e.g. a = 100), it will understand that that variable will be part of the global scope only, unless the `global a` keyword is specified; if a variable is called but not assigned inside the function (e.g. print(a)), the compiler will determine that it is a non-local reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0 # global scope/namespace\n",
    "\n",
    "def func1():\n",
    "  print(a) # the compiler understand it is a non-local variable since there is no assignment inside the local scope\n",
    "\n",
    "func1()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2():\n",
    "  a = 100 # the compiler knows that this will be a local variable\n",
    "  print(a)\n",
    "\n",
    "func2()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func3():\n",
    "  global a # the compiler knows that this will refer to a global variable\n",
    "  a = 100 \n",
    "  print(a)\n",
    "\n",
    "func3()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "It is defined masking, and should be avoided, when we overwrite a keyword from the built-in scope. Since Python first look at the module scope, if we have assigned a variable to an existing element in the built-in namespace, we will modify its standard behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module1.py\n",
    "print = lambda x: f'Hello {x}'\n",
    "# we are redefining locally the 'meaning' of the variable print so now:\n",
    "print('world') # -> 'Hello world'\n",
    "# python is invoking the local definition of print e not the built-in one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same behavior is applied between Global and Local scopes. When assigning a variable inside a functional scope, python sees it at compilation time and stores it (it will be created only when the function is called but the compiler is already aware that it exists).\n",
    "Therefore, if the same variable exist in the Global scope, when the function is called, it will be masked by the assignation in the local scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del print # remove the previously overwrite of standard print function\n",
    "\n",
    "a = 0 # global scope/namespace\n",
    "\n",
    "def my_func():\n",
    "  a = 100 # local scope/namespace\n",
    "  print(a)\n",
    "\n",
    "my_func() # a = 100, the global scope 'a' has been masked \n",
    "\n",
    "print(a) # a = 0, the global scope hasn't been modified, and the local scope of `my_func` has been destroyed after its execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the possibility to avoid masking by explicitly tell python that the variable assigned in the local spaces are actually owned in the global space. This is done by declaring the `global` keyword at the beginning of a local scope. This is telling the compiler to look first in the global scope for that particular variable and, if not found, to create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "\n",
    "def my_func():\n",
    "  global a\n",
    "  a = 100 # local scope/namespace\n",
    "  print(a)\n",
    "\n",
    "my_func() # a = 100, since `global a` has been declared, the variable `a` in the global scope has been modified\n",
    "\n",
    "print(a) # a = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlocal scope\n",
    "When we define a function inside another function, a new scope is created which is not the global (module level) scope, nor the local (function level) scope. It is a middle scope called `non-local scope`. Variables belonging to the nonlocal scope are called `free variables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_func():\n",
    "  x = 10 # local scope of outer_func == non-local scope of inner_func\n",
    "  \n",
    "  def inner_func():\n",
    "    x = 20 # local scope of inner_func\n",
    "  \n",
    "  inner_func()\n",
    "\n",
    "  print(x)\n",
    "\n",
    "# if we call outer_func:\n",
    "outer_func() # x = 10 since the local scope of inner func has not modified the non-local scope (local scope of outer_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as we tell python that a variable in a local scope is `global`, we can specify a variable to be `non-local` (i.e. with the same reference of the one in the outer_func scope). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_func():\n",
    "  x = 10 # local scope of outer_func == non-local scope of inner_func\n",
    "  \n",
    "  def inner_func():\n",
    "    nonlocal x # now the reference of x is shared with the non-local (outer_func) scope\n",
    "    x = 20 # local scope of inner_func\n",
    "  \n",
    "  inner_func()\n",
    "\n",
    "  print(x)\n",
    "\n",
    "# if we call outer_func:\n",
    "outer_func() # x = 20 since the local scope of inner func has modified the non-local scope (local scope of outer_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. if in a local scope we define a `global` variable, python will look in global scope for a match, otherwise it will create the global variable. Instead, when defining a `nonlocal` variable, python will look only in the non-local scope (the local scope of the parent function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def outer():\n",
    "  x = 0 # local scope of outer\n",
    "  \n",
    "  def inner1():\n",
    "    # local scope of inner1 == nonlocal scope of inner2\n",
    "    def inner2():\n",
    "      nonlocal x\n",
    "      x = 10 \n",
    "    inner2()\n",
    "\n",
    "  inner1()\n",
    "  print(x) # x = 0 because inner2 looked only in its nonlocal scope\n",
    "\n",
    "outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closure\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Closure` is a special python constructor that is composed by a function and an extended scope (nonlocal scope) that contains free variables (nonlocal variables). This means that both the functions and the extended scope point to the same object, but python don't do this directly. Instead a `cell object` is created, pointing to the value of the free variable, while the free variable, in both the extended scope and the function scope, point to the cell object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer():\n",
    "  a = 10\n",
    "\n",
    "  x = 'python'  #----------\n",
    "                # THIS IS A\n",
    "  def inner():  # CLOSURE\n",
    "    print(x)    #----------\n",
    "  \n",
    "  return inner\n",
    "\n",
    "fn = outer() \n",
    "'''\n",
    "now outer has returned the function inner which should print x without directly containing a reference to the variable.\n",
    "Therefore, since the scope of the function outer is exhausted after it is called, we should expect that the variable x=python is lost and can't be referenced by fn.\n",
    "Instead, it is possible since python, during compilation, sees a Closure and create an intermediate cell object that share the reference to x both from inner and outer functions.\n",
    "'''\n",
    "\n",
    "# x = 'python' #--|\n",
    "                 #|--> cell object --> str object `python`\n",
    "# print(x)     #--|              \n",
    "\n",
    "fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `x` in outer and inner functions point to a `cell object` which contains a reference to another object in memory containing the string `python`. This lets us be able to call the function inner, returned from the function outer, even if the scope of outer is already exhausted. We can inspect the closure and free variables of an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = outer()\n",
    "\n",
    "# fn.__code__.co_freevars # (x) while a is not a \n",
    "# fn.__closure__ # cell object at address xxx containing a str object ('python') at address yyy\n",
    "# the memory address of both the `x` (local and free) is the same and pointing to yyy\n",
    "\n",
    "fn.__code__.co_freevars, fn.__closure__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have multiple instance of the same closure, this means che each time a cell object is created, leaving the behavior of the different instance of the closure independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counter():\n",
    "  # beginning of Closure\n",
    "  count = 0\n",
    "\n",
    "  def inc():\n",
    "    nonlocal count\n",
    "    count += 1\n",
    "    return count\n",
    "  # end of Closure\n",
    "\n",
    "  return inc\n",
    "\n",
    "f1 = counter()\n",
    "f2 = counter()\n",
    "# f1 and f2 behavior is independent\n",
    "\n",
    "f1()\n",
    "f1()\n",
    "f1(), f2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared extend scope\n",
    "At the same time we can have `shared extended scope` of two different closures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer():\n",
    "  count = 0\n",
    "\n",
    "  def inc1():\n",
    "    nonlocal count\n",
    "    count += 1\n",
    "    return count\n",
    "\n",
    "  def inc2():\n",
    "    nonlocal count\n",
    "    count += 1\n",
    "    return count\n",
    "\n",
    "  return inc1, inc2\n",
    "\n",
    "f1, f2 = outer() \n",
    "\n",
    "f1()\n",
    "f2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f1` and `f2` are two closure that share the free variable `count` therefore both the functions, when called, will increment the value of count. If this behavior is wanted, then no problem, but often happens to share the same free variable without knowing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of functions that add a two values\n",
    "adders = []\n",
    "for n in range(1, 4):\n",
    "  adders.append(lambda x: x + n)\n",
    "\n",
    "# what we expect to have is a list of functions\n",
    "adders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# therefore calling \n",
    "adders[1](10) # should return 12 = 10 + 2 \n",
    "# instead all the three functions will add 3, i.e. the last value at which n was pointing to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n` is a global variable, and it doesn't get evaluated until the function is called, and at that time, after the for loop is executed is equal to 3. As a matter of fact we don't have a closure since `n` is a global variable. The correct way to achieve this would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adders():\n",
    "  adders = []\n",
    "  for n in range(1, 4):\n",
    "    adders.append(lambda x, y=n: x + y) # in this way we are saving the value of n at each iteration \n",
    "  return adders\n",
    "\n",
    "adders = create_adders()\n",
    "\n",
    "adders[1](10)\n",
    "\n",
    "# since we have specified a default value for `y`, this will be evaluated at creation time, not at runtime (i.e. when the function is called).\n",
    "# `y` won't point to the object `n` itself but to its value at each iteration.\n",
    "# Therefore, `y=n` belong to the local scope of the `create_adders` function, therefore, the functions appended to adders are actually closures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Closure\n",
    "It is common, e.g. in decorators, to have nested closures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes an increment and a starting values and return a function that add the increment each time is called. The\n",
    "def increment(n):\n",
    "\n",
    "  def inner(start):\n",
    "    current = start\n",
    "    \n",
    "    def inc():\n",
    "      nonlocal current\n",
    "      current += n\n",
    "      return current\n",
    "\n",
    "    return inc\n",
    "  return inner\n",
    "\n",
    "# Now inc has two free variables (current, n) one that lives in the `inner` scope and one in the `increment` scope.\n",
    "# if we call:\n",
    "fn = increment(2) # we will return the inner function with the variable n = 2\n",
    "fn.__code__.co_freevars # `n` is the free variable of the closure containing the `inner` function\n",
    "# if we than call:\n",
    "inc_2 = fn(100) # we will return the `inc` function with the variable n = 2 and current = 100\n",
    "inc_2.__code__.co_freevars # `n` and `current` are the free variables of the closure containing the `inc` function\n",
    "# now if we call:\n",
    "inc_2() # -> 102\n",
    "inc_2() # -> 104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Application\n",
    "\n",
    "*hold*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorators\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decorator is a function that takes a function as argument and returns a closure (that in general accept any number of arguments *args and **kwargs) that contain that same function passed with the addition of extra functionality. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(fn): # counter takes a function as argument\n",
    "  count = 0\n",
    "  def inner(*args, **kwargs): \n",
    "    nonlocal count\n",
    "    count +=1\n",
    "    print(f'Function {fn.__name__} was called {count} times')\n",
    "    return fn(*args, **kwargs)\n",
    "  return inner\n",
    "\n",
    "def add(a, b):\n",
    "  return a + b\n",
    "\n",
    "add = counter(add) # closure function is returned by counter()\n",
    "# now add is no more referencing to the 'def add' function but to it's decorated version, returned by counter()  \n",
    "\n",
    "result = add(1, 2) # -> 3\n",
    "result = add(1, 3) # -> 4\n",
    "result = add(5, 2) # -> 7\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, `counter()` is essentially a `decorator`; it takes an arbitrary function with any arbitrary arguments, and return the same function with the new \"ability\" of taking track of how many times it has been called. We reassigned the name add to the decorated function, to pointing out that the function is still the same but now points to the closure returned by `counter()`. Returning a closure is something pretty common in python, therefore and handy way has been defined to decorate a function using the `@` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the function counter has been defined from previous example\n",
    "\n",
    "@counter\n",
    "def add(a, b):\n",
    "  '''Documentation'''\n",
    "  return a + b\n",
    "\n",
    "add(1,2)\n",
    "add(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good so far, but now if we look for the metadata of the function `add` we'll see that these now refers to the closure function `inner` and not to the original definition (`__name__`, `__doc__` etc. point to the closure function). The pythonic solution to this problem is to use the module `functools.wraps`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add.__name__, add.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def counter(fn):\n",
    "  count = 0\n",
    "  @wraps(fn) # we are decorating the inner function\n",
    "  def inner(*args, **kwargs): \n",
    "    nonlocal count\n",
    "    count +=1\n",
    "    print(f'Function {fn.__name} was called {count} times')\n",
    "    return fn(*args, **kwargs)\n",
    "  return inner\n",
    "\n",
    "@counter\n",
    "def add(a, b):\n",
    "  '''Documentation'''\n",
    "  return a + b\n",
    "\n",
    "add.__name__, add.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple decorators\n",
    "Multiple decorators can be passed to a function; care must be taken to ensure that the order of execution of the two or more decorators respect what wanted by the coder. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_1(fn):\n",
    "  def inner(*args, **kwargs):\n",
    "    print('dec_1 called')\n",
    "    result = fn() # calling the decorated function\n",
    "    return result\n",
    "  return inner # return the closure\n",
    "\n",
    "def dec_2(fn):\n",
    "  def inner(*args, **kwargs):\n",
    "    print('dec_2 called')\n",
    "    result = fn() # calling the decorated function\n",
    "    return result\n",
    "  return inner # return the closure\n",
    "\n",
    "\n",
    "@dec_1\n",
    "@dec_2\n",
    "def my_func():\n",
    "  print('my_func called')\n",
    "\n",
    "'''\n",
    "Calling my_func, decorated in this order, is equal to do:\n",
    "\n",
    "my_func = dec_1(dec_2(my_func))\n",
    "\n",
    "Therefore, first the closure dec_2(my_func) is evaluated and passed to dec_1(). \n",
    "Since inside the decorators the print() is executed before the function call (result = fn()), the printing output will be:\n",
    "\n",
    "dec_1 called\n",
    "dec_2 called\n",
    "my_func called\n",
    "\n",
    "because first the dec_1 function is called, it prints its output, then call fn passed as argument, which is dec_2(my_func);\n",
    "therefore dec_2 is called, it prints its output, then call the fn passed as argument, i.e. my_func, that prints its output.\n",
    "N.B. if the print() had been placed after the fn() call, the print-out order would had been reversed!\n",
    "This is to say that depending on the functionality we want to implement with our decorators, the order of application matters!\n",
    "'''\n",
    "\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoization\n",
    "Another very powerful application of decorators is called `memoization`, i.e. the process of storing data into cache to avoid excessive recursive calculation (like in the fibonacci or factorial function). Let's take as an example a function to compute the fibonacci value at the n position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with recursion\n",
    "def fib(n):\n",
    "  print(f'computing fib({n})')\n",
    "  return 1 if n < 3 else fib(n-1) + fib(n-2)\n",
    "\n",
    "fib(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, the function works and it is elegant, but it is not performant since it has to compute each time all the previous numbers in the fibonacci series. A way to solve this problem is to cache the results each time they are computed, and this can be easily implemented creating a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fib:\n",
    "  def __init__(self):\n",
    "    self.cache = {1: 1, 2: 1}\n",
    "\n",
    "  def fib(self, n):\n",
    "    if n not in self.cache:\n",
    "      print(f'computing fib({n})')\n",
    "      self.cache[n] = self.fib(n-1) + self.fib(n-2)\n",
    "    return self.cache[n]\n",
    "\n",
    "f = Fib()\n",
    "f.fib(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, after creating an instance of Fib(), fibonacci sequence will be stored while computed (n.b. cache won't be shared between instances, each new instance will have its cache empty at the beginning). The same can be accomplished with a closure (i.e. with a decorator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib():\n",
    "  cache = {1: 1, 2: 1}\n",
    "\n",
    "  def calc_fib(n):\n",
    "    if n not in cache:\n",
    "      # cache is a nonlocal parameter\n",
    "      print(f'computing fib({n})')\n",
    "      cache[n] = calc_fib(n-1) + calc_fib(n-2)\n",
    "    return cache[n] \n",
    "\n",
    "  return calc_fib # return the closure\n",
    "\n",
    "f = fib()\n",
    "f(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the function `fib()` to a decorator the path is short; we just need to generalize its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoize_fib(fib):\n",
    "  cache = dict()\n",
    "\n",
    "  def inner(n):\n",
    "    if n not in cache:\n",
    "      # the decorator is not carrying out the recursion\n",
    "      # it is only caching values\n",
    "      cache[n] = fib(n)\n",
    "    return cache[n] \n",
    "\n",
    "  return inner # return the closure\n",
    "\n",
    "@memoize_fib\n",
    "def fib(n):\n",
    "  print(f'computing fib({n})')\n",
    "  return 1 if n < 3 else fib(n-1) + fib(n-2)\n",
    "\n",
    "fib(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth to note that `memoize_fib` is not a general purpose decorator since it does not accept any number of arguments or keyword arguments (*args, **kwargs) as it usually does, but it is precisely built to work with the function `fib`. Another important aspect to handle is to limit the cache size to safeguard the tradeoff between performance and memory usage. Of course python as already a builtin decorator specifically design for memoization. It comes shipped with the `functools` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache # least recently used cache\n",
    "\n",
    "@lru_cache() # lru_cache decorators accept arguments.. see below\n",
    "def fib(n):\n",
    "  print(f'computing fib({n})')\n",
    "  return 1 if n < 3 else fib(n-1) + fib(n-2)\n",
    "\n",
    "fib(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrized decorators\n",
    "Parametrized decorators are the ones that can handle arguments (such as `wrap` and `lru_cache`). Imagine we have a decorator that run a function a number of time `n` set by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_times(fn):\n",
    "  n =3\n",
    "\n",
    "  def inner(*args, **kwargs):\n",
    "    for _ in range(n):\n",
    "      fn(*args, **kwargs)\n",
    "    return print(f'{fn.__name__} was called {n} times')\n",
    "\n",
    "  return inner\n",
    "\n",
    "@run_n_times\n",
    "def my_func():\n",
    "  print('called')\n",
    "  pass\n",
    "\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the number of times the function is called has been hardcoded in the decorator, but we want to be able to change that parameter. We can think at something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_times(fn, n: int):\n",
    "  def inner(*args, **kwargs):\n",
    "    for _ in range(n):\n",
    "      fn(*args, **kwargs)\n",
    "    return print(f'{fn.__name__} was called {n} times')\n",
    "\n",
    "  return inner\n",
    "\n",
    "# now we would expect to call the decorator as:\n",
    "@run_n_times(10)\n",
    "def my_func():\n",
    "  print('called')\n",
    "  pass\n",
    "\n",
    "# but we get an error since run_n_times requires two arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func():\n",
    "    print('called')\n",
    "    pass    \n",
    "\n",
    "# however, the argument `10` in the decorator call is in the position of `fn`, therefore it won't work.\n",
    "# we could instead apply the decorator indirectly as:\n",
    "my_func = run_n_times(my_func, 3)\n",
    "# and this will work but how to implement the same behavior with the @ method?\n",
    "\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to use the `@` symbol with a decorator that accept arguments, we need that decorator to return a decorator itself when called. The result of `run_n_times(10)` has to be another decorator that actually perform the decoration we want. The solution is straightforward: we need to enclose our decorator in a `decorator factory` that will olds the extra parameters needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_times(n: int): # decorator factory\n",
    "\n",
    "  def inner1(fn): # decorator\n",
    "\n",
    "    def inner2(*args, **kwargs):\n",
    "      for _ in range(n):\n",
    "        fn(*args, **kwargs)\n",
    "      return print(f'{fn.__name__} was called {n} times')\n",
    "\n",
    "    return inner2\n",
    "  \n",
    "  return inner1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the call `@run_n_times(10)` actually returns the decorator `inner1` which implement the functionality we originally looked for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_n_times(3) # returns the decorator `inner1`\n",
    "def my_func():\n",
    "  print('called')\n",
    "  pass\n",
    "\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is equivalent to say:\n",
    "my_func = run_n_times(3)(my_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator class\n",
    "Not only functions can be used to create decorators factory, but also classes. As a matter of fact, thanks to the `__call__` method, we can replicate the same exact behavior seen in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "  def __init__(self, n): # the instance of the class become the decorator factory\n",
    "    self.n = n\n",
    "\n",
    "  def __call__(self, fn): # this is the actual decorator\n",
    "    def inner(*args, **kwargs):\n",
    "      for _ in range(self.n):\n",
    "        fn(*args, **kwargs)\n",
    "      return print(f'{fn.__name__} was called {self.n} times')\n",
    "    \n",
    "    return inner # closure\n",
    "\n",
    "@MyClass(3)\n",
    "def my_func():\n",
    "  print('called')\n",
    "\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkey Patching and Decorating classes\n",
    "Functions are not the only object that can be decorated; Classes too can thanks to the dynamic behavior of python that allows the so called `Monkey Patching`, i.e. the modification/addition of attributes/methods to classes at runtime. Essentially, we are able to mutate the behavior of a class at runtime. Imagine we are using the class `Fraction` and we want to add to it some functionality; we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "f = Fraction(2,3) # create an instance of the Fraction class\n",
    "# we want the class `Fraction` to be able to speak ...\n",
    "# if we write:\n",
    "Fraction.speak = 100\n",
    "# we are Monkey Patching the class Fraction at runtime, so if following we say:\n",
    "\n",
    "f.speak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the `Monkey Patched` methods also callable, for example using a lambda function (we can directly patch the class instead of an instance of it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraction.speak = lambda self, message: f'Fraction says {message}'\n",
    "\n",
    "# we need `self` as argument because we will pass to the method an instance of the class Fraction\n",
    "# Now we can call:\n",
    "f.speak('You cannot pass!') # -> 'Fraction says You cannot pass!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the process of monkey-patching is essentially a decoration of a class and, as a matter of fact it can be done with a decorator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator_speak(cls): # we are passing a class to the function\n",
    "  cls.speak = lambda self, message: f'{self.__class__.__name__} says {message}'\n",
    "  return cls # return is only needed if we want to decorate with the `@` symbol\n",
    "\n",
    "# Now we can simply write on any class:\n",
    "class Person:\n",
    "  pass\n",
    "\n",
    "Person = decorator_speak(Person) # indirect decoration\n",
    "p = Person() # instance of the class\n",
    "p.speak('I am ALIVE!') # method inherited from the decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something more useful; Imagine we want to debug an existing class creating a decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(obj): # think of this as of the method we would write inside the class, i.e. 'obj' would be 'self'\n",
    "  from datetime import datetime, timezone\n",
    "  results = {\n",
    "    'time' : f'{datetime.now(timezone.utc)}',\n",
    "    'name' : obj.__class__.__name__,\n",
    "    'id' : hex(id(obj)),\n",
    "    'vars' : [(k,v) for k, v, in vars(obj).items()]\n",
    "  }\n",
    "  return results\n",
    "\n",
    "\n",
    "def debug_class(cls): # This is the decorator\n",
    "  cls.debug = info\n",
    "  return cls \n",
    "\n",
    "\n",
    "# if we want to pass the decorator in function-style:\n",
    "debug_class(Person)\n",
    "# we don't need the function to 'return cls' since we are modifying an object inplace.\n",
    "# However, if we want to use the `@` we need the return, because otherwise, the default return is 'None'.\n",
    "Person = debug_class(Person) \n",
    "# the rhs is returning None and it is assign it to 'Person' that therefore doesn't point anymore to the class Person nor to its decorated version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@debug_class\n",
    "class Person:\n",
    "  def __init__(self, name, age, employed=True):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "    self.employed = employed\n",
    "\n",
    "p = Person('Giovanni', 32)\n",
    "p.debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Dispatch Generic Functions\n",
    "First lets define what overloading is:\n",
    "\n",
    "`Overloading` in object oriented programming is the ability to create more then a function with the same name al long as its signature is different (essentially if the two functions are distinguishable, i.e. different number/type of arguments etc..). When the program is compiled, the interpreter will understand, based on the signature at which function with the same name we are referring to. \n",
    "\n",
    "In python, since there is no static typing, we can't declare a function signature, therefore, overloading, in its strict sense, is not possible. A workaround to this problem is called  `single dispatch generic function`, which allows us to overload functions based on the type of the first argument (if we want to consider the type of more arguments we need `multi dispatch`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application - `Htmlizer`\n",
    "Link to [Single_dispatch_generic_function_Htmlizer](Single_dispatch_generic_function_Htmlizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python optimizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interning\n",
    "Python at startup automatically pre-loads (caches) a global list of integer in the range [-5, 256], so these integers have a fixed memory reference. Since these numbers show up often, avoid to reference these each time they appear results in an optimization. A number outside this range will require a new memory reference, and that's why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 500\n",
    "b = 500\n",
    "a is b # will return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The caches integers are called `Singletons`, basically classes that can be instantiated only once.\n",
    "\n",
    "The same might happen with some strings; python can interning some string (that follow certain rules, letters and numbers concatenated with underscores) in order to speed up the equality (if a string in interned than i can use the `is` operator, otherwise i have to use the `==` character by character). We can force python to interning strings with the sys module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "usually is something we don't need, unless for example we are working with a large ste of string\n",
    "for NPL and we need to tokenize some words that are reaped often. In this case it can be a useful optimization,\n",
    "since if a string is interned it becomes a Singleton and can be compared with the mush faster 'is' operator.\n",
    "'''\n",
    "import sys\n",
    "\n",
    "a = sys.intern('this will be interned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peephole\n",
    "Is an optimization that occur at compile time (so it is repeated each time the script is launched). For example we can have `Constant expression` like numeric calculation thata are better read as the operation rather than the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_in_day = 60 * 24 # 1440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression `60 * 24` is more readable than `1440` but we may thing that, if the variable is called multiple times, we may have performance issues. This is not the case because this is a constant expression and python knows it, so the first time it encounters the variable stores its results, without having to compute it again.\n",
    "\n",
    "The same happen for membership tests, i.e. check if an object is in a list. If we have a constant expression, python will replace the mutable object with is immutable counterpart (list-> tuples, sets -> fronzensets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```py\n",
    "for i in range(100000):\n",
    "  if i in [1,2,3]:\n",
    "    pass\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list `[1,2,3]` is converted into a tuple `(1,2,3)` so that, being immutable, it has a fixed memory address.\n",
    "\n",
    "N.B. sets, since are similar to dictionaries (hashmaps), are much more efficient than lists for membership testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Modules\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `string`\n",
    "Module with some useful string constants and representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `functools`\n",
    "Module with useful functions:\n",
    "\n",
    "* `total_ordering` : decorator for classes that automatically implement comparison functionality (le, ge, lt, gt), if only one of these is already implemented\n",
    "* `reduce`: iterate over a sequence applying a function\n",
    "* `partial`: lets us set some arguments of a function as default parameters\n",
    "* `wraps`: decorator that allow to wrap a function/class metadata and keep it after the decoration\n",
    "* `lru_cache`: decorator that allows caching data in recursive structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `itertools`\n",
    "Module with useful iteration methods:\n",
    "* `cycle` create a cyclic iterator from an iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `collections`\n",
    "Module with useful functions:\n",
    "\n",
    "* `namedtuple` : tuple with argument assignment - substitute of classes or dictionary in some case\n",
    "* `Counter`: it is a class that takes a list returns a dictionary with the number of occurence of each element in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `random`\n",
    "Pseudo random number generator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `seed` value is required to create a repeatable random sequence (essential for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0) # now i can execute thi cell as many time as i want but the output won't change\n",
    "import random\n",
    "for _ in range(3):\n",
    "    print(random.randint(2,10), end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `shuffle` inplace mixing up of a list \n",
    "* `gauss` draw numbers from a gaussian distribution\n",
    "* `choice` draw a random element from a list\n",
    "* `choices` draw a defined number of random element from a list; it has the option to weights the appereance of the elements in the list.\n",
    "* `sample` draw a sample from a list without repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `timeit`\n",
    "Platform specific timer for performance evaluation of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es. of usage\n",
    "timeit(stmt='math.sqrt(2)', setup='import math', number=n, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `argparser`\n",
    "When we run a python script from terminal it might be useful to be able to pass some arguments/variables that will be used by the script itself. The easiest way to retrieve command arguments is to use the `sys.argv` method which returns a list of strings containing the name of the module runned and the argument passed (arguments must be whitespace-separated).\n",
    "\n",
    "However, the smart way of doing it is to use the builtin module `argparser`:\n",
    "\n",
    "```py\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Description of the parser\")\n",
    "\n",
    "# now we populate the parser with what we expect to retrieve in the command line\n",
    "parser.add_argument('first_arg', help='description of first arg', type=str)\n",
    "parser.add_argument('second_arg', help='description of second arg', type=int)\n",
    "\n",
    "# now we need to tell the parser to parse these arguments from sys.argv[1:] \n",
    "# (by default if nothing specified inside parse_args())\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.first_argument)\n",
    "print(args.second_argument)\n",
    "```\n",
    "\n",
    "We can call the module from terminal with the flag `-h` to receive in output the descriptions of the parser, that should help us understand the expected usage of the module\n",
    "\n",
    "We can also specify keyword arguments and many more options:\n",
    "\n",
    "```py\n",
    "parser.add_argument('-kw', '--keyword', help='first kw arg', type=str, required=False, dest='alias name')\n",
    "```\n",
    "\n",
    "where the first two arguments are the short and long way of assign the kw argument in the command line, `required` is to specifuy if the argument is mandatory and `dest` gives an alias to the variable that will be actually used in the code. Other argumnet that we may be interested in are:\n",
    "* `nargs` to accept more value per argument. It can be equal to `+` or `*` depending if we require at least one argument or not \n",
    "* `action` to specify a behavior on the argument like `store_true`, `store_constant` etc..\n",
    "\n",
    "Another useful think is to define a group of mutually exclusive arguments (i.e only one can be specified not both), particularly useful when creating flags:\n",
    "\n",
    "```py\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument('-v', '--verbose', action='store_true')\n",
    "group.add_argument('-q', '--quite', action='store_true')\n",
    "# so doing only one betweee -v and -q can be specified\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptions handling\n",
    "\"look before you need\" or \"ask for permission\" are two different approach we can use to catch errors; the first corrispond to try and except, the latter to and if statement. Generally speaking the if statement is faster than the try-except but it may be less expressive; moreover the try-except becomes a burden only if the exception is raised often, otherwise there is not much difference in computational time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### which import is faster\n",
    "N.B. it matters only if the code is runned a humongus number of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "# SLOWER\n",
    "import math\n",
    "\n",
    "# FASTER\n",
    "from math import sqrt\n",
    "\n",
    "n = 10_000_000\n",
    "\n",
    "timeit(stmt='math.sqrt(2)', setup='import math',number=n), timeit(stmt='sqrt(2)', setup='from math import sqrt',number=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel values instead of None\n",
    "Sometimes we need to define function or classe with kw arguments and have a way to check if the user passed thata argument or not. The standard way of proceeding is to set the default value to `None`. This is fine, but sometimes `None` itself can be an acceptable parameter given by the user, but we won't catch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_func(kw=None):\n",
    "    if not kw:\n",
    "        print('kw was not passed')\n",
    "    else:\n",
    "        print('kw was passed')\n",
    "        \n",
    "some_func(), some_func(None), some_func(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, passing `None` as argument result in the wrong behavior (because actually an argument was passed.. it was None!). (btw.  in this case the same goes for any argument that has a truth values of `False`). An alternative approach is tu set a sentinel values as a flag, something so unique that the user could never use it.\n",
    "\n",
    "To do this, a smart idea is to use as sentinel value the `id` of an object since it will be unique in memory. The most genereci way is to use the id of the python object class `id(object()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SENTINEL = object()\n",
    "\n",
    "def some_func(kw=_SENTINEL):\n",
    "    if kw is _SENTINEL:\n",
    "        print('kw was not passed')\n",
    "    else:\n",
    "        print('kw was passed')\n",
    "        \n",
    "some_func(), some_func(None), some_func(0), some_func([]), some_func(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, now the behavior is correct and we know exactly if the user has passed an argument. Actually we could have set directly the values of `kw` equal to `object()` since it would have been created at runtime by python when encounter the function definition, thus that object id would have been unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch statement in python PEP 3103\n",
    "In other programming language, like Java, a switch is a structure that holds different values, and is able to switch from one to the other depending on the key value passed. In python we have different way to simulate this behavior, the simplest (and worst) is with a series of `if, elif` statements, using a dictionary or, more elegantly using an associative array like a single_dispatch_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to [TOC](#TOC)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b66e1d652d26545cf719f53d79fc7f17380815b403cf9c9ba2b908f3efa6c6a8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
