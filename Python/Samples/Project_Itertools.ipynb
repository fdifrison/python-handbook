{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Itertools module\n",
    "\n",
    "We have a set of 4 *.csv* files with presorted data, where the primamry ckey is the `SSN`\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. [Goal 1](#Goal-1)\n",
    "Create a (lazy) iterator for each file that return named tuple with appropriate data type\n",
    "2. [Goal 2](#Goal-2)\n",
    "Create a Single iterable that combines the data from all the 4 files\n",
    "3. [Goal3](#Goal-3)\n",
    "Based on the 'update_status.csv', filter out the data that have a last update date `< 3/1/2017`\n",
    "4. [Goal4](#Goal-4)\n",
    "Using the filtered data from Goal 3, generate a group of number of car makes divided by gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1\n",
    "Create a (lazy) iterator for each file that return named tuple with appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from itertools import chain, compress, groupby, tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# navigate to data folder\n",
    "import os\n",
    "\n",
    "os.chdir('./Project_itertools_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['employment.csv', 'personal_info.csv', 'update_status.csv', 'vehicles.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir()) # NB the order is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the reader function using csv\n",
    "def csv_parser(file, *,  delimiter=',', quotechar='\"', include_header=False):\n",
    "    with open(file) as f:\n",
    "        if not include_header:\n",
    "            next(f) # skip header\n",
    "        reader = csv.reader(f, delimiter=delimiter, quotechar=quotechar)\n",
    "        yield from reader\n",
    "        \n",
    "def header_class(file, class_name:str):\n",
    "    with open(file) as f:\n",
    "        reader = csv_parser(file, include_header=True)\n",
    "        return namedtuple(class_name, next(reader))\n",
    " \n",
    "def row_parser(row, dtype):\n",
    "    row = (func(r) for func, r in zip(dtype, row))\n",
    "    return row\n",
    "\n",
    "def iter_files(fname, ntuple, dtype):\n",
    "    reader = csv_parser(fname)\n",
    "    for row in reader:\n",
    "        parsed_row = row_parser(row, dtype)\n",
    "        yield ntuple(*parsed_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dtypes parser\n",
    "def parse_date(value, *, default=None, fmt='%Y-%m-%dT%H:%M:%SZ'):\n",
    "    format_ = fmt\n",
    "    try:\n",
    "        return datetime.strptime(value, format_)\n",
    "    except ValueError:\n",
    "        return default    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets discover the header of each file in order to identify how many parser we need to properly convert the data to a proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('employer', 'department', 'employee_id', 'ssn')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employment_nt = header_class('employment.csv', 'Employment')\n",
    "employment_nt._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_dtype = (str, str, str, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ssn', 'vehicle_make', 'vehicle_model', 'model_year')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veichles_nt = header_class('vehicles.csv', 'Vehicle')\n",
    "veichles_nt._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "veichles_dtype = (str, str, str, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ssn', 'first_name', 'last_name', 'gender', 'language')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_info_nt = header_class('personal_info.csv', 'PersonalInfo')\n",
    "personal_info_nt._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_info_dtype = (str, str, str, str, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ssn', 'last_updated', 'created')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_status_nt = header_class('update_status.csv', 'UpdateStatus')\n",
    "update_status_nt._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_status_dtype = (str, parse_date, parse_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the files have a common presorted field that uniquely identify the data (primary key).\n",
    "\n",
    "Now, to process all the files together we can create a series of tuples that contain the pieces needed to parse:\n",
    "\n",
    "* the files name\n",
    "* the corrisponding namedTuples\n",
    "* the dtypes corresponding to each file\n",
    "\n",
    "N.B. it is important to have the same ordering since we are going to zip the 3 tuples together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('employment.csv', __main__.Employment, (str, str, str, str))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = sorted(os.listdir())\n",
    "ntuples = (employment_nt, personal_info_nt, update_status_nt, veichles_nt)\n",
    "dtypes = (employment_dtype, personal_info_dtype, update_status_dtype, veichles_dtype)\n",
    "\n",
    "# check if the order is consistent\n",
    "list(zip(fnames, ntuples, dtypes))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over all the files at the same time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************employment.csv*************\n",
      "Employment(employer='Stiedemann-Bailey', department='Research and Development', employee_id='29-0890771', ssn='100-53-9824')\n",
      "\n",
      "***********personal_info.csv************\n",
      "PersonalInfo(ssn='100-53-9824', first_name='Sebastiano', last_name='Tester', gender='Male', language='Icelandic')\n",
      "\n",
      "***********update_status.csv************\n",
      "UpdateStatus(ssn='100-53-9824', last_updated=datetime.datetime(2017, 10, 7, 0, 14, 42), created=datetime.datetime(2016, 1, 24, 21, 19, 30))\n",
      "\n",
      "**************vehicles.csv**************\n",
      "Vehicle(ssn='100-53-9824', vehicle_make='Oldsmobile', vehicle_model='Bravada', model_year=1993)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fname, ntuple, dtype, in zip(fnames, ntuples, dtypes):\n",
    "    file_iter = iter_files(fname, ntuple, dtype)\n",
    "    print(fname.center(40, '*'))\n",
    "    for _ in range(1): # check\n",
    "        print(next(file_iter))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2\n",
    "Create a Single iterable that combines the data from all the 4 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.B. The following is my approach, then we will revise Freed approach (ofc the best one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [iter_files(fname, ntuple, dtype)\n",
    "          for fname, ntuple, dtype in zip(fnames, ntuples, dtypes)]\n",
    "def combine_files(result):\n",
    "    #print(result)\n",
    "    r1, r2, r3, r4 = result\n",
    "    for r in zip(r1, r2, r3, r4):\n",
    "        temp = {**r[0]._asdict(), **r[1]._asdict(), **r[2]._asdict(), **r[3]._asdict()}\n",
    "        keys = temp.keys()\n",
    "        values = temp.values()\n",
    "        combined = namedtuple('Data', keys)\n",
    "        yield combined(*values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(employer='Stiedemann-Bailey', department='Research and Development', employee_id='29-0890771', ssn='100-53-9824', first_name='Sebastiano', last_name='Tester', gender='Male', language='Icelandic', last_updated=datetime.datetime(2017, 10, 7, 0, 14, 42), created=datetime.datetime(2016, 1, 24, 21, 19, 30), vehicle_make='Oldsmobile', vehicle_model='Bravada', model_year=1993)\n",
      "\n",
      "Nicolas and Sons\n",
      "Connelly Group\n",
      "Upton LLC\n",
      "Zemlak-Olson\n",
      "Kohler, Bradtke and Davis\n",
      "Roberts, Torphy and Dach\n",
      "Lind-Jast\n",
      "Bashirian-Lueilwitz\n",
      "Windler, Marks and Haley\n",
      "Leffler-Hahn\n"
     ]
    }
   ],
   "source": [
    "combined = combine_files(result)\n",
    "\n",
    "print(next(combined), end='\\n\\n')\n",
    "\n",
    "for _ in range(10):\n",
    "    print(next(combined).employer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fred solution:\n",
    "\n",
    "Fisrt, we create an inclusion/esclusion tuple for each column in the files to be able to tune specifically wich field we want to retain (in this case everithing but one ssn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_fields = (True, True, True, False)\n",
    "personal_info_fields = (True, True, True, True, True)\n",
    "veichles_fields = (False, True, True, True)\n",
    "update_status_fields = (False, True, True)\n",
    "\n",
    "fields = (employment_fields, personal_info_fields, update_status_fields, veichles_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_files(fname, ntuple, dtype):\n",
    "    reader = csv_parser(fname)\n",
    "    for row in reader:\n",
    "        parsed_row = row_parser(row, dtype)\n",
    "        yield ntuple(*parsed_row)\n",
    "\n",
    "\n",
    "def create_combo_named_tuple(fnames, fields):\n",
    "    fields = chain.from_iterable(fields)\n",
    "    field_names = chain.from_iterable(header_class(fname, fname.strip('.csv'))._fields for fname in fnames)\n",
    "    compressed_field_names = compress(field_names, fields)\n",
    "    return namedtuple('Data', compressed_field_names)\n",
    "    \n",
    "        \n",
    "def iter_combined(fnames, ntuple, dtype, fields):\n",
    "    # create the jointed namedtuple with only the specified fields\n",
    "    combo_nt = create_combo_named_tuple(fnames, fields)\n",
    "    # zipped_tuples is an iterator of tuples that contain 4 iterator generated by iter_files\n",
    "    zipped_tuples = zip(*(iter_files(fname, ntuple, dtype)\n",
    "                          for fname, ntuple, dtype in zip(fnames, ntuples, dtypes)))\n",
    "    # now we merge the 4 iterator inside the zip\n",
    "    # merge_iter is another iterator that contains only the values of one row of the 4 files chained together\n",
    "    merged_iter = (chain.from_iterable(zipped_tuple) for zipped_tuple in zipped_tuples)\n",
    "    # now we want to compress the data based on the valuse in 'fields' toretain only what we want\n",
    "    # first we need to chain the fields together in order to have the same dimension of 'merged_iter'\n",
    "    fields = tuple(chain.from_iterable(fields))\n",
    "    for row in merged_iter:\n",
    "        compressed_row = compress(row, fields) # carefull! fields will be exahusted after the first loop! unless we make 'fields' an iterable (es. creating a tuple from it)\n",
    "        yield combo_nt(*compressed_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(employer='Stiedemann-Bailey', department='Research and Development', employee_id='29-0890771', ssn='100-53-9824', first_name='Sebastiano', last_name='Tester', gender='Male', language='Icelandic', last_updated=datetime.datetime(2017, 10, 7, 0, 14, 42), created=datetime.datetime(2016, 1, 24, 21, 19, 30), vehicle_make='Oldsmobile', vehicle_model='Bravada', model_year=1993)\n",
      "\n",
      "Nicolas and Sons\n",
      "Connelly Group\n",
      "Upton LLC\n",
      "Zemlak-Olson\n",
      "Kohler, Bradtke and Davis\n",
      "Roberts, Torphy and Dach\n",
      "Lind-Jast\n",
      "Bashirian-Lueilwitz\n",
      "Windler, Marks and Haley\n",
      "Leffler-Hahn\n"
     ]
    }
   ],
   "source": [
    "combined = iter_combined(fnames, ntuple, dtype, fields)\n",
    "\n",
    "print(next(combined), end='\\n\\n')\n",
    "\n",
    "for _ in range(10):\n",
    "    print(next(combined).employer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 3\n",
    "Based on the 'update_status.csv', filter out the data that have a last update date `< 3/1/2017`\n",
    "\n",
    "As always, first my idea and then fred's. His solution is more general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_on_update(combo_nt, date):\n",
    "    date = map(int,date.split('/'))\n",
    "    if combo_nt.last_updated > datetime(*list(date)[::-1]): # datetime wants (year, month, day)\n",
    "        yield combo_nt\n",
    "        \n",
    "        \n",
    " \n",
    "def iter_combined_filter_date(fnames, ntuple, dtype, fields):\n",
    "    # create the jointed namedtuple with only the specified fields\n",
    "    combo_nt = create_combo_named_tuple(fnames, fields)\n",
    "    # zipped_tuples is an iterator of tuples that contain 4 iterator generated by iter_files\n",
    "    zipped_tuples = zip(*(iter_files(fname, ntuple, dtype)\n",
    "                          for fname, ntuple, dtype in zip(fnames, ntuples, dtypes)))\n",
    "    # now we merge the 4 iterator inside the zip\n",
    "    # merge_iter is another iterator that contains only the values of one row of the 4 files chained together\n",
    "    merged_iter = (chain.from_iterable(zipped_tuple) for zipped_tuple in zipped_tuples)\n",
    "    # now we want to compress the data based on the valuse in 'fields' toretain only what we want\n",
    "    # first we need to chain the fields together in order to have the same dimension of 'merged_iter'\n",
    "    fields = tuple(chain.from_iterable(fields))\n",
    "    for row in merged_iter:\n",
    "        compressed_row = compress(row, fields) # carefull! fields will be exahusted after the first loop! unless we make 'fields' an iterable (es. creating a tuple from it)\n",
    "        #filter_on_update(combo_nt(*compressed_row), '3/1/2017')\n",
    "        yield from filter_on_update(combo_nt(*compressed_row), '25/3/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-26 06:15:57\n",
      "2018-03-27 21:18:39\n",
      "2018-03-27 09:56:49\n",
      "2018-03-26 22:14:13\n",
      "2018-03-26 10:53:26\n",
      "2018-03-28 19:40:00\n",
      "2018-03-27 00:00:32\n",
      "2018-03-25 08:52:19\n",
      "2018-03-30 05:53:21\n",
      "2018-03-30 21:07:08\n",
      "2018-03-27 01:24:58\n"
     ]
    }
   ],
   "source": [
    "filter_combined = iter_combined_filter_date(fnames, ntuple, dtype, fields)\n",
    "\n",
    "for row in filter_combined:\n",
    "    print(row.last_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fred Solution:\n",
    "using `filter` function; the big advantage (apart from cleanes) is the fact that we can specify any filter on any field with a `lambda` function as `key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_combined_filterd(fnames, ntuple, dtype, fields, *, key=None):\n",
    "    combined = iter_combined(fnames, ntuple, dtype, fields)\n",
    "    yield from filter(key, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-26 06:15:57\n",
      "2018-03-27 21:18:39\n",
      "2018-03-27 09:56:49\n",
      "2018-03-26 22:14:13\n",
      "2018-03-26 10:53:26\n",
      "2018-03-28 19:40:00\n",
      "2018-03-27 00:00:32\n",
      "2018-03-25 08:52:19\n",
      "2018-03-30 05:53:21\n",
      "2018-03-30 21:07:08\n",
      "2018-03-27 01:24:58\n"
     ]
    }
   ],
   "source": [
    "filter_combined = iter_combined_filterd(fnames, ntuple, dtype, fields, key=lambda row: row.last_updated >= datetime(2018, 3, 25))\n",
    "for row in filter_combined:\n",
    "    print(row.last_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 4\n",
    "Using the filtered data from Goal 3, generate a group of number of car makes divided by gender\n",
    "As always, first my idea and then fred's. His solution is more general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_combined_filterd_grouped(fnames, ntuple, dtype, fields, *, _filter=None, _group=None):\n",
    "    combined = iter_combined(fnames, ntuple, dtype, fields)\n",
    "    filterd = filter(_filter, combined)\n",
    "    _sorted = sorted(filterd, key=lambda row: row.gender)\n",
    "    groupped = groupby(_sorted, key=lambda row: row.gender)\n",
    "    for group in groupped:\n",
    "        print(group[0].center(40, '*'), end='\\n\\n') # male or female\n",
    "        group_sorted = sorted(group[1], key=lambda row: row.vehicle_make)\n",
    "        group_sorted = groupby(group_sorted, key=lambda row: row.vehicle_make)\n",
    "        group_sorted = ((row[0], len(list(row[1]))) for row in group_sorted)\n",
    "        yield from sorted(test, key=lambda x: x[1], reverse=True)\n",
    "        print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Female*****************\n",
      "\n",
      "('Chevrolet', 42)\n",
      "('Ford', 42)\n"
     ]
    }
   ],
   "source": [
    "filter_combined = iter_combined_filterd_grouped(fnames, ntuple, dtype, fields,\n",
    "                                                _filter=lambda row: row.last_updated >= datetime(2017, 3, 1),\n",
    "                                                _group=lambda row: row.gender)\n",
    "for _ in range(2):\n",
    "    print(next(filter_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fred Solution: \n",
    "First a group_key function is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_key(item):\n",
    "    return (item.gender, item.vehicle_make)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our filtered dataset, sorting and then grouping it by the 'group_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_combined = iter_combined_filterd(fnames, ntuple, dtype, fields, key=lambda row: row.last_updated >= datetime(2017, 3, 1))\n",
    "sorted_combined = sorted(filter_combined, key=group_key)\n",
    "grouped = groupby(sorted_combined, key=group_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to be able to select one gender at the time; we can create a generator with an if condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Female', 'Acura'), <itertools._grouper object at 0x00000177E72AD488>)\n",
      "(('Female', 'Aston Martin'), <itertools._grouper object at 0x00000177E6E2F248>)\n"
     ]
    }
   ],
   "source": [
    "group_by_gender = (item for item in grouped if item[0][0] == 'Female')\n",
    "for _ in range(2):\n",
    "    print(next(group_by_gender))\n",
    "    \n",
    "# N.B. now 'grouped' has been exahusted, if we want to use it again we need to recreate it or teeing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to clean the key, since we dont need to display 'Female', and to compute the length of the itertools object; Then we need to sort by the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chevrolet 42\n",
      "Ford 42\n",
      "Ford 40\n"
     ]
    }
   ],
   "source": [
    "grouped = groupby(sorted_combined, key=group_key)\n",
    "cleaned = ((item[0][1], len(list(item[1]))) for item in grouped)\n",
    "sort = sorted(cleaned, key=lambda x: x[1], reverse=True) \n",
    "\n",
    "for i in range(3):\n",
    "    print(*sort[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved Approach\n",
    "Now, what Fred points out is that this approach is not efficient since we are sorting twice the whole dataset (sorting cost is not linear), while we could have sorted first by gender, split the dataset and only then sort by vehicle make. So lets see this approach.\n",
    "\n",
    "So first we are going to filter the dataset by gender, creating two distinct generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_key(item):\n",
    "    return item.vehicle_make\n",
    "\n",
    "data = iter_combined_filterd(fnames, ntuple, dtype, fields, key=lambda row: row.last_updated >= datetime(2017, 3, 1))\n",
    "data1, data2 = tee(data, 2)\n",
    "\n",
    "data_m = (item for item in data1 if item.gender == 'Male')\n",
    "sorted_data_m = sorted(data_m, key=group_key)\n",
    "grouped = groupby(sorted_data_m, key=group_key)\n",
    "cleaned = ((data[0], len(list(data[1]))) for data in grouped)\n",
    "table_m = sorted(cleaned, key=lambda x : x[1], reverse=True)\n",
    "\n",
    "data_f = (item for item in data2 if item.gender == 'Female')\n",
    "sorted_data_f = sorted(data_f, key=group_key)\n",
    "grouped = groupby(sorted_data_f, key=group_key)\n",
    "cleaned = ((data[0], len(list(data[1]))) for data in grouped)\n",
    "table_f = sorted(cleaned, key=lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ford 40 Chevrolet 42\n",
      "Chevrolet 30 Ford 42\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(*table_m[i], *table_f[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to cluster what we have done in a function to clean up the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_key = lambda row: row.last_updated >= datetime(2017, 3, 1)\n",
    "group_key = lambda row: row.vehicle_make\n",
    "\n",
    "def group_data(fnames, ntuple, dtype, fields, *, filter_key, group_key, gender):\n",
    "    data = iter_combined_filterd(fnames, ntuple, dtype, fields, key=filter_key)\n",
    "    filtered = (item for item in data2 if item.gender == gender)\n",
    "    sorted_data = sorted(filtered, key=group_key) # it is the only step that is not lazy\n",
    "    grouped = groupby(sorted_data_f, key=group_key)\n",
    "    cleaned = ((data[0], len(list(data[1]))) for data in grouped)\n",
    "    table = sorted(cleaned, key=lambda x : x[1], reverse=True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chevrolet', 42) ('Ford', 42)\n"
     ]
    }
   ],
   "source": [
    "result_female = group_data(fnames, ntuple, dtype, fields, filter_key=filter_key, group_key=group_key, gender='Female')\n",
    "print(*result_female[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last improvement we can do is to reduce the filtering steps from one to two, specifying the gender directly in the filter_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chevrolet', 42) ('Ford', 42)\n"
     ]
    }
   ],
   "source": [
    "filter_key = lambda row: row.last_updated >= datetime(2017, 3, 1) and row.gender == 'Female'\n",
    "group_key = lambda row: row.vehicle_make\n",
    "\n",
    "def group_data(fnames, ntuple, dtype, fields, *, filter_key, group_key):\n",
    "    data = iter_combined_filterd(fnames, ntuple, dtype, fields, key=filter_key)\n",
    "    sorted_data = sorted(data, key=group_key) # it is the only step that is not lazy\n",
    "    grouped = groupby(sorted_data_f, key=group_key)\n",
    "    cleaned = ((data[0], len(list(data[1]))) for data in grouped)\n",
    "    table = sorted(cleaned, key=lambda x : x[1], reverse=True)\n",
    "    return table\n",
    "\n",
    "result_female = group_data(fnames, ntuple, dtype, fields, filter_key=filter_key, group_key=group_key)\n",
    "print(*result_female[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
